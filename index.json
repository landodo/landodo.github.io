[{"content":"论文简介  会议：CVPR 2020 arXiv: 1912.08193 [PDF下载] 代码在 mmsegmentation 中有实现。 目前总引用数：225 Citations   Kirillov, A., Wu, Y., He, K., \u0026amp; Girshick, R.B. (2020). PointRend: Image Segmentation As Rendering. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 9796-9805.\n Abstract 该项工作创新地采用计算机图形学的渲染思路来解决计算机视觉领域的图像分割问题。\n作者提出了 PointRend（Point-based Rendering）神经网络模块：该模块基于迭代细分算法，在自适应选择的位置执行基于点的分割预测。PointRend 可以与现有的最新模型结合，灵活地应用于实例和语义分割任务。定性实验表明，对于先前的方法过度平滑的区域，PointRend 可以输出清晰的对象边界。定量实验表明，PointRend 在 COCO 和 Cityscapes 数据集上的实例分割和语义分割任务的表现都有显著提高。在同样的内存和算力情况下，PointRend 与现有方法相比能输出更高的分辨率。\n1 Introduction 用于图像分割任务的卷积神经网络通常在规则网格（regular grid）上操作：输入的是图像像素的规则网格，隐藏表征是规则网格上的特征向量，而输出则是基于规则网格的标签图。\n规则网格会对对象区域过采样，而同时对对象边界欠采样。使得预测结果的轮廓变得模糊（如图 1 左上）。PointRend 的细节效果更好，并且分辨率高，甚至五指的轮廓都可以分割出来。\n这篇论文的核心思想是将图像分割视为渲染问题，并利用计算机图形学中的经典思想来高效地“渲染”高质量标签图。作者将这一思想实现为一个新型神经网络模块——PointRend，该模块使用细分策略来自适应地选择一组非均匀点，进而计算标签。PointRend 可以整合到常用的实例分割元架构（如 Mask R-CNN）和语义分割元架构（如 FCN）中。PointRend 的细分策略所需的浮点数运算相比于直接密集计算减少了一个数量级，能高效地计算高分辨率分割图。\nPointRend 不对输出网格上的所有点执行过度预测，仅对精心选择的点执行预测。\n作者在 COCO 和 Cityscapes 基准数据集上评估了 PointRend 在实例分割和语义分割任务上的性能。定性结果显示，PointRend 能够高效计算不同对象之间的清晰边界。定量实验结果显示，PointRend 显著提升了 Mask R-CNN 和 DeepLabV3 的性能。\n3. Method PointRend 模块包含 3 个主要组件：\n(i) 点选择策略（point selection strategy）：选择少量真值点执行预测，避免对高分辨率输出网格中的所有像素进行过度计算。 (ii) 点特征表示（point-wise feature representation）：使用每个选中点在 f 规则网格上的 4 个最近邻点，对 f 进行双线性内插，计算点的特征。 (iii) point head：一个小型神经网络，基于逐点特征表示预测标签。\n3.1 Point Selection PointRend 的核心思想是在图像中自适应地选择预测分割标签的点。理论上来说，这些点的位置应该在邻近高频区域（如边界区域）分布较为稠密。\n作者提出的用于推断的点选择策略受到计算机图形学中自适应细分（adaptive subdivision）这一经典技术的启发。该技术仅在与近邻值显著不同的位置上进行计算，来高效渲染高分辨率图像；其他位置的值则保留原来的预测结果。\nPointRend 使用双线性插值对其先前预测的分割进行上采样，然后在这个更密集的网格上选择N个最不确定的点（例如，binay mask 概率最接近 0.5）。然后，PointRend 为这 N 个点中的每一个计算点的特征表示（第3.2节）并预测其标签。这个过程不断重复，直到分割图被升采样到一个理想的分辨率。\nPointRend 输入的特征图大小为 $M_0 \\times M_0$，需要升采样至 $M\\times M$，则采样点的数量不超过 $Nlog_2 \\frac{M}{M_0}$ 。\nTrtaining：Figure 4 的迭代策略对于利用反向传播训练神经网络不够友好。因此，训练过程采用基于随机采样的非迭代策略。\n点采样策略选择特征图上的 N 个点进行训练。它的目的是偏向不确定区域的选择，同时保持一定程度的均匀覆盖。基于以下三个原则：\n（i）Over generation（过度生成）：通过从均匀分布中随机抽样 kN 个点（k\u0026gt;1）来过度生成候选点。 （ii）Importance sampling（重要性抽样）：通过在所有 kN 个点对粗略预测插值，并计算特定任务的不确定性估计来关注具有不确定粗略预测的点。从 kN 个候选点中选出最不确定的βN 个点（β∈[0，1]）。 （iii）Coverage（覆盖范围）：其余（1-β）N 个点从均匀分布中采样得到。\n3.2 Point-wise Representation PointRend 通过组合细粒度特征（fine-grained features）和粗略预测特征（coarse prediction features），来构建所选点的逐点特征表示。\n通过在特征图上对应位置进行双线性插值来计算特征向量。\n coarse prediction features：decoder_head1 得到的粗分割结果； fine-grained features：将 backbone 输出的几组多尺度特征图进行融合的结果。  3.3 Point Head 对于每个选定点的逐点特征表示，PointRend 使用简单的多层感知器（MLP）进行逐点分割预测。损失函数使用常规的交叉熵损失。\n5. Experiments 分别进行了实例分割和语义分割实验。下面列出语义分割的实验结果。\n采用 Cityscapes 语义分割数据集，采用 mIoU 评价指标。作者采用 DeeplabV3 和 SemanticFPN 作为语义分割网络。PointRend 中的粗略预测特征来自语义分割网络的输出，细粒度特征分别从 DeeplabV3 的 res2 层以及 SemanticFPN 的 P2 层插值得到。\n（1）表 6 展示了 DeepLabV3 和 DeeplabV3 + PointRend 的对比情况。\nDeeplabV3 + PointRend 在 Cityscapes 语义分割任务上的性能超过基线 DeepLabV3\n（2）表 4 展示了 PointRend 在训练过程中使用不同点选择策略时的性能。\n（3）模型在 Cityscapes 样本上的实例分割和语义分割结果。\n","permalink":"http://landodo.github.io/posts/20220424-pointrend-image-segmentation-as-rendering/","summary":"论文简介 会议：CVPR 2020 arXiv: 1912.08193 [PDF下载] 代码在 mmsegmentation 中有实现。 目前总引用数：225 Citations Kirillov, A., Wu, Y., He, K., \u0026amp; Girshick, R.B. (2020). PointRend: Image Segmentation As Rendering. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 9796-9805. Abstract 该","title":"PointRend: Image Segmentation as Rendering"},{"content":"字节二面面试题：使用实现一个基础的 string 类。要求类中成员函数包含：\n 默认构造函数 拷贝构造函数 移动构造函数 —————— 拷贝赋值运算 移动赋值运算 —————— 析构函数 —————— 重载 +, ==, \u0026lt;\u0026lt; 等基本运算符  实现结果：\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;ostream\u0026gt;#include \u0026lt;istream\u0026gt;#include \u0026lt;cstring\u0026gt;using std::ostream; using std::istream;  class String { public: \t// 1. 默认构造函数 \t// 加上 const 作用：(1)不会修改指向值, (2)使得str可以指向右值 \t// String s(\u0026#34;HELLO\u0026#34;); \t// String s = \u0026#34;HELLO\u0026#34;; \t// String s; \tString(const char* str = nullptr) { \tstd::cout \u0026lt;\u0026lt; \u0026#34;[\u0026#34; \u0026lt;\u0026lt; this \u0026lt;\u0026lt; \u0026#34;] \u0026#34;; \tstd::cout \u0026lt;\u0026lt; \u0026#34;Call: String(const char* str = nullptr)\u0026#34; \u0026lt;\u0026lt; std::endl; \tif (str == nullptr) { \tlen_ = 0; \tstr_ = new char[1]; \t} \telse { \tlen_ = strlen(str); \tstr_ = new char[len_ + 1]; \tstrcpy(str_, str); \t} \tstr_[len_] = \u0026#39;\\0\u0026#39;; \t}  \t\t// 2. 析构函数 \t~String() { \tstd::cout \u0026lt;\u0026lt; \u0026#34;[\u0026#34; \u0026lt;\u0026lt; this \u0026lt;\u0026lt; \u0026#34;] \u0026#34;; \tstd::cout \u0026lt;\u0026lt; \u0026#34;Call: ~String()\u0026#34; \u0026lt;\u0026lt; std::endl; \tdelete[] str_; \tstr_ = nullptr; \t}  \t\t// 3.拷贝构造函数 \t// String s1 = s; \t// String s1(s); \tString(const String\u0026amp; other) { \tstd::cout \u0026lt;\u0026lt; \u0026#34;[\u0026#34; \u0026lt;\u0026lt; this \u0026lt;\u0026lt; \u0026#34;] \u0026#34;; \tstd::cout \u0026lt;\u0026lt; \u0026#34;Call: String(const String\u0026amp; other)\u0026#34; \u0026lt;\u0026lt; std::endl; \tthis-\u0026gt;len_ = other.len_; \tthis-\u0026gt;str_ = new char[this-\u0026gt;len_ + 1]; \tstrcpy(this-\u0026gt;str_, other.str_); \tthis-\u0026gt;str_[this-\u0026gt;len_] = \u0026#39;\\0\u0026#39;; \t}  \t\t// 4. 移动构造函数 \t// String s1(std::move(s)); \tString(String\u0026amp;\u0026amp; other) noexcept { // 告诉编译器不会发生异常，便于优化；如若运行时发生异常，则调用std::terminate()函数终止. \tstd::cout \u0026lt;\u0026lt; \u0026#34;[\u0026#34; \u0026lt;\u0026lt; this \u0026lt;\u0026lt; \u0026#34;] \u0026#34;; \tstd::cout \u0026lt;\u0026lt; \u0026#34;Call: String(String\u0026amp;\u0026amp; other)\u0026#34; \u0026lt;\u0026lt; std::endl; \tthis-\u0026gt;len_ = other.len_; \tthis-\u0026gt;str_ = other.str_; \t// 阻止 other 析构数据 \tother.str_ = nullptr; // C++ delete nullptr 是安全的行为! 因此不影响 other 对象析构函数 \t}  \t\t// 5. 拷贝赋值函数 \t// String s1; \t// s1 = s; \tString\u0026amp; operator=(const String\u0026amp; other) { \tstd::cout \u0026lt;\u0026lt; \u0026#34;[\u0026#34; \u0026lt;\u0026lt; this \u0026lt;\u0026lt; \u0026#34;] \u0026#34;; \tstd::cout \u0026lt;\u0026lt; \u0026#34;Call: String\u0026amp; operator=(const String\u0026amp; other)\u0026#34; \u0026lt;\u0026lt; std::endl; \tif (\u0026amp;other == this) { // String s; s = s; \treturn *this; \t} \tdelete[] this-\u0026gt;str_; // this-\u0026gt;str_ 至少包含 \u0026#39;\\0\u0026#39; \tthis-\u0026gt;len_ = other.len_; \tthis-\u0026gt;str_ = new char[this-\u0026gt;len_ + 1]; \tstrcpy(this-\u0026gt;str_, other.str_); \tthis-\u0026gt;str_[this-\u0026gt;len_] = \u0026#39;\\0\u0026#39;; \treturn *this; \t}  \t// 6. 移动赋值函数 \t// String s1 = std::move(s); \tString\u0026amp; operator=(String\u0026amp;\u0026amp; other) noexcept { \tstd::cout \u0026lt;\u0026lt; \u0026#34;[\u0026#34; \u0026lt;\u0026lt; this \u0026lt;\u0026lt; \u0026#34;] \u0026#34;; \tstd::cout \u0026lt;\u0026lt; \u0026#34;Call: String\u0026amp; operator=(String\u0026amp;\u0026amp; other)\u0026#34; \u0026lt;\u0026lt; std::endl; \tif (\u0026amp;other == this) { \treturn *this; \t} \tdelete[] this-\u0026gt;str_; \tthis-\u0026gt;len_ = other.len_; \tthis-\u0026gt;str_ = other.str_; \t// 阻止 other 析构数据 \tother.str_ = nullptr; // C++ delete nullptr 是安全的行为! 因此不影响 other 对象析构函数 \treturn *this; \t}  \t// 7. 重载运算符 \t// +, ==, \u0026lt;\u0026lt;：字符串拼接、判断两个字符串是否相等、输出. \tbool operator==(const String\u0026amp; other) { \tstd::cout \u0026lt;\u0026lt; \u0026#34;Call: bool operator==(const String\u0026amp; other)\u0026#34; \u0026lt;\u0026lt; std::endl; \t// strcmp(str1, str2); 相等返回0，不等为-1 \treturn (strcmp(this-\u0026gt;str_, other.str_) == 0 \u0026amp;\u0026amp; this-\u0026gt;len_ == other.len_); \t}  \tString\u0026amp; operator+(const String\u0026amp; other) { \tthis-\u0026gt;len_ += other.len_; \tchar* tmp = this-\u0026gt;str_; \tthis-\u0026gt;str_ = new char[this-\u0026gt;len_ + 1]; \tstrcpy(this-\u0026gt;str_, tmp); \tstrcat(this-\u0026gt;str_, other.str_); \tthis-\u0026gt;str_[this-\u0026gt;len_] = \u0026#39;\\0\u0026#39;;  \tdelete[] tmp; \ttmp = nullptr; \treturn *this; \t}  \t\t// 需要声明为友元函数 \tfriend ostream\u0026amp; operator\u0026lt;\u0026lt;(ostream\u0026amp; os, const String\u0026amp; s) { \tos \u0026lt;\u0026lt; \u0026#34;str: \u0026#34; \u0026lt;\u0026lt; s.str_ \u0026lt;\u0026lt; \u0026#34;, len: \u0026#34; \u0026lt;\u0026lt; s.len_; \treturn os; \t}  private: \tchar* str_; \tunsigned int len_; };  int main() { \t// 1.默认构造 \tString s = \u0026#34;12345\u0026#34;; \t// 3. 拷贝构造 \tString s1(s); // or String s1 = s; \t// 4. 移动构造 \tString s2(std::move(s));  \tString s3 = \u0026#34;ABCDE\u0026#34;; \t// 5. 拷贝赋值 \ts = s3; \t// 6. 移动赋值 \ts = std::move(s3);  \tString s4 = \u0026#34;XYZ\u0026#34;; \t// 7. 重载 +, ==, \u0026lt;\u0026lt; \ts = s + s4; \tif (s == s4) \tstd::cout \u0026lt;\u0026lt; \u0026#34;s == s4\u0026#34; \u0026lt;\u0026lt; std::endl; \telse \tstd::cout \u0026lt;\u0026lt; \u0026#34;s != s4\u0026#34; \u0026lt;\u0026lt; std::endl; \tstd::cout \u0026lt;\u0026lt; s \u0026lt;\u0026lt; std::endl; \treturn 0; }  /*** 运行结果 [008FFBA4] Call : String(const char* str = nullptr) [008FFB94] Call : String(const String\u0026amp; other) [008FFB84] Call : String(String\u0026amp;\u0026amp; other) [008FFB74] Call : String(const char* str = nullptr) [008FFBA4] Call : String \u0026amp; operator=(const String\u0026amp; other) [008FFBA4] Call : String \u0026amp; operator=(String\u0026amp;\u0026amp; other) [008FFB64] Call : String(const char* str = nullptr) [008FFBA4] Call : String \u0026amp; operator=(const String\u0026amp; other) Call : bool operator==(const String\u0026amp; other) s != s4 str : ABCDEXYZ, len : 8 [008FFB64] Call : ~String() [008FFB74] Call : ~String() [008FFB84] Call : ~String() [008FFB94] Call : ~String() [008FFBA4] Call : ~String()*/ ","permalink":"http://landodo.github.io/posts/20220420-cpp-implement-string-class/","summary":"字节二面面试题：使用实现一个基础的 string 类。要求类中成员函数包含： 默认构造函数 拷贝构造函数 移动构造函数 —————— 拷贝赋值运算 移动赋值运算 ———","title":"实现C++类的基本函数：3构造、2赋值、1析构、运算符重载"},{"content":"HyperSeg  Nirkin, Y., Wolf, L., \u0026amp; Hassner, T. (2021). HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 4060-4069.\n 论文简介：\n CVPR 2021 https://arxiv.org/abs/2012.11582  💡亮点：\n 解码器中的 Meta block 模块，在模块中提出了 Dynamic Patch-wise Convolution(DPWConv)； Context head 也比较新颖。该模块的作用就是学习meta block所用的参数；即本论文的创新点：“Hypernetworks——使用一个网络为另一个网络生成权重”； 网络的整体结构视为类 U-Net 结构（嵌套的U-Net）；  Abstract 提出了一个新颖的、实时的、语义分割网络，其中编码器既编码又生成解码器的参数（权重）。 此外，为了允许最大的适应性，每个解码器块的权重在空间上都有变化。该网络由一个嵌套的 U-Net 组成，用于提取更高层次的上下文特征； 一个多头的权重生成模块；主网络由动态补丁式卷积（DPWConv）组成。\n在 PASCAL VOC 2012（val. set.）、Cityscapes、CamVid 实现了SOTA 准确性与运行时间的折衷。\n1 Introduction 本篇论文试图通过为网络提供额外的适应性的方式来提高性能。\n使用元学习技术来增加这种适应性，通常被称为动态网络或超网络（dynamic networks or hypernet\u0002works）。以前的方法所建议的超网络并不能完全捕捉到高分辨率图像的信号，因此很少用于生成 image-like maps。\n本篇论文提供了一种新颖的编码器-解码器方法，其中编码器的主干是基于该领域的最新进展。编码信号通过内部U-Net映射到动态网络权重，而解码器则由具有空间变化权重的动态块组成。\n采用了具有动态权重的局部连接层，方法非常的有效。运行时间/精度权衡见 Fig 1.\n本论文的贡献总结为：\n A new hypernetwork architecture that employs a UNet within a U-Net.（嵌套 U-Net） Novel dynamic patch-wise convolution with weights that vary both per input and per spatial location. SOTA accuracy vs. runtime trade-off on the major benchmarks of the field.  2 Related Work Hypernetworks\nHypernetwork 是为其他网络（通常被称为主网络）产生权重值的网络。我们发现，Hypernetworks 从未被用于语义分割领域。\nLocally connected layers.\n连接性遵循一种空间模式，类似于传统的卷积层，但没有权重共享。\n在语义分割的背景下，我们是第一个提出将局部连接层与 Hypernetworks 结合起来。\nSemantic segmentation.\nFCN， 条件随机场(CRF)、U-Net、SPP、ASPP、Attention。\nReal-time segmentation.\n其目标是在准确性和计算量之间实现最佳权衡，重点是保持实时性能。网络架构通常由基于高效主干的编码器和相对较小的解码器组成。\nSegNet，ENet，ICNet，GUNNet，SwiftNet。\n深度可分离卷积，inverted residual blocks，BiSeNet，BiSeNetV2，TDNet。\n3. Method 提出的模型涉及三个子网络：\n Backbone (b)：EfficientNet context head (h)：Figure 2 (c) for details primary network(Decoder)：multiple meta-block  三个网络的权重：θb、θh 和 θw，在推理过程中是固定的，并在训练过程中学习。而 θmi，即 decoder meta block mi 的权重，在推理时是动态预测的（不是常规的卷积，而是动态的、局部的卷积）。\nBackbone：将输入图像 $I \\in \\mathbb{R}^{3 \\times H \\times W}$ 映射到组 5 组分辨率不同的特征图。\nContext head：将最后一个特征图映射为一个信号 φ。信号输入w生成主网络元块的权重d。 请注意，这些权重在不同的空间位置是不同的（动态）。\n最后，给定输入图像和特征图，F1，\u0026hellip;\u0026hellip;，Fn，其相应的相同分辨率的位置编码 P0，……Pn，以及权重 θd，解码器 d 输出分割预测，$S \\in \\mathbb{R}^{C×H×W}$，其中 C 是语义分割任务中的类别数量。\n整个网络是由以下一组方程定义：\n3.1. The encoder and the hypernetwork Backbone 提取不同分辨率特征图后，采用 1×1 卷积，为的是减少特征图的通道数量，以减小解码器的规模，\nContext head 输出的通道数为输入的一半。 最底层的特征图被平均池化以提取最高级别的上下文，然后使用近邻插值将其上采样到之前的分辨率。在 h 的上采样路径中，在每一级，将特征图与相应的上采样特征图连接起来，然后是一个全连接层。\n权重映射网络，w = [w0, \u0026hellip; , wn]，是 Hypernetworks 的一个关键部分，将 w 分成若干部分，并将这些部分附加到 Primary Network 块上，是比较有效的（ Figure 2 (b)）。将权重映射网络的各层w0, . . , wn, 的权重映射网络被嵌入到d的每个元块中。每个wi是与通道组1×1的卷积，见 Figure 3 (a)。\n3.2 Decoder(Primary Network) Decoder 包含 n+1 个 Meta-Block（$m_0, \u0026hellip;m_n$）。$m_0$ 对应输入图片，$m_i$ 对应特征图 $F_i$，每个块之后是双线性上采样并与下一个更高分辨率的特征图连接。\n解码器 d 的权重不仅取决于输入图像，而且会在图像的不同区域之间变化。d 受益于知道像素的位置信息。 出于这个原因，我们通过额外的位置编码来增强输入图像和编码器的特征图。\n每个 Meta-Block 基于 inverted resid\u0002ual block of MobileNetV2。Figure 3(a) 所示，包含一个点卷积 pw1，然后是深度卷积 dw，以及另一个没有激活函数的点卷积 pw2。\n3.3. Dynamic patch-wise convolution dynamic patchwise convolution (DPWConv):\n4. Experimental results 三个数据集： PASCAL VOC 2012, Cityscapes, 和 CamVid.\n评估指标：mIoU、FPS、GFLOP。\nBatch size = 1 模拟实时推理。\n$\\theta^b$ 在 ImageNet 上预训练得到 ，$\\theta^h, \\theta^w$ 取随机的正态分布。采用 Adam Optimizer，学习率采用 polynomial learning rate scheduling。\nConclusion 我们提出将自动编码器与超网络结合起来，以完成语义分割的任务。在我们的方案中，超网络是由三个网络组成的：语义分割编码器的主干b、内部U-Net形式的上下文头h、以及多个权重映射头wi。解码器是一个多块解码器，其中每个块，di，实现局部连接的层。其结果是一种新型的U-Net，能够动态地、局部地适应输入，从而有可能使分割过程更好地适应输入图像。正如我们的实验所显示的，在这个竞争激烈的领域，我们的方法在多个基准上都超过了SotA方法。\n","permalink":"http://landodo.github.io/posts/20220320-hyperseg/","summary":"HyperSeg Nirkin, Y., Wolf, L., \u0026amp; Hassner, T. (2021). HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 4060-4069. 论文简介： CVPR 2021 https://arxiv.org/abs/2012.11582 💡亮点： 解码器中的 Meta block 模块，在模块中提出了 Dynamic Patch-wise Convolution(","title":"HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation"},{"content":"Notes   Qt Charts是二维图表模块，用于绘制柱状图、饼图、曲线图等常用二维图表。\n  Qt Data Visualization是三维数据图表模块，用于数据的三维显示，如散点的三维空间分布、三维曲面等。\n  pyuic5 是用于将Qt Designer（或Qt Creator内置的UI Designer）可视化设计的界面文件（.ui文件）编译转换为Python程序文件的工具软件。\n  pyrcc5 是用于将Qt Creator里设计的资源文件（.qrc文件）编译转换为Python程序文件的工具软件，资源文件一般存储了图标、图片等UI设计资源。\n  widget 是 label 的父容器。在创建时，将父容器作为参数传入。指定父容器，这样标签才能显示在窗体上。\n  widget = QtWidgets.QWidget()  label = QtWidgets.Qlabel(widget)   基于QMainWindow类的窗体，具有主窗口的特性，窗口上有主菜单栏、工具栏、状态栏等。\n  QWidget类是所有界面组件的基类，如QLabel、QPushButton等界面组件都是从QWidget类继承而来。\n  Qt Designer中的Property Editor界面，还显示了组件的继承关系。如QLabel的继承关系为：QObject→QWidget→QFrame→QLabel；QPushButton为QObject-\u0026gt;QWidget-\u0026gt;QAbstractButton。\n  objectName是组件的对象名称，界面上的每个组件都需要一个唯一的对象名称，以便被引用。\n  retranslateUi()函数集中设置了窗体上所有的字符串，利于实现软件的多语言界面。\n  函数setupUi()用于窗体的初始化，它创建了窗体上的所有组件并设置其属性。窗体是外部传入的，作为所有界面组件的父容器。\n  # setupUi()函数只创建窗体上的其他组件，而作为容器的窗体是靠外部传入的。 self.ui = Ui_MainWindow() # 创建UI对象 self.ui.setupUi(self) # 构造UI界面 # or baseWidget = QtWidgets.Qwidget() # 创建窗体的基类QWidget的实例 ui = Ui_MainWindow() ui.setupUi(baseWidget) # 以baseWidget作为传递参数，创建完整窗体   Ui_MainWindow的父类是object，不是Qt的窗体界面。\n  过程化的程序，难以实现业务逻辑功能的有效封装。\n  界面与业务逻辑分离的设计方法可以有多继承方法，另一种是单继承方法。\n  在多继承时，使用super()得到的是第一个基类。\n  class MainWindow(QMainWindow, Ui_MainWindow): def __init__(self, *args, **kwargs): super(MainWindow, self).__init__(*args, **kwargs) self.setupUi(self) 如上程序中，super()执行之后得到的就是一个QMainWindow对象，可以作为参数传递给setupUi()函数。通过这样的多继承，Ui_MainWindow中定义的窗体上的所有界面组件对象就变成了新定义的类MainWindow的公共属性，可以直接访问这些界面组件。\n  使用 self.__ui = Ui_MainWindow() 设置私有属性，更符合面向对象封装隔离的设计思想。self.__ui.Lab表示窗体上的对象标签，self.Lab则表示MainWindow类里新定义的属性。两者不易混淆，有利于界面与业务逻辑的分离。\n  信号（Signal）/槽（Slot）：GUI程序设计的主要内容就是对界面上各组件发射的特定信号进行响应，只需要知道什么情况下发射了哪些信号，然后合理地去响应和处理这些信号。\n  槽（Slot）：槽实质上是一个函数，它可以被直接调用。槽函数与一般的函数不同的是：槽函数可以与一个信号关联，当信号被发射（emit）时，关联的槽函数会被自动执行。\n  用Qt Designer设计界面时，对于需要在窗体业务逻辑类里访问的界面组件，修改其objectName，以便在程序中进行区分。每个组件需要有一个唯一的objectName，自动生成的槽函数名称与objectName有关。\n  添加总布局后，当窗体大小改变时，各个组件都会自动改变大小。组件之间的层次关系可以在Object Inspector获取。\n  伙伴关系（Buddy ）是为了在程序运行时，在窗体上用快捷键快速将输入焦点切换到某个组件上。\n  Qt的界面组件都是从QWidget继承而来的，都支持信号与槽的功能。\n  UI Designer工具栏里的“Edit Signals/Slots”使用可视化的方式实现信号与槽函数的关联。需要设置Sender、Signal、Receiver、Slot。\n  sender.signalName.connect(receiver.slotName)  在一个GroupBox中应用布局，体现在代码中就是：  groupBox = QGroupBox(Dialog) # 以groupBox为父容器创建布局和组件 layout = QHBoxLayout(groupBox) chkBox1 = QCheckBox(groupBox) chkBox2 = QCheckBox(groupBox) # 将部添加到layout中 layout.addWidget(chkBox1) layout.addWidget(chkBox2)   toggled(bool)信号在复选框的状态变化时发射，复选框的勾选状态作为参数传递给函数。\n  下面的函数实现了信号与槽的关联。\n  QtCore.QMetaObject.connectSlotsByName(MainWindow) # 槽函数的命名规则 on_objectName_signalName(signal parameters) # 例子 on_btnClear_clicked()   @pyqtSlot()修饰符用于声明槽函数的参数类型。自定义的槽函数的函数名可以使用“do_”作为前缀，提高可读性。\n  使用PyQt5.QtCore.pyqtSignal()为一个类定义新的信号。类必须是QObject类的子类。信号具有connect()、disconnect()和emit()，分别对应关联槽函数、断开与槽函数的关联、发射信号。\n  尽量不要定义overload型信号。\n  资源文件（.qrc）最主要的功能是存储图标和图片文件，图标通常保存为十六进制编码数据。利用pyrcc5将 .qrc 文件转为对应的Python文件。\n  Qt C++类库和PyQt5之间存在差异的类和接口函数并不多。\n  第三章 GUI 应用程序设计   在对overload型信号编写槽函数时，如果不清楚哪个是默认的信号，最好直接使用@pyqtSlot()修饰符对参数类型进行声明。\n  QPushButton有一个checkable属性，如果设置为True, QPushButton按钮可以当作CheckBox或RadioButton使用。\n  属性 autoExclusive=True, checkable=True 可以将一组QPushButton设置为互斥的。\n  QSlider和QScrollBar最常用的一个信号是valueChanged(int)，在拖动滑块改变当前值时就会发射这个信号。\n  QTimer主要的属性是interval，是定时中断的周期，单位是毫秒。QTimer主要的信号是timeout()，在定时中断时发射此信号。\n  timer.timeout.connect()   QMainWindow是主窗体类，可以作为一个应用程序的主窗体，具有主菜单栏、工具栏、状态栏等主窗体常见的界面元素。\n  QAction是直接从QObject继承而来的一个类，不是一个可视组件。QAction就是一个实现某些功能的“动作”，可以为其编写槽函数，使用一个QAction对象可以创建菜单项、工具栏按钮，点击菜单项或工具栏按钮就执行了关联的Action的槽函数。\n  互斥的 Action 通过QActionGroup分组对象实现：\n  format_group = QActionGroup(self) format_group.setExclusive(True) format_group.addAction(self.alignl_action) #左对齐 format_group.addAction(self.alignc_action) #居中 format_group.addAction(self.alignr_action) #右对齐 format_group.addAction(self.alignj_action) #两端   以Word程序为例，窗体最上方是菜单栏（Menu Bar），菜单栏下方是工具栏（Tool Bar），最下方是状态栏（Status Bar）。\n  状态栏 QStatusbar，addWidget()函数按从左到右顺序将一个组件添加到状态栏，addPermanentWidget()添加的组件则位于状态栏的最右边。\n  工具栏 QToolbar，addWidget()：添加一个界面组件到工具栏；addAction()：添加一个QAction对象并创建工具栏按钮； addSeparator()：添加一个分隔条。\n  QAction常用的信号是triggered()和triggered(bool)，它们是overload型信号。triggered(bool)是带有复选状态参数的信号。\n  QToolButton有一个setDefaultAction()函数，使其与一个Action关联，自动获取Action的文字、图标、ToolTip等设置作为按钮的相应属性。单击一个QToolButton按钮就会执行Action的槽函数，与工具栏上的按钮一样。\n  下拉菜单的实现方式：创建一个QMenu对象、选择列表项的Action添加作为菜单项；setMenu(QMenu_obj)为一个ToolButton按钮指定下拉菜单、QToolButton的setPopupMode()函数为一个ToolButton按钮的下拉式菜单设置不同的弹出方式；\n  每个从QWidget继承的类都有信号customContextMenuRequested()，这个信号在鼠标右键单击时发射，为此信号编写槽函数，可以创建和运行右键快捷菜单。\n  首先创建一个QMenu类型的对象menuList，然后利用QMenu的addAction()方法添加已经设计的Action作为菜单项。创建完菜单后，使用QMenu的exec()函数显示快捷菜单。\n  menuList = QMenu(self) menuList.addAction(...) # 类函数QCursor.pos()获得鼠标光标当前位置 menuList.exec(QCursor.pos())   QTreeWidget是创建和管理目录树结构的类。\n  ScrollArea上面放置一个QLabel组件，QLabel的pixmap属性可以显示图片。通过QPixmap对象的操作可进行缩放显示。当图片较小时，显示的图片可以自动居于scrollArea的中间，当显示的图片大小超过scrollArea可显示区域的范围时，scrollArea会自动显示水平或垂直方向的滚动条，用于显示更大范围的区域。\n  QPixmap存储图片数据，并且可以缩放图片，缩放只需调用相应函数，返回缩放后的图片副本。QPixmap.load(fileName)函数直接载入一个图片文件的内容。scaledToHeight(height, mode = Qt.FastTransformation)：返回一个缩放后的图片的副本，图片缩放到一个高度height。\n  scaledToWidth(width, mode = Qt.FastTransformation)：返回一个缩放后的图片的副本，图片缩放到一个宽度width。\n  scaled(width, height, ratio = Qt.IgnoreAspectRatio , mode = Qt.FastTransformation)：返回一个缩放后的图片的副本，图片缩放到宽度width和高度height。\n  QLabel.setPixmap(pixmap)函数显示一个QPixmap类对象pixmap存储的图片。\n  QTableWidget是PyQt5中的表格组件类，每一个单元格是一个QTableWidgetItem对象。\n  QGroupBox组件是常用的容器类组件，可以在一个GroupBox里放置其他界面组件且进行布局。QGroupBox有两个属性：checkable和checked。当checked为False时，GroupBox组件内部的所有组件都被禁用。\n  layoutLeftMargin、layoutTopMargin、layoutRightMargin和layoutBottomMargin这4个属性用于设置布局组件与父容器的4个边距，默认为9。\n  水平布局类QHBoxLayout和垂直布局类QVBoxLayout都有一个属性spacing，用于设置布局内组件之间的间隔，默认为6。\n  水平布局QHBoxLayout和垂直布局QVBoxLayout都有一个layoutStretch属性，用于设置各组件宽度分配比例。\n  # groupBox内有3的组件,如下设置只有一个组件可随窗口伸缩 self.horizontalLayout.setStretch(0, 0, 1)  Lay Out Horizontally in Splitter左右分割布局。  第4章 Model/View结构   Model/View（模型/视图）结构：源数据由模型（Model）读取，然后在视图（View）组件上显示和编辑，在界面上编辑修改的数据又通过模型保存到源数据。将数据模型和用户界面分离开来。\n  将界面组件与原始数据分离，又通过数据模型将界面和原始数据关联起来，从而实现界面与原始数据的交互操作。\n Data（源数据）是原始数据。 View（视图或视图组件）是界面组件，视图从数据模型获得数据然后显示在界面上。 Model（模型或数据模型）与源数据通信，并为视图组件提供数据接口。 Delegate（代理或委托）在视图与模型之间交互操作时提供临时编辑组件的功能。    Delegate代理负责从数据模型获取相应的数据，然后显示在编辑器里，修改数据后又将数据保存到数据模型中。\n  通过数据模型存取的每个数据都有一个模型索引，视图组件和代理都通过模型索引来获取数据。保证数据的表示与数据存取方式的分离。\n  要获得一个模型索引，必须提供3个参数：行号、列号、父项的模型索引。\n  在构造数据项的模型索引时，必须指定正确的行号、列号和父节点。\n  QFileSystemModel为本机的文件系统提供一个数据模型，可用于访问本机的文件系统。\n  QStringListModel是用于处理字符串列表的数据模型，可以作为QListView的数据模型，在界面上显示和编辑字符串列表。QListView的setModel()函数用于设置一个数据模型。\n  数据模型与视图组件之间信号与槽作用的结果，当数据模型的内容发生改变时，通知视图组件更新显示。数据模型的数据与界面上视图组件显示的内容是同步的。\n  QStandardItemModel通常与QTableView组成Model/View结构，实现通用的二维数据的管理。\n  为TableView组件的某列或某个单元格设置自定义代理组件，根据数据的类型限定使用不同的编辑组件。\n  第5章 事件处理   基于窗体（Widget）的应用程序都是由事件（event）驱动的，鼠标单击、按下某个按键、重绘某个组件、最小化窗口都会产生相应的事件，应用程序对这些事件作出相应的响应处理以实现程序的功能。\n  app.exec_()开启了应用程序的事件处理循环。\n  QEvent还有很多子类表示具体的事件，如QKeyEvent表示按键事件，QMouseEvent表示鼠标事件，QPaintEvent表示窗体绘制事件。\n  当一个事件发生时，PyQt5会根据事件的具体类型用QEvent相应的子类创建一个事件实例对象，然后传递给产生事件的对象的event()函数进行处理。\n  QWidget定义了很多的默认事件处理函数，都会传递一个event参数，但是event的类型由具体事件类型决定。\n  用户在继承于QWidget或其子类的自定义类中可以重新实现这些默认的事件处理函数，从而实现一些需要的功能。如重新实现mouseReleaseEvent()函数对鼠标单击事件进行处理，为QWidget添加clicked()信号。\n  事件与信号是有区别的，但是也有关联。Qt为某个界面组件定义的信号通常是对某个事件的封装，例如QPushButton有clicked()信号和clicked(bool)信号，就可以看作是对mouseReleaseEvent()事件的不同封装。\n  QLabel没有doubleClicked()信号，可以通过事件处理和自定义信号创建一个具有doubleClicked()信号的新的标签类。\n  事件与信号的关系：信号可以看作是对事件的一种封装。\n  QEvent.Paint事件类型的默认处理函数是paintEvent()，就是用于绘制窗体背景图片的函数。\n  事件过滤器（event filter）：将一个对象的事件委托给另一个对象来监测并处理。\n  如下程序，self是两个QLabel组件所在的窗体，这样，界面组件LabHover和LabDBClick就将窗体注册为其事件监测者，在LabHover或LabDBClick组件上触发的事件会发送给窗体进行处理。\n  self.ui.labHover.installEventFilter(self) self.ui.labelDBClick.installEventFilter(self) # 窗体通过重新实现eventFilter()函数对被监测的对象及其事件进行处理 def eventFilter(self, watched, event): # 通过watched判断哪个是被监测对象 # 根据event.type()判断事件类型并作出相应处理 # 执行父类的eventFilter()函数 return super().event.Filter(watched, event)   QApplication类型的应用程序执行exec_()函数后就开始了事件的循环处理。事件队列未能得到及时处理，用户会感觉到响应迟滞。有两种解决方案：\n 采用多线程方法。 另外一种简单的处理方法是使用QCoreApplication的类函数processEvents()。    在一个耗时较长的计算处理过程中不允许用户用鼠标或键盘操作。\n  from PyQt5.QtWidgets import qApp qApp.processEvents(QEventLoop.ExcludeUserInputEvents)   拖放操作涉及：mousePressEvent()、mouseMoveEvent()、dragEnterEvent()、dropEvent() 事件函数。\n  QLabel组件的scaledContents属性设置为True，让图片适应QLabel组件的大小。\n  setAcceptDrops()是QWidget类定义的函数，用于设置一个窗体组件是否接受放置操作。将界面组件设置为不接受放置，而窗体接受放置，事件也是传播到组件所在的父容器，也就是由窗口处理。\n  MIME (Multipurpose Internet Mail Extensions)是多功能因特网邮件扩展。QMimeData是对MIME数据的封装，在拖放操作和剪切板操作中都用QMimeData类描述传输的数据。\n  第6章 对话框与多窗口设计   若要打开一个文件，调用类函数QFileDialog.getOpenFileName()。\n  QColorDialog是选择颜色对话框，选择颜色使用类函数QColorDialog.getColor()。\n  QFontDialog是选择字体对话框，选择字体使用类函数QFontDialog.getFont()。\n  QProgressDialog是用于显示进度的对话框，可以在大的循环操作中显示操作进度。\n  QInputDialog有单行字符串输入、整数输入、浮点数输入、列表框选择输入和多行文本输入等多种输入方式。\n  消息对话框QMessageBox用于显示提示、警告、错误等信息，或进行确认选择。\n  常用的窗体基类是QWidget、QDialog和QMainWindow，在创建GUI应用程序时选择窗体基类就是从这3个类中选择。\n  QWidget：在没有指定父容器时可作为独立的窗口，指定父容器后可以作为父容器的内部组件。QWidget是所有界面组件的基类。\n  MDI（Multiple Document Interface）就是多文档界面，它是一种应用程序窗口管理方法，一般是在一个应用程序里打开多个同类型的窗口。\n  第8章 绘图   PyQt5提供了两种绘图方法。一种是使用QPainter类在QWidget类提供的画布上画图，可以绘制点、线、圆等各种基本形状，从而组成自己需要的图形。\n  所有界面组件都是QWidget的子类，界面上的按钮、编辑框等各种组件的界面效果都是使用QPainter绘制出来的。\n  PyQt5另外提供一种基于Graphics View架构的绘图方法，这种方法使用QGraphicsView、QGraphicsScene和各种QGraphicsItem图形项绘图，在一个场景中可以绘制大量图件，且每个图件是可选择、可交互的，如同矢量图编辑软件那样可以操作每个图件。Graphics View架构为用户绘制复杂的组件化图形提供了便利。\n  QPainter是用来进行绘图操作的类，一般的绘图设备包括QWidget、QPixmap、QImage等，这些绘图设备为QPainter提供了一个“画布”。\n  使用QPainter对象在一个QWidget窗体的paintEvent()事件函数里直接绘图。\n  可以从QWidget继承一个类，创建自定义界面组件。\n  在UI可视化设计时，可以使用提升法（promotion）将一个组件类提升为其某个子类。\n  PyQt5为绘制复杂的可交互的图形提供了Graphics View绘图架构，它是一种基于图形项（GraphicsItem）的模型/视图结构。\n  Graphics View架构主要由3部分组成，即场景、视图和图形项。\n  视图（View）是QGraphicsView提供绘图的视图组件，用于显示场景中的内容。可以为一个场景设置多个视图，用于对同一个数据集提供不同的视口。\n  QGraphicsItem可以被选择、拖放、组合，若编写信号的槽函数代码或事件函数响应代码，还可以实现各种编辑和操作功能。\n  QGraphicsView没有与mouseMoveEvent()相关的信号。从QGraphicsView继承定义一个类QmyGraphicsView，实现鼠标移动事件函数mouseMoveEvent()和鼠标按键事件函数mousePressEvent()的处理，并把事件转换为自定义信号，这样就可以在主程序里设计槽函数响应这些鼠标事件。\n  第9章 文件   os.gtcwd()函数获取当前路径。内建函数open()打开文本文件并读取文件内容。\n  PyQt5中用于文件读写操作的类是QFile，它提供了文件读写的接口函数，可以直接对文件进行读写。\n  Python自带的os和os.path模块中提供了大量的目录和文件操作相关的函数，如获取当前目录、新建目录、复制文件、分离文件的路径和基本文件名、判断文件是否存在等。\n  QFile类是直接与I/O设备打交道进行文件读写操作的类，使用QFile可以直接打开或保存文本文件。\n  除文本文件之外，其他的需要按照一定的格式定义读写的文件都可称为二进制文件。每种格式的二进制文件都有自己的格式定义，写入数据时按照一定的顺序，读出时也按照相应的顺序。\n  目录和文件操作包括获取当前目录、新建或删除目录、获取文件的基本文件名和后缀、复制或删除文件等操作。\n  ","permalink":"http://landodo.github.io/posts/20220304-python-qt-notes/","summary":"Notes Qt Charts是二维图表模块，用于绘制柱状图、饼图、曲线图等常用二维图表。 Qt Data Visualization是三维数据图表模块，用于数据的三维","title":"《Python Qt GUI 与数据可视化编程》阅读笔记"},{"content":"（1）Simple ITK-SNAP  多个绘画区域相互独立。  class Canvas(QtWidgets.QLabel):  def __init__(self):  super().__init__()  self.clear(Qt.yellow)  self.last_x, self.last_y = None, None  self.pen_color = QtGui.QColor(\u0026#39;#000000\u0026#39;)   def clear(self, color):  self.background = QtGui.QPixmap(self.width(), self.height())  self.background.fill(color) # 背景  self.paintboard = self.background  self.setPixmap(self.paintboard) # 在这张图像上画   def mouseMoveEvent(self, event):  if self.last_x is None: # First event.  self.last_x = event.x()  self.last_y = event.y()  return # Ignore the first time.   painter = QtGui.QPainter(self.pixmap())  p = painter.pen()  p.setWidth(4)  p.setColor(self.pen_color)  painter.setPen(p)  painter.drawLine(self.last_x, self.last_y, event.x(), event.y())  painter.end()  self.update()   # Update the origin for next time.  self.last_x = event.x()  self.last_y = event.y()   def mouseReleaseEvent(self, e):  self.last_x = None  self.last_y = None # 被监测对象使用installEventFilter()函数将自己注册给监测对象 self.canvas1.installEventFilter(self) self.canvas2.installEventFilter(self) self.canvas3.installEventFilter(self) self.canvas4.installEventFilter(self)  # 监测对象实现eventFilter()函数，对监测到的对象和事件作出处理 def eventFilter(self, watched, event):  if watched == self.canvas1:  self.status.setText(\u0026#34;canvas 1\u0026#34;)  elif watched == self.canvas2:  self.status.setText(\u0026#34;canvas 2\u0026#34;) 预览医学图像数据\n  拖拽加载医学图像 .nii.gz 文件；\ndef dragEnterEvent(self, event: QtGui.QDragEnterEvent) -\u0026gt; None:  if event.mimeData().hasUrls():  # ...  event.acceptProposedAction()  def dropEvent(self, event): \tfilename = event.mimeData().urls()[0].path() # 完整文件名  # ...  event.accept()   图像放大/缩小功能；\n   SliderBar 切换当前切片；  （2）多目标自动精确分割 多目标包含：肿瘤、直肠系膜、内淋巴结、外淋巴结。\n   分割目标 Intersection over Union (%) Sørensen-Dice Coefficient(%)     肿瘤 79.17 88.37   直肠系膜 88.30 93.78   系膜内淋巴结 29.13 45.11   系膜外淋巴结 77.42 87.27     Model 基于 HRNet。   Training Curve   [origin image]-[Ground Truth]-[Prediction]  2D U-Net Epoch: 500\n（使用 Wisdom 进行可视化）\nTrain image/Ground Truth/Predict: Test image/Ground Truth/Predict: ","permalink":"http://landodo.github.io/app/","summary":"（1）Simple ITK-SNAP 多个绘画区域相互独立。 class Canvas(QtWidgets.QLabel): def __init__(self): super().__init__() self.clear(Qt.yellow) self.last_x, self.last_y = None, None self.pen_color = QtGui.QColor(\u0026#39;#000000\u0026#39;) def clear(self, color): self.background = QtGui.QPixmap(self.width(), self.height()) self.background.fill(color) # 背景 self.paintboard = self.background self.setPixmap(self.paintboard) # 在这张图像上画 def mouseMoveEvent(self, event): if self.last_x is None: # First event. self.last_x =","title":"直肠癌 MRI 多目标精确分割及靶区自动勾画软件进展"},{"content":"学习计算机的过程中，那些优秀的资料、网站。\n","permalink":"http://landodo.github.io/cs-zoo/","summary":"学习计算机的过程中，那些优秀的资料、网站。","title":"Cs Zoo"},{"content":"关于我\n","permalink":"http://landodo.github.io/about/","summary":"关于我","title":"About"},{"content":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介：\n 作者：Chufeng Tang（https://chufengt.github.io/） CVPR 2021 单位：清华大学计算机科学与技术系人工智能研究所 代码：https://github.com/tinyalpha/BPR  思考：\n（0）实例分割精度要想进一步提升，应该在哪里下功夫？\nTable 1 显示了边界修复所带来的性能提升具有巨大的潜力。\n（1）由于硬件的限制，分割时一般需要 Resize 或切 Patch。那么，应该怎么切patch呢？\n  随机切固定size的patch；\n  按网格切；\n  按实例所在位置切；\n  ✅先进行一个粗分割，然后以粗分割的边界为中心，提取Patch块。\n  （2）High-level information/Low-level information？\n高层次语义信息用于提供定位和粗略分割实例（localize and roughly segment objects）；低层次语义信息用于对于分割局部边界细节更为关键。在单一模型中权衡两者是非常困难的。\n（3）本文提出的BPR作为一种后处理方案，目前已有的后处理框架是怎么做的？\n设计一个边界感知的分割模型是一个分割任务的研究热点。主要的有两个方向：\n 集成一个额外的、专门的模块(分支)来处理边界； 基于现有分割模型的结果和后处理方案来细化边界  PolyTransform：将实例的轮廓转换为一组多边形顶点，应用基于Transformer的网络来预测顶点向对象边界的偏移；缺点：巨大的实例patch和复杂的Transformer架构。     注：PolyTransform(CVPR 2020)\n Abstract 由于特征图的空间分辨率较低，以及边界像素比例极低导致的不平衡问题，预测分割图的边界通常不精确。为了解决这个问题，本论文提出了一个概念上简单但有效的后处理改进框架，将实例分割模型的结果进行边界质量改进，称为 BPR。启发自看得更近，分割边界更好（looking closer to segment boundaries better），沿着预测的实例边界提取并细化一系列小边界 patch。通过更高分辨率的边界 patch 细化网络（BPR）。BPR 框架在 Cityspace 基准上，尤其是在边界感知指标上，比 Mask R-CNN 基线有显著改进。将 BPR 框架应用于“PolyTransform+SegFix”基线后，在 Cityspace 排行榜上取得了 SOTA。\n1 Introduction 分割所面临的最重要的问题之一是实例边界分割不精确（Figure 1 Left）。校正物体边界附近的误差像素可以大大提高 Mask 质量（Table 1），对于较小的物体，在一定的欧几里德距离(1px/2px/3px)范围内的像素可以获得较大的增益（AP 为9.4/14.2/17.8）。\n导致低质量边界分割的关键问题有两个：\n（1）特征图空间分辨率的降低使得物体边界周围的细节消失，预测的边界总是粗糙和不精确的（Figure 1,4）。\n（2）物体边界周围的像素只占整个图像的一小部分（不到1%），而且本质上很难分类。不平衡导致了优化偏向，低估了边界像素的重要性。\n许多研究试图改善边界质量，但上述问题仍未得到很好的解决。\n考虑到人工标注行为， 标注人员通常首先对给定图像中的每个对象进行定位和分类，然后在低分辨率下显式或隐式地分割一些粗略的实例掩码。之后，为了获得高质量的模板，注释者需要反复放大局部边界区域，探索更清晰、分辨率更高的边界分割。直观地说，需要高级语义来定位和粗略地分割对象，而低层细节（例如颜色一致性和对比度）对于分割局部边界区域更为关键。本文受人类分割行为的启发，提出了一种概念上简单而有效的后处理框架，通过crop-then-refine策略来提高边界质量。\n 首先提取沿预测实例边界的一系列小图像块； 拼接预测图像边界块后，将送入细化网络，细化粗边界。 精确的小块随后被重新组装成紧凑且高质量的实例分割图。  我们将提出的框架称为边界补丁精化（BPR，Boundary Patch Refinement）。\n由于我们只在对象边界附近裁剪，因此可以用比以往方法高得多的分辨率来处理 patch，从而可以更好地保留低层细节。同时，小patch中边界像素的比例自然增加，可以消除优化偏差。\n在本文的框架中，采用了目前流行的HRNet，它可以在整个网络中保持高分辨率的表示。提出的方法也是一种后处理方案，重点是对边界块进行细化，以提高分割图质量。\n3 Framework 提出框架的概述如 Figure 2 所示。作为一种后处理机制，无需对预分割模型本身进行任何修改或微调。\n3.1 边界块提取 Boundary Patch Extraction 首先需要确定掩码的哪一部分应该被细化。我们提出了一种有效的滑动窗式算法来提取沿预测实例边界的一系列patch。具体地说，我们密集地分配了一组方形边界框，框的中心区域应该覆盖边界像素，如图2(B)所示。得到的方框仍然包含较大的重叠和冗余，因此我们进一步应用非最大抑制(NMS)算法过滤出patch的子集(图2c)。经验表明，重叠越大，分割性能越好，但同时也存在计算量较大的问题。我们可以调整NMS阈值来控制重叠的数量，以实现更好的速度/精度折衷。除了图像补丁（image patches）外，我们还从给定的实例掩码中提取相应的二值掩码（corresponding binary mask patches）。调整拼接image patches、mask patches后，输入到 boundary patch refinement network 中。\n3.2. 边界块细化 Boundary Patch Refinement Mask Patch\n提供的位置和语义信息，使得精化网络无需从头开始学习实例级语义。取而代之的是，优化网络只需要学习如何定位决策边界周围的硬像素，并将它们推到正确的一侧。这一目标可以通过探索局部和高分辨率图像patch中提供的低级别图像特性（例如，颜色一致性和对比度）来实现。相邻的实例可能共享一个相同的边界patch，而学习目标则完全不同且不确定。如果不使用mask patch，则模型很难收敛（Figure 3）。\nBoundary Patch Refinement Network.\n采用HRNetV2，它可以在整个网络中保持高分辨率表示。通过适当增大输入大小，可以得到比以往方法更高分辨率的边界块。\nReassembling.\n将精分割的边界块替换先前的预测，对于那些边界框外没有细化的像素，预测是不变的。对于相邻patch重叠的区域，进行简单的求和取平均，应用0.5的阈值来区分前景和背景。\n3.3. Learning and Inference 基于从训练图像中提取的边界块来训练细化网络。在训练过程，只从预测掩码与地面真实掩码的交集大于0.5的实例中提取边界块，而在推理过程中保留所有预测实例。使用像素级的二进制交叉熵损失，用对应的地面真实掩模对模型输出进行监督。在训练时将NMS消除阈值固定为0.25，而在推理时根据速度要求采用不同的阈值。\n4 Experiments 数据集：Cityscape\n评价指标：COCO-style mask AP、F-score（AF：Average F-score）\n","permalink":"http://landodo.github.io/posts/boundary-patch-refinement/","summary":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介： 作者：Chufeng Tang（https://chufengt.github.io/）","title":"Boundary Patch Refinement"},{"content":"SETR  Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020).\n  Zheng, Sixiao, et al. \u0026ldquo;Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n 论文简介 SETR：用 Transformer 从 Sequence-to-Sequence 的角度重新思考语义分割。\n最早于 2020 年 12 月发布于 arXiv。\n会议：2021 CVPR\n一作单位：Fudan University \u0026amp; Tencent-Youtu Lab\nAbstract 语义分割大多使用空洞卷积和注意力模块来扩大感受野，提升对全局上下文的建模能力。本篇论文基于 ViT，使用一个纯 Transformer（没有卷积和分辨率降低）来将图片编码为一系列的 patch。通过在 Transformer 的每一层建模全局上下文，该编码器结合一个简单的解码器可以形成强大的分割模型，命名为 SEgmentation TRansformer(SETR)。\n实验结果，SETR 在 ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) 上取得了 SOTA。\n1 Introduction 语义分割一直被基于编码器-解码器（encoder-decoder）的架构主导，其中编码器可以说是最重要的模型组件。基于 CNN 的编码器通过卷积层的堆叠（考虑到计算成本，特征图的分辨率逐渐降低），感受野逐渐增加，学到更抽象/语义视觉信息（ abstract/semantic visual concepts）。但由于感受野的限制，学习对语义分割至关重要的远程依赖仍然具有挑战性（long-range dependency information）。\n克服如上限制提出的方法如下：\n 修改卷积操作（卷积核大小、空洞卷积、特征金字塔） 注意力机制  这些工作大多没有改变 FCN model 的本质：编码器对输入的空间分辨率进行下采样，学习到有助于区分语义类别的低分辨率特征图。解码器上采样特征图为全分辨率的分割图。\n本篇论文提出使用纯 Transformer 来替换 CNN 的编码器，称为 SEgmentation TRans- former(SETR)。这个基于 Transformer 的编码器将输入的图像视为一系列的图像块（image patchs），这些块通过可学习的 embedding 表示。然后利用全局自注意模型对序列进行变换，实现区分特征表征学习（discriminative feature representation learning.）。\n具体地，首先将图像分解成一个固定大小的网格，形成一系列地 patches。然后通过线性层得到每个图像块的特征嵌入向量（feature embedding vector），这些向量作为 Transformer 的输入。之后解码器将 Transformer 的输出恢复成原始分辨率。\n在编码器的每一层都没有降低空间分辨率，而是进行全局上下文建模，从而为语义分割问题提供了一个全新的视角。\nViT 在分类任务上有效性证明了图像特征不一定需要从局部逐渐学习到全局上下文（CNN 结构不是必要的）。SETR 将分类拓展到了分割上，提供了一个模型设计的新视角，并且在一些 benchmark 数据集上取得了 SOTA。\n主要贡献总结如下：\n（1）从 sequence-to-sequence 学习的角度，针对图像语义分割问题，提供了一个 FCN-based 模型的替代方案；\n（2）利用 Transformer 架构实现编码器；\n（3）为了验证 self-attention feature 的表现，介绍了 3 中不同复杂度的解码器设计。\n本篇论文提出的 SETR 模型，state of the art on ADE20K(50.28%), Pascal Context (55.83%)，在 Cityscapes 上取得了有竞争力的结果。\n3 Method 3.1 FCN-based semantic segmentation 回顾 FCN\n原始输入为 $H \\times W \\times 3$，后续层的输入为 $h \\times w \\times d$，$d$ 为特征图的通道数。感受野随着层的深度线性增加（取决于卷积核的大小）。FCN 中拥有大感受野的高层才可以建模远程依赖（long-range dependencies）。一味的加深层数以增加感受野带来的收益将迅速减少，上下文信息建模的感受野有限是 FCN 系列架构固有的局限性。\n将 FCN 和注意机制结合起来是一种更有效的学习远距离语境信息的策略。但是二次复杂性（特征图的像素个数）这些注意力方法通常作用于输入较少的高层。\n3.2. Segmentation transformers (SETR) Image to sequence\nSRTR 遵循与 NLP 相同的输入输出结构。\n","permalink":"http://landodo.github.io/posts/20211208-setr/","summary":"SETR Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020). Zheng, Sixiao, et al. \u0026ldquo;Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. 论文简介 SETR：用 Transformer 从 Sequence-to-Sequence 的角度重新思考语义分割","title":"SETR: Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers"},{"content":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation\n  CVPR 2021\n  代码未开源\n  关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep Supervision、Self-Taught Deep Supervision、Mean-Taught Deep Supervision\n  Abstract 针对医学图像分割任务，本论文提出一种半-弱监督方法（semi-weakly supervised segmentation algorithm ），该方法建立在深度监督和师生模式的基础上，并且容易整合不同的监督信号。考虑如何将深监督整合到较低的层次，multi-label deep supervision 是本方法成果的关键。\n通过提出的训练机制（novel training regime for segmentation），可以灵活利用图像，这些图像要么有像素级标注、要么有边框标注、要么有全局标注、要么无标注。将标注需求降低 94.22%，比全监督的方法只降低 5% mIoU。在视网膜液分割数据集上进行实验验证。\n1 Introduction 像素级标注在医学图像语义分割任务中获取代价昂贵。因此，所面临的问题是最小化所需的注释工作，同时最大化模型的准确性。\n现有的方法：结合无标注的数据，快速获取不同质量的弱标签，如从 image-level 到 bounding box 标注。半监督、弱监督方法（semi- and weakly-supervised approaches）取得了令人信服的结果。\n针对像素级标注小数据集，本文提出通过一种新的深度监督，将训练信号整合到分割网络层中，通过丰富的未标注图像放大这些信号。然后利用 mean-teacher 分割模型推理出健壮的伪标签（pseudo-labels）\n本文的贡献：\n（1）对不同数量的训练样本和大量不同的监督类型进行了深入的研究，以实现语义分割；\n（2）引入了一种新的深度监督范式，适用于所提出的 Multi-label Deep Supervision 技术，在此基础上，引入了灵活的半-弱监督路径来集成未标记或弱标记的图像：新的自学深度监督方法（Self-Taught Deep Supervision）。\n（3）Mean-Taught Deep Supervision 增加了对扰动的不变性和健壮的伪标签生成，获得了接近完全监督基线的结果，而只使用了 5.78% 的标签。\n3 Proposed Approach semi-weakly supervised semantic segmentation\nMulti-label Deep Supervision\n对于半-弱监督的语义分割（semi-weakly supervised semantic segmentation），图像数据集为：\n 图像 $x_i$ 可以有不同的标注，如：  $m_i$：pixel-wise annotated mask $b_i$：bounding box $g_i$：image-level label no annotation（无标注）    对于每张图像 $x \\in \\mathcal{D}$，并不一定包含对于的分割图 $\\mathcal{M}$。即每张图像可能有 mask、bounding box、image-level label 或无标注。对于没有 mask 情况，称为半监督分割。\nSupervision integration\n在 encoder-decoder 架构中集成额外的输出（deep supervision）。这些输出在decoder 中的特征图 $f_0, \u0026hellip;, f_h$ 上进行操作。其中，$f_0$ 是 decoder 最里面的特征图，$f_h$ 是最外边的特征图。\n特征图 $f_i$ 的空间维度为 $H_i \\times W_i$：\n 实验中：$H_0 \u0026laquo; H_h$，$W_0 \u0026laquo; W_h$ 输出头 $k_i$ 基于 $f_i$ 计算预测结果：$k_i(f_i) \\in \\mathbb{R}^{C \\times H_i \\times W_i}$  深监督信号（Supervision signals）\n不同的监督模式需要不同的损失函数，在有像素级掩码（seg map）的情况下，训练语义分割模型最常见的目标是最小化交叉熵损失：\n3.2 Multi-label Deeply Supervised Nets 参数高效的多标签深度监督\n深度监督集成到分割网络中的方式存在问题。即，全尺寸的 ground-truth 和网络特征图的空间分辨率之间的空间维度不匹配。\n The problem we identify is the way deep supervision is commonly integrated into segmentation networks. Specifically, the challenge arises due to the mismatch in spatial dimensions between the full-scale ground-truth mask and the smaller spatial resolution within the network’s feature maps (Equation 5).\n 除了使用有损的最近邻插值外，大多数工作是迫使网络学习提升尺度（up-scaling）来解决 ground-truth mask 和 low-resolution spatial features 之间的不匹配。up-scaling 后，标准的输出头将特征图转换为与 mask 相同的 size。\n总结两个缺点：\n（1）网络必须学会 up-scaling，代价是额外的参数；\n（2）中间特征担负着对输出空间中复杂的分类信息和空间关系进行建模的重任，我们怀疑这些信息和空间关系在解码过程中是有用的，但可能只是作为更稳定梯度的跳跃连接。\n (2) intermediate features are burdened to model complex classification information and spatial relations in output space that we question to be useful in the decoding process, but presumably only serve as skip-connections for more stable gradients.\n 本论文建议将特征图中每个位置（x，y）的每个特征向量 $f^{:,x,y}$ 建模为其在输入图像中感受野的 patch-descriptors。因此，我们的目标是将 patch-descriptors 的感受野中包含的所有像素的语义信息植入模型。我们认为，这可以通过强制执行多标签损失来实现，其标签包含接受域中存在的所有语义类别。\n由此可见，我们可以简单地缩小 ground-truth mask 的比例，使之与特征图的大小相匹配，而不需要花费任何参数，并包含限制在感受野中的所有类别的标签，以保留语义信息。down-scaling 过程可以通过 max-pooling 实现。降尺度后的目标 $m_i^{*} \\in \\mathbb{R}^{C \\times H_i \\times W_i}$ 包含特征 $f_i^{:,x, y}$ （patch-descriptor，由特征感受野内的空间位置汇总而成）的多标签 Ground Truth。\n down-scaled multi-label ground-truths: $m$ feature maps: $f$  Self-taught deep supervision\n使用多标签深度监督（Multi-label Deep Supervision）生成伪标签，下采样平滑了噪声监督信号（Fig 2. Right）。\nSelf-taught Deep Supervision 通过如下方式为未标注样本生成 binary ground-truth tensor。\n 将未标注图像 $x_i$ 通过分割网络获得伪标签 $p_i$； 利用伪标签实施 Multi-label Deep Supervision； 最外层对真实 ground-truth 和 pseudo-label 设置单独的输出头；  一个输出头使用干净的标签计算交叉熵损失； 另一个输出头使用公式（9）和伪标签计算损失；   伪标签的推理生成，使用的是干净标签的输出头；  如果 image-level label $g_i$ 可以获取，可以进一步将生成的伪标签约束到包含在 $g_i$ 中的类。以类似的方式，相关联的边界框标签 $b_i$ 可以将伪标签约束为位于粗略区域内。这导致了弱标签图像的灵活集成，以提高伪标签质量。\nMean-taught deep supervision.\n通过(1)强制关于扰动的一致预测和(2)使用教师模型来生成更健壮的伪标签，该教师模型是先前迭代的所有模型的组合。\nMean-Teachers（即先前模型参数的指数移动平均数）\n通过使用学生模型和先前教师模型的移动平均值不断更新教师模型的参数来维护教师模型，可以获得更好的预测。因此，教师不会单独接受培训，而只是通过以下方式进行更新：\n $\\theta$ 表示模型参数 $t$ 表示训练迭代 $\\alpha$ 平滑参数  4 Experiments output-head for pseudo-labels 和 standard output-head 有什么区别？\n goround truth 不同，噪声伪标签用于平滑    过分割问题，是否可以加入一个分类的深监督来缓解？\n多尺度 concatenate 和这个方法的区别？\n","permalink":"http://landodo.github.io/posts/every-annotation-counts/","summary":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation CVPR 2021 代码未开源 关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep S","title":"Every Annotation Counts"},{"content":"论文简介 论文名称：TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation\n arXiv：https://arxiv.org/pdf/2107.09843.pdf 单位：（美国）加利福尼亚大学、中科院计算所 会议：MICCAI 2021 source code：https://github.com/YaoZhang93/TumorCP  Motivation：Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation(CVPR 2021) https://arxiv.org/abs/2012.07177，针对自然图像\nAbstract 启发自最近提出的 “Copy-Paste” 数据增强方法，本篇论文提出 TumorCP：一种简单而有效的 object-level 数据增强方法，适用于肿瘤分割。\nTumorCP 在肿瘤分割任务上， Dice 以 7.12% 的显著幅度超过了 baseline。TumorCP + image-level 增强方法，比目前的 SOTA 方法 Dice 提升了 2.32%。\n1 Introduction 高质量的带注释的数据集需要大量的工作和领域知识，这在医学领域尤为显著。为了提高数据高效学习，从不同的角度提出了几种成功的方法：\n leveraging unlabeled data for semi-supervised self-training self-supervised pre-training distilling priors from data 通过不同形态的解剖成像生成新的数据 利用适当的数据增强方法增加数据多样性  不同于复杂的 GAN，“Copy-Paste” 是一种简洁的增强方法，其在自然图像的实例分割中取得了新的突破（CVPR 2021）。但在医学图像领域中，这种方法在很大程度上是未被探索的，因为 “Copy-Paste” 中往往忽略上下文信息，这在直觉上是不可取的！\n“Copy-Paste” 增强方法通过简单地将标注实例粘贴到新的背景图像上作为额外的训练数据，从而避免了从表示空间到像素空间的昂贵生成过程。\n Copy-Paste augmentation avoids costly generation processes from representation space to pixel space by simply pasting the labeled instance onto new background images as additional training data.\n 此外，由于在 “Copy-Paste” 中上下文信息往往被忽略，因此它对医疗任务的有效性仍有待观察。例如，在肿瘤分割中，周围视觉线索，即上下文环境（Context），对肿瘤出现的重要性；医学图像固有的解剖结构使得肿瘤分割离不开上下文。\n本篇论文旨在通过检验 “Copy-Paste” 增强技术在肿瘤分割中的有效性，来填补对上下文在医学领域中作用的理解的空白。\nTumorCP 从源图像中随机选取一个肿瘤，经过一系列空间增强、对比度增强、模糊增强后，将其粘贴到目标图像中的器官上。 TumorCP randomly chooses a tumor from a source image and paste it onto the organs in the target image after a series of spatial, contrast, and blurring augmentations.\n 使用肾肿瘤分割数据集（KiTS19 dataset），nnUNet 来评估此方法。\n2 Method TumorCP 是一种用于肿瘤分割的在线随机增强过程，它的实现是简单和直接的。\n给定一组训练样本 $\\mathcal{D}$，TumorCP 有 $(1 - p_{cp})$ 的概率不执行任何操作；有 $p_{cp}$ 的概率从 $\\mathcal{D}$ 中采样出一个对图像 $(x_{src}, x_{tgt}) \\sim \\mathcal{D}$，并执行一次 “Copy-Paste”。\n令 $\\mathcal{O}{src}$ 为图像 $x{src}$ 上肿瘤集合，$\\mathcal{V}{tgt}$ 为 $x{tgt}$ 上的器官的体积坐标集合，$\\mathcal{T}$ 是一组随机数据转换，每个转换都有一个称为 $p_{trans}$ 的概率参数。\n一次 “Copy-Paste” 流程：\n TumorCP 首先采样一个肿瘤 $o \\sim \\mathcal{O}{src}$、一组数据转换 $\\tau \\sim \\mathcal{T}$ 和一个目标位置 $v \\sim \\mathcal{V}{tgt}$； 然后将 $τ(o)$ 以 $v$ 为中心，取代原始数据和标注。  为了充分利用 TumorCP 的优势，精心设计了两种肿瘤 “Copy-Paste” 模式：\n intra-patient Copy-Paste inter-patient Copy-Paste  2.1 TumorCP’s augumentation Intra-/Inter- Copy-Paste\n为了研究病人之间的差异对 TumorCP 的影响，使用两种设置：\n（1）intra-CP：源图像和目标图像来自同一病人；\n（2）inter-CP：源图像和目标图像来自不同的病人。\n从数据分布的角度来看，由于其强度与数据整体一致，所以首选 intra-CP，但这限制了数据的多样性。从数据多样性的角度来看，inter-CP 更受青睐，因为它打开了利用其他患者的新背景和前景的途径，但它也带来了分布差异。（实验表明 inter-CP 要优于 intra-CP）\n使用三个不同的 object-level 转换来对 “Copy-Paste” 进行扩展。\n Spatial transformation decouples context and improves morphology diversity.  空间转换解耦了上下文，改善了形态的多样性。image-level 的一些增强方法（镜像、旋转）仍然作为一个整体处理图像，保持前景和背景之间的耦合。因此，模型可能会寻找并倾向于过度拟合周围看似合理但实际上不相干的线索。Figure 1 是应用缩放、旋转和镜像的刚性变换和使肿瘤变形的弹性变换来增加形态的多样性。\n Gamma transformation enhances contrast and improves intensity diversity.  伽马变换增强了对比度，提高了强度的多样性。随机采样的伽玛参数增强了肿瘤的强度多样性；幂律非线性增强局部对比度，有利于肿瘤鉴别。\n Blurring transformation improves texture diversity  模糊化改造提高了纹理多样性。使用高斯滤波器作为模糊变换，聚集噪声干扰的低层次纹理可以间接增加相对高层次纹理的纹理多样性。\n整个 Pipeline 可以结合 image-level 增强方法。\n2.2 Intuitions on TumorCP’s Effectiveness TumorCP 有两个目标：i) 增加数据多样性，ii) 学习高层次和抽象肿瘤的不变表示。数据多样性随着肿瘤的新组合和周围环境的增加而增加。为了了解高层次的信息，如下讨论了 TumorCP 的三个特性来解释其有效性。 通过语境不变的预测消除背景偏差（Eliminated Background Bias by Context-Invariant Prediction）  CNN 不可避免地将周围的视觉上下文与物体本身进行卷积，可能会使得模型偏向于看似合理但实际上与肿瘤无关的线索，从而增加过度拟合的风险。\nTumorCP 则为肿瘤提供了一个更为独立的区域，从而为肿瘤周围上下文环境提供了无限的可能性。提升了模型的泛化能力、消除了背景偏差。\n 通过转换-不变预测提高泛化能力（Improved Generalizability by Transformation-Invariant Prediction）  该模型应同时捕获高级语义信息和低级边界信息，以实现成功的分割。TumorCP 可以生成不同大小、形状、颜色和纹理的肿瘤，增加了类内差异。\n它能够帮助模型从数据中捕获更好的语义信息。换句话说，它使模型的预测在不同的数据转换（可能类似于真实世界的数据）中保持不变，并提高了通用性。\n Oversampling Behavior  数据不平衡是一个普遍存在的问题，典型的解决方案通常是根据类分布重新加权损失函数或重新采样训练数据。在肾脏肿瘤分割任务中，背景、器官、肿瘤极度不平衡，TumorCP 就像一个数据重采样器，以较小的成本显著增加肿瘤的增殖程度。\n3 Experiments and Discussion 在 KiTS19 数据集（肾肿瘤的分割）上评估 TumorCP，使用 Sørensen-Dice系数 (Dice) 评分。\n消融学习\n（1）Ablation on intra-CP with different transformations.\n（2）Ablation on intra-/inter-CP\n（3）Ablation on compatibility\n  TumorCP（Object-level）和 Image-level 的图像增强是兼容的；\n  TumorCP 也改善了器官分割；\n  极低数据量的实验\n4 Conclusion  提出了 TumorCP： a simple but effective object-level data augmentation for tumor segmentation； 在肾脏肿瘤分割任务上，比目前的 SOTA 提升了 2.31% Dice； 实验验证了 TumorCP 在极低数据量情况下的潜力； TumorCP 不直接处理 Inter-CP（不同的病人）之间的分布不匹配问题，但仍然取得了惊人的性能提升。  ","permalink":"http://landodo.github.io/posts/20211119-tumorcp/","summary":"论文简介 论文名称：TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation arXiv：https://arxiv.org/pdf/2107.09843.pdf 单","title":"TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation"},{"content":"🎯Topic: 2.5D methods for Volumetric Medical Image Segmentation论文简介 （1）2.5D 医学图像分割网络综述，发表于 2020 年 10 月。\n Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D Method: An Empirical Study\n （2）2.5D 分割网络实例，发表在 MICCAI 2019\n Learning shape priors for robust cardiac MR segmentation from multi-view images\n Abstract 医学图像数据（CT/MRI）大多是 3D 的，然而 3D CNN 并不一定是好的选择。更多的推理时间、计算代价、参数的增加有过拟合的风险（医学图像标注获取昂贵）。为了解决这一问题，人们提出了许多 2.5D 分割方法，以更低的计算成本利用体积空间信息。\n1. Introduction 3D 医学图像，如 CT 和 MRI 已广泛应用于临床诊断。体积图像的自动分割在生物医学应用中变得越来越重要。目前主要两种策略：（1）将三维体积切割成二维切片，并根据切片内信息训练二维 CNN 进行分割；（2）3D CNN。\n两种方法有其各自的优缺点：\n ✅ 2D CNN 更轻的计算和更高的推理速度。 ❌ 2D CNN 忽略相邻切片之间的信息，阻碍了分割精度的提高。 ✅ 3D CNN 具有对空间信息的感知能力。 ❌ 3D CNN 需要较高的计算成本，更高的推理延迟，参数越多，过拟合风险越大（尤其是小的数据集），这阻碍了进一步临床应用。  为了弥补 2D 和 3D CNN 之间的差距，提出了许多 2.5D 分割方法（也称为伪 3D 方法），通过设计新的体系结构或使用策略将体积信息融合到 2D CNN 中来实现高效的医学图像体积分割。\n本篇文章的主要贡献可以总结如下：\n 本文综述了 2.5D 医学图像分割方法的最新进展，将 2.5D 医学图像分割方法分为多视图融合（multi-view fusion）、融合层间信息（incorporating inter-slice information）和融合二维/三维特征（fusing 2D/3D features）三大类。 对这些 2.5D 方法在 3 各具有代表性的数据集（CT/MRI、心脏/前列腺/腹部）上进行了大规模的评估、比较。  2. Related Work 本节对多视图融合（multi-view fusion）、融合层间信息（incorporating inter-slice information）和融合二维/三维特征（fusing 2D/3D features）三大类 2.5D 分割方法进行综述。\n2.1 多视图融合（multi-view fusion）\n为了将体积空间信息整合到二维 CNN 中，一个简单而直观的解决方案是多视图融合（MVF）。\n多视图包括矢状面（sagittal）、冠状面（coronal）、横断面（axial）。训练 3 个2D CNN 分别从矢状面、冠状面和横断面进行分割，再将 3 个视图的结果融合在一起，可以充分利用 3D 空间信息，从而获得比 2D CNN 更好的分割结果。\n融合的方式有：多数投票、 Volumetric Fusion Net（一个浅层的 3D CNN）。\n2.2. 融合层间信息（Incorporating inter-slice information）\n三维空间的分辨率并不总是相同的，有时 x 和 y 轴的图像分辨率比 z 轴高十倍以上（直肠癌数据就是这样 shape=768x696x48）。这种情况下训练长轴（冠状面：xz 或 矢状面：yz）是不可取的。\n另一种策略将层间信息整合到二维 CNN 中，以探索空间相关性。网络不仅可以利用切片的信息，还可以利用相邻切片的信息。网络的输入是连续的切片，输出是对应中间切片的分割结果。通过整合片间信息，可以利用体积的空间相关性，同时避免了 3D 计算的沉重负担。\n不少方法都是喂入 3 张 slices，但是直接添加相邻的片作为多通道输入可能效率低下。当相邻切片混合成通道维时，输入切片的信息在第一卷积层融合，这一过程网络很难提取用于区分每个片的有用信息。\n计新的层间信息抽取体系结构成为研究的重点。\n bi-directional convolutional long short-term memory (BC-LSTM) ：3D 体的 2D 切片被视为一个时间序列，以提取切片间的上下文和特征； inter-slice attention module：利用相邻切片的信息生成 attention mask，为分割提供先验形状调节；contextual-attention block：使用片与片之间的元素减法，强制模型聚焦于边界区域。  2.3. 融合二维/三维特征（Fusing 2D/3D features）\n一些工作也集中在融合从二维和三维 CNN 提取的特征，虽然这些方法仍然使用三维卷积来提取空间信息，但是与训练纯三维 CNN 相比，总体计算成本降低了。\n#########################################################################\n2.5D 分割网络实例，MICCAI 2019\nAbstract 受经验丰富的临床医生如何通过多个标准视图（即长轴和短轴视图）评估心脏形态和功能的启发，本篇论文提出了一种新的方法，在不同的二维标准视图中学习解剖形状先验，并利用这些先验从短轴（SA） MR 图像中分割左心室 (LV) 心肌。\n提出的分割方法具有二维网络的优点，但同时结合了空间背景。在不同的短轴切片上实现了准确和稳健的心肌分割。\n1. Introduction 基于二维的分割网络，以 slice-by-slice 的方式训练，对于复杂形状的目标，小目标的情况，分割的结果不太理想。这是由于 2D 网络没有结合相邻的 short-axis(SA) 图像或 long-axis(LA) 图像的空间信息。\n对于三维的分割网络，心脏的三维空间背景可能受到潜在的层间运动伪影和低平面空间（SA）分辨率的影响，从而限制了它们的分割性能。（直肠癌使用单纯的 3D 网络，估计无法取得良好的效果）\n x轴和y轴保持的分辨率远高于z轴，3D cnn的性能优势并不明显，有时甚至不如一些2.5D方法。\n 经验丰富的临床医生能够从多个标准视图评估心脏形态和功能，即使用 SA 和 LA 图像来形成对心脏解剖的理解。直觉上，从多个标准视图学习到的表示对 SA 切片的分割任务是有益的。受此启发，论文提出了一种通过四个标准视图学习解剖学先验知识的方法，并利用该方法对二维 SA 图像进行分割。\n贡献总结：\n a) developed a novel autoencoder architecture (Shape MAE)，它从多个标准视图中学习心脏形状的潜在表示； b) developed a segmentation network (multi-view U-Net)，结合多视图图像的解剖形状先验来指导SA图像的分割； c) 与 2D/3D 方法进行评估，表明该方法具有更强的鲁棒性，且对训练数据大小的依赖性较小。  2. Methods 提出的方法包含两个新的架构：\n shape-aware multi-view autoencoder (Shape MAE)：从标准心脏采集平面（包括短轴和长轴视图）学习解剖形状先验； multi-view U-Net：通过将 Shape MAE 学习到的解剖学先验信息整合到改进的 U-Net 体系结构中，实现心脏图像分割。  Shape MAE: Shape-aware multi-view autoencoder\n通过多任务学习从标准心脏视图学习解剖形状先验。对于给定输入的原图 $X_i$，网络学习 $X_i$ 的低维表示 $z_i$，它最能重建所有 $j$ 个目标视图分割 $Y_j$。\n本论文采用 4 source views $X_i \\ (i = 1, 2, 3, 4)$，分别是 3 long-axis(LA) views，和 1 short-axis(SA) views，。\n LA: two-chamber view (LA1), three-chamber view (LA2), the four-chamber view (LA3)\nSA: mid-ventricular slice (Mid-V) from the SA view\n 网络从其中一个视图学习低维表征 $z_i$ 来重建分割图。分割图的视图 $Y_i$ 有 6 个，4 个对应源视图，另外两个为 SA lices：apical 和 basal。\n损失函数：\n 前两项为交叉熵； 最后一项是 latent representations $z_i$ 的正则化项；  该网络的原理是，不同的视图需要独立的函数将它们映射到描述全局形状特征的潜空间；而将这个潜空间转换到另一个视图或平面也需要一个特定的投影函数。根据六个目标视图而不是单一视图来预测心肌的形状，鼓励网络学习和利用不同视图之间的相关性，从而形成一个全局的、视图不变的形状表征，而不是一个特定视图的局部表征。这个框架中的所有编码器和解码器都是以多任务学习的方式联合训练的，这样做的好处是避免过度拟合，鼓励模型的泛化。\nMV U-Net: Multi-view U-Net.\nMV U-Net 相比于原始的 U-Net 卷积核更少一些（取决于当前的任务），并且结合 shape MAE 学习到的解剖形状先验信息。\nFuse Block 模块由两个卷积核 (3 × 3) 和一个残差连接组成，通过可学习权值将不同视图的形状表示结合起来。给定任意短轴图像切片 $I_p$，和它相应的 shape representation $z^{p}_1, z^{p}_2, z^{p}_3, z^{p}4$（通过 Shape MAE 获得），网络可以将先验知识提炼为网络的高级特征，使其能够通过多视图信息有效地细化分割：$S_p = f{MV\\ U-Net}(I^p, z^{p}_1, z^{p}_2, z^{p}_3, z^{p}_4; \\theta)$。\n该网络采用标准的交叉熵损失进行训练。\n3. Experiments and Results 4 Conclusion 提出了一个形状感知的多视图自动编码器，一个能够从多个标准视图学习解剖形状先验信息的多视图 U-Net，该网络是对原始 U-Net 架构的修改，合并了学习的形状先验信息，以提高心脏分割的鲁棒性。\n本论文将长轴 LA 和短轴 SA 结合起来，利用长轴图像的空间背景来指导短轴图像的分割。从 LA 视图中提取的额外解剖信息，对那些具有挑战性的切片的分割特别有利。\nMV U-Net 保持了 2D 网络的计算优势，在有限的训练数据下实现较高的分割性能。\n 什么是 LA？什么是 SA？\n✅答：即长轴和短轴数据。例如在直肠数据中，一个 .nii.gz 的 shape 为 768x696x48，那么 xy 为短轴，xz/yz 为长轴。xy 为短轴，横断面，分辨率较高。长轴分辨率低，数据模糊。\n","permalink":"http://landodo.github.io/posts/20211105-2-5d-network/","summary":"🎯Topic: 2.5D methods for Volumetric Medical Image Segmentation论文简介 （1）2.5D 医学图像分割网络综述，发表于 2020 年 10 月。 Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D Method: An Empirical Study （2）","title":"2.5D methods for Volumetric Medical Image Segmentation"},{"content":"Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation\n论文题目：协同训练和隐空间数据增强的鲁棒性医学图像分割\n https://arxiv.org/abs/2107.01079  会议：MICCAI 2021\n作者：Chen CHEN 陈晨\n  https://sites.google.com/view/morningchen-site/home\n  Research Assistant/PhD 伦敦帝国理工学院 Research Assistant/PhD\n  本科哈工大 物联网工程\n  代码：https://github.com/cherise215/Cooperative_Training_and_Latent_Space_Data_Augmentation\nAbstract 基于深度学习的分割方法在部署过程中容易受到不可预见的数据分布（unforeseen data distribution）变化的影响，例如，不同扫描仪引起的图像外观或对比度的变化，伪影等。本篇论文提出了一个用于图像分割模型的协同训练框架和一个用于生成困难样本的隐空间数据增强方法，提升了模型的泛化性和有限数据的稳健性。\n 协同训练框架由 fast-thinking network(FTN) 和 slow-thinking network(STN) 构成。FTN 学习解耦的图像特征和形状特征，用于图像重建和分割任务； STN 学习形状先验，用于分割校正和细化。 隐空间数据增强（latent space DA）是通过在通道和空间上遮盖解耦的隐空间，为训练生成具有挑战性的样本。  在公共心脏成像数据集进行了广泛的实验，证实了分割性能的提高，以及提高了对各种不可预见的成像伪影的稳健性。与标准的训练方法相比，具有隐空间数据增强的协同训练在平均 Dice 提高了15%。\n1 Introduction 将基于深度学习的方法部署到实际应用中的一个主要障碍是临床部署过程中的域转移（domain shift），其中包括不同医疗中心和扫描仪的图像外观和对比度的变化，以及各种成像伪影。在 Multi-domain datasets 上学习域不变的特征进行分割可以解决上面的问题，但是 Multi-domain 数据的获取成本较高，因此从 single-domain 和有限的数据中学习稳健的网络，对医学影像研究具有重要的实用价值。\n本篇论文提出了一个新的协同训练框架，用于从 single-domain 数据中学习到一个稳健的分割网络。主要贡献可以总结如下：\n（1）设计了一个由两个网络组成的协同训练框架这个框架启发自人类行为中的双系统模型：快速思维系统做出直觉判断，而慢速思维系统则通过逻辑推理进行纠正。\n对应到框架中，fast-thinking network (FTN) 旨在理解图像的上下文，并提取与任务相关的图像和形状特征进行初始分割；slow-thinking network (STN) 根据学习到的形状先验来完善初始分割。\n（2）提出了一种隐空间数据增强方法 latent space data augmentation (DA) method随机和有针对性的方式对从 FTN 中学习到的隐编码（latent code）进行通道和空间的遮盖（屏蔽），这样就会重建出一组多样化的挑战性图像和对应的分割图，以加强两个网络的训练。\n2 Methodology single domain dataset $D_{tr} = {(x_i, y_i)}^{n}_{i=1}$\nimage $x_i \\in \\mathbb{R}^{H \\times W}$\none-hot encoded C-class label maps $y_i \\in {0, 1}^{H \\times W \\times C}$ （ground truth）\n2.1 Framework 给定输入图像 x，FTN 提取特定任务的形状特征 $z_s$ 来执行分割任务，提取图像上下文特征 $z_i$ 来执行图像重建任务。共享 encoder $E_{\\theta}$，特征解耦器 $\\mathcal{H}$， 两个特定任务的解码器 $D_{\\phi_s}$、$D_{\\phi_i}$ 用于图像分割和重建任务。\n对 $z_i$ 应用特征解耦器 $\\mathcal{H}$，使与分割任务无关的信息（如图像纹理信息、亮度）在 $z_s$ 中被停用，稀疏的 $z_s$ 有益于模型的稳健性。\n$\\mathcal{H}$ 由两个卷积层堆叠，后接 ReLU 激活函数。图像重建需要低层次的信息，而图像分割则依赖于更集中的高层次信息。引入 $\\mathcal{H}$ 明确地定义了一个分层的特征结构，以提高模型的通用性；\nSTN 是一个去噪自动编码器网络 $C_{\\psi}$，学习形状先验来纠正 FTN 预测的分割。在推理时，FTN 进行对给定的图像 x 进行快速分割：$p = D_{\\phi_s}(\\mathcal{H}(E_{\\theta}(x)))$；STN 完善分割结果：$p\u0026rsquo; = C_{\\psi}(p)$。\n2.2 Standard Training 用监督的多任务损失函数联合训练三个编码器-解码器对，损失包含：图像重建 $L_{rec}$、图像分割 $L_{seg}$、形状校正 $L_{shp}$。\n $L_{rec}$ 是 Mean Squared Error(MSE) $L_{seg}, L_{shp}$ 是 cross-entropy function $y\u0026rsquo; = C_{\\psi}(y)$  2.3 Latent Space Data Augmentation for Hard Example Generation 为了缓解过拟合，提出了 latent space DA method，使得 FTN 可以自动构建具有挑战性的样本。\n掩码生成器 $\\mathcal{G}$ 在 latent code $z$ 上产生一个掩码 m，之后 $\\hat{z} = z \\cdot m$ 输入到解码器重构出被破坏的图像 $\\hat{x} = D_{\\phi_i}(\\hat{z_i})$ 和其对应的分割图 $\\hat{p} = D_{\\phi_s}(\\hat{z_s})$。这就是 latent code masking 数据增强方法。\n通过动态屏蔽 latent code，所提出的方法可以生成具有广泛多样性的图像外观和分割的样本。如下介绍几种 latent-code masking 方案：\n（1）Random Masking with Dropout\nlatent-code 的整个通道在训练时以 p 的概率被掩盖为全零。\n被遮盖后第 i 个通道的结果为：$\\hat{z}^{(i)} = z^{(i)} \\cdot m^{(i)}$。$z \\in \\mathbb{R}^{c \\times h \\times w}$。\n（2）Target Masking\n提出了有针对性的 latent-code masking 方案，该方案以梯度为线索来识别要掩蔽的“突出”特征。 采取图像重建损失和图像分割损失，分别计算 $z_i$ 和 $z_s$ 的梯度 $g_{z_i}$和 $g_{z_s}$。\n对梯度值进行排序，可以确定出那些最具预测性的元素。作者假设，对损失函数反应较高的元素是导致在不可预见的域转移下性能下降的主要原因。因此，对这种元素进行有针对性的遮蔽，以模拟数据分布的转移（data distribution shifts）。可以在通道维度和空间维度遮蔽 latent-code $z$ 的特征。\n $z^{ch}_p, z^{sp}_p$ 阈值 a 是 (0, 0.5) 之间随机抽取的退火系数，创建 soft masks；  Channel-wise masked code at i-th channel：\n$\\hat{z}^{(i)} = z^{(i)} \\cdot m^{(i)}$\nSpatial-wise masked code at (j, k) position：\n$\\hat{z}^{(j, k)} = z^{(j, k)} \\cdot m^{(j, k)}$\n2.4 协同训练 训练过程中，随机地将上面介绍的三种 mask generator 应用于 $z_i, z_s$，这个操作生成一组丰富的增强图像 $\\hat{x}$ 和分割图 $\\hat{p}$。这就得到 3 example pairs 用于训练：\n corrupted images-clean images $(\\hat{x}, x)$ corrupted images-GT $(\\hat{x}, y)$ corrupted GT-GT $(\\hat{p}, y)$  合作训练的最终损失定义为简单例子和困难例子上的损失的组合。\n3 Experiments and Results 应用与心脏图像分割任务，从 MR 图像中分割出左心室腔、左心室心肌和右心室。\n数据集：\n  ACDC：训练、intra-domain 测试\n  M\u0026amp;Ms：cross-domain 测试\n  ACDC-C (corrupted ACDC)：评估数据增强方法的鲁棒性\n  （1）实验 1：Standard Training vs Cooperative Training\n 两种方法在域内测试集上取得了相近的性能； 双网络（FTN+STN）的协同训练在域外测试集上取得了更高的性能； 隐空间数据增强方法数据上，协同训练也表现出了其优越性。  （2）实验 2: Latent Space DA vs Image Space DA\nTable 1 中 AdvBias 在 M\u0026amp;Ms 数据集和 RandBias 上取得了最好的性能，但这种方法有一个副作用，使其对尖峰伪影更加敏感（Dice 得分 0.4901 vs 0.3840）。latent-space DA 在六个数据集上取得了最高的平均性能。\n图 3. 本篇论文的方法不仅可以生成扰动的图像，还可以生成不确定性增加的真实的损坏的分割。\n（3）实验 3：Ablation Study\n(a) the proposed targeted masking; b) latent code decoupler H; c) cooperative training.\n 禁用 $\\mathcal{G}{ch}、\\mathcal{G}{sp}$ 后，平均 Dice 得分从 0.6901 降至 0.6584； 图像重建需要低层次的信息，而图像分割则依赖于更集中的高层次信息。引入 H明确地定义了一个分层的特征结构，以提高模型的通用性； 突出了合作训练策略加强基于学习的形状细化和修正；  4 Conclusion  提出了一个新的协同训练框架，以及一个 latent space masking-based 的数据增强方法； 实验验证了模型的通用性和对不可预见的领域转变的鲁棒性； latent-space DA 方法只需要很少的代价； 目前只在心脏图像分割上展示了其性能，该通用框架有可能扩展到广泛的数据驱动的应用。  ","permalink":"http://landodo.github.io/posts/20211024-latent-space-data-augmentation/","summary":"Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation 论文题目：协同训练和隐空间数据增强的鲁棒性医学图像分割 https://arxiv.org/abs/2107.01079 会议：MICCAI 2021 作者：Chen CHEN 陈晨 https://sites.google.com/view/morningchen-site/home Research Assistant/PhD 伦敦帝国理工学","title":"20211024 Latent Space Data Augmentation"},{"content":"3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation #结直肠癌分割📌\n  Cited by: 18\n  Publish Year: 2020\n  Published in: IEEE Transactions on Cybernetics\n  地址：https://ieeexplore.ieee.org/document/9052757\n  Code：https://github.com/huangyjhust/3D-RU-Net\n  0. Abstract 基于深度学习的方法在 3D 图像分割任务中提供了一个良好的 baseline，但是由于内存的限制，较小的 patch 限制了有效的感受野，影响分割性能。\n将 RoI 定位作为前项操作，在速度、目标完整性、减少假阳性（FN）等方面具有多重优势。本篇论文：\n  提出了一种==多任务框架（3D RoI-aware U-Net）==，用于 ROI 定位和区域分割；\n  设计了一个==基于 Dice 的损失函数（MHL）==，用于从全局（RoI 定位）到局部（区域内分割）的多任务学习过程。\n  在 64 例癌症病例上进行实验，结果表明，该方法明显优于传统的方法，具有较强的泛化性、拓展潜力，可用于医学图像的其他 3D 目标分割任务。\n1. Introduction 基于深度学习的方法在医学图像检测和分割领域处于领先地位，然而，依旧面临着许多挑战：强度特异性弱、缺乏形状特征、缺乏位置先验、类别不平衡，以及在较差的 GPU/CPU-only 上的处理时间过长。除此之外，patch 大小受限于 GPU 显存，扩大感受野和减少降采样过程细粒度丢失是一个至关重要的问题。\n✅在医学应用中，由于目标和背景高度相关，因此==全局理解==甚至更为重要。\n🚩本篇论文贡献总结如下：\n  提出一种新的联合 RoI 定位-分割框架（3D RoI-aware U-Net），具有如下优势：fast RoI localization；target completeness； large effective receptive field；easy-to-train；detail-preserving；end-to-end；volume-to-volume segmentation。\n  设计的混合损失函数（Dice formulated global-to-local multi-task hybrid loss, MHL）帮助网络既处理大体积的小目标，又专注于准确识别局部 RoI 中的边界。\n  通过实验验证了所提出的框架的有效性、通用性；\n  2. Related Work 现有的 3D 图像病变检测和分割方法一般可以分为：基于局部的模型（part based models）和 non-joint localization-segmentation based methods。\n  part based：FCN、V-Net，有效感受野有限。\n  non-joint localization-segmentation based：RoI 定位模块作为独立的部分，外部模块 Selective Search、Multiscale Combinatorial Grouping、FPN 提取候选区域；\n  上述两种方式存在的问题：使用基于 patch 的分割无法解决感受野有限的问题；使用独立的外部模块进行候选区域的提取，再独立的 FCN 进行 RoI 分割时，无法共享特征。\n联合 RoI 定位-分割模型是一种很有前景的发展，共享 backbone 来实现区域候选、区域分类和区域内分割，消除了冗余特征提取。\n  Multi-task Network Cascades\n  Mask R-CNN: FPN\n  类别不平衡问题：\n V-Net：Dice loss Deep Contour-aware Network Multilevel Contextual 3D CNNs DeepMedic \u0026hellip;  3. Methodology 3D RU-Net 结构如上图所示。\n  将整个 image volumes 输入 ==Global Image Encoder==，进行多层次编码；\n  采用编码器专用的 ==RoI locator== 进行 RoI 定位；\n  利用 ==RoI Pyramid Layer== 从多尺度特征图中裁剪区域内特征张量，获得多尺度的 RoI 区域，图中称为 RoI Tensor Pyramid $(f^I, f^{II}, f^{III})$；\n  设计一个 ==Local Region Decoder== 来进行多级特征融合，用于高分辨率癌症病灶分割。\n  3.1 Main Modules （1）Global Image Encoder\n谨慎设计 3D backbone feature extractor 以避免 GPU 内存溢出和过拟合。构建一个紧凑的仅有编码器的网络，名为 Global Image Encoder，用于处理 whole volume images。\nResBlocks + MaxPooling 堆叠。\nResidual Block:\n 3 convolutional layer 3 Instance Normalization (batch size = 1) 3 ReLU Skip Connection  （2）RoI Locator\nRoI Locator 是一个模板，以特征图 $F^{III}$ 作为输入，得到 $Bbox^{III}$ 输出。任何采用纯编码骨干的目标检测方法都可以被采用。\n由于数量有限的训练样本的长宽比多样性，学习准确的边界框可能是困难的，建议充分利用可用的体素级掩码。为了解决前景与背景比例极不平衡的问题，采用基于 Dice 的损失来训练 ROI Locator。\n进行快速的三维连通性分析（Fast 3D connectivity analysis）计算出所需的 Bounding Box（$Bbox^{III}$）。\n📌**（3）==RoI Pyramid Layer==**\n从每个特征尺度提取一组多层次的特征张量，充分利用多尺度特征。\n为了提取检测目标的 RoI Tensor Pyramid，首先从(2) RoI Locator 计算得到的边界框（Bounding box）$Bbox^{III}=(z^3, y^3, x^3, d^3, h^3, w^3)$ ，公式(1)构建 Bounding Box Pyramid $(Bbox^{I}, Bbox^{II}, Bbox^{III})$。\n $(s_{z}^{i}, s_{y}^{i}, s_{x}^{i})$ 表示 $MaxPooling^{i}$ 的 stride；  得到了 Bounding Box Pyramid $(Bbox^{I}, Bbox^{II}, Bbox^{III})$ 后，从 $F^{I},F^{II}, F^{III}$ 中裁剪出 RoI Tensor Pyramid $(f^I, f^{II}, f^{III})$。\n（4）Local Region Decoder\n得到了 RoI Tensor Pyramid $(f^I, f^{II}, f^{III})$ 之后，构建一个名为 Local Region Decoder 的区域内分割网络，这个网络融合了多尺度的特征。\n3.2 Loss Function design 文章的另一个核心点在于 ==Dice-based Multi-task Hybrid Loss Function (MHL)== 的设计。\n上图的网络结构属于多任务学习（Localization + Segmentation），Global Image Encoder 主要面临类别不平衡问题；而 Local Region Decoder 则是目标区域的精确边界分割问题。\n（1）Dice Loss\n N voxels $p_i \\in P$ ：predicted volume $g_i \\in G$：ground truth volume $\\epsilon = 10^{-4}$ ：平滑参数  （2）Dice Loss for Global Localization\n $P_{global}$ and $G_{global}$ denotes predictions of the localization top and down-sampled annotations.  ❓这个我不太明白。我的理解是不同的检测方法，这是只是提供了一个范式。    （3）Dice-based Contour-aware Loss for Local Segmentation\n Contour 表示边轮廓。（3D 空间中轮廓标签的极端稀疏性）； 在分割的输出端增加一个额外的由 Sigmoid 激活的 1 × 1 × 1 卷积层来预测轮廓体素，并与区域分割任务并行训练； $\\lambda_c = 0.5$，辅助任务的权重，确保区域分割任务占主导地位。  ===\u0026gt; 最后得到总的损失函数，Dice-based Multi-task Hybrid Loss Function (MHL)：\n $\\beta = 10^{-4}$  3.3 多感受野模型集成 本文提出采用多感受野模型集成策略，融合结构相同但感受野设置不同的模型。如下图，将三个网络的输出取平均，生成最终的预测。\n不同感受野模型，实现的方法是控制空洞卷积的 dilation rate。下表感受野为 26 × 64 × 64 为原始的 3D R-U-Net，记为 3D RU-Net-RF-64。\n4. Experiments 4.1 数据集和预处理 64 例 MRI 图像，T2 模态。目标区域由经验丰富的放射科医生进行标注，一个 3D 图像通常有一到两个含有癌组织的 RoI。癌组织轮廓标签 contour labels were automatically generated from the region labels of one-voxel thickness using erosion and subtraction operations.\ncrop 黑边、重采样 4.0 × 2.0 × 2.0 mm、强度归一化。\n下图是归一化（intensity-normalized）的效果。\n4.2 实现细节 网络结构如 Table 1 所示。\n Optimizer: Adam batch size = 1 输入的 shape = ？ learning rate: 10e-4 L2 norm: 10e-4 先训练 RoI Locator，直到评估 Loss 不在降低； 再联合训练 RoI Locator 和分割分支。联合训练过程的 Loss 来自 RoI Locator + SegHead1 + SegHead2。  评估指标：Dice Similarity Coefficient (DSC)、Voxel-wise Recall Rate、Average Symmetric Surface Distance (ASD)。\n2 块 NVIDIA Titan(12 GB GPU memory)\n4.3 实验结果 Table 2（消融学习）、Figure 5、Figure 6\n（1）（2）5. Discussion 相比于传统的 Encoder-Decoder 在每个路径上各花费 50%，本文通过构建 Local Region Decoder，GPU 可以将其 90% 的 GPU 内存分配给 Encoder，以处理更大的输入体积，只在分割阶段花费 10% 的内存。因此，可以处理的体积大小被大大的扩大了！（有几率不用预先切 Patch 了）\n提出的方法的局限性：\n  模型经常混淆哪个切片开始或结束，这对得分的影响较大（Figure 6）\n   这个困难是与数据相关的，由于癌组织边界的对比度较弱，沿 Z 轴的分辨率较低，开始和结束切片指数的决定可能取决于观察者。\n     没有进行实例分割相关的探索  Conclusion  提出了联合 RoI localization-segmentation-based 框架（3D RoI-aware U-Net）； 强调了将 RoI 定位和区域内分割结合==全局编码特征==的重要性和有效性； 提出多任务混合损失(MHL)来平滑训练过程； 实验结果表明，该方法在速度和准确性方法具有较大优势； 原则上，此框架具有良好的可拓展性，可以用于其他医学图像分割任务。  ","permalink":"http://landodo.github.io/posts/20210924-3d-roi-aware-u-net/","summary":"3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation #结直肠癌分割📌 Cited by: 18 Publish Year: 2020 Published in: IEEE Transactions on Cybernetics 地址：https://ieeexplore.ieee.org/document/","title":"3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation"},{"content":"论文简介 基于三维卷积神经网络的头颈部血管图像快速分割与重建\n Fu, F., Wei, J., Zhang, M., Yu, F., Xiao, Y., Rong, D., Shan, Y., Li, Y., Zhao, C., Liao, F., Yang, Z., Li, Y., Chen, Y., Wang, X., \u0026amp; Lu, J. (2020). Rapid vessel segmentation and reconstruction of head and neck angiograms using 3D convolutional neural network. Nature communications, 11(1), 4829. https://doi.org/10.1038/s41467-020-18606-2\n Publish Year: 2020\n期刊：Nature Communications\n下载地址：https://www.nature.com/articles/s41467-020-18606-2.pdf\n摘要 计算机断层扫描血管造影图像（ computed tomography angiography, CTA）的后处理（分割、重建）操作时一项非常消耗时间、且易于出错的任务。本片论文提出了一种充分利用生理解剖学的 3D 卷积神经网络 CTA 重建系统。\n数据集来自中国 5 家三甲医院的 18766 例头颈部 CTA 扫描图像。最后的结果为 DSC = 0.931，该系统的结果与人工处理的图像具有一致性，符合临床要求，临床评估结果合格率为 92.1%。\n该系统部署应用 5 个月之后，对于每一个病例的处理时间从 $14.22\\pm 3.64$ 减少到 $4.96 \\pm 0.36$ 分钟，促进了临床工作流程。\nIntroduction 计算机断层血管造影（CTA）是一种广泛的、微创的、经济有效的成像方式，用于头颈部血管的常规临床诊断。可视化分析头颈部的血管系统（血管成像重建）通常由经验丰富的计算机断层扫描技术人员进行。\n血管是曲折和有分支的，因此，准确的血管分割、保证连续性（不中断）是一个具有挑战性的任务。除此之外，分割还容易受到其他组织的影响，如颅内入口血管的 CT值与颅骨 CT 值高度相似，这可能导致血管外组织粘连，影响后续疾病诊断。\n深度神经网络架构是克服上述技术障碍的最有效途径。U-Net 已经被证实了其在医学图像分析中的应用前景。3D CNN 符合生理解剖，目标图像的形态学特征是专门为分割任务设计的。\n本篇论文开发了一种基于优化解剖学先验知识的 3D-CNN 自动成像重建系统（CerebralDoc，脑科医生），用于重建原始的头颈部 CTA 图像，帮助工作人员建立节省时间的工作流程。\n最后，对 CerebralDoc 进行全面评估，结果表明，将人工智能技术集成到放射科工作流程中，能大幅度提高工作流程效率，降低医疗成本。\nResults 患者及影像特征\n数据集包含 18766 例头颈部 CTA 扫描的患者，手工筛选排除 507 例低质量图像。表 1 总结了用于训练、验证和独立测试的 CTA 扫描数据集。\n模型性能\n数据增强后，有 91,295 例（扩增了 5 倍）用于训练。3D-CNN 使用使得 U-Net + bottleneck-ResNet（ResU-Net）。CerebralDoc 系统分为两个主要部分：\n ResU-Net：主要负责骨分割和血管提取； connected growth prediction model (CGPM). 关联增长预测模型：负责曹正血管的连续性；  模型训练过程包括骨分割和血管分割。血管分割（Vessel Segmentation）和 Bone Segmentation 的 DSC、V-score、Recall 如下 Fig.2。\nCerebralDoc 的临床评估\n AI 和手工处理在整体临床评估数据集和不同疾病中的 5 分评级分布； Fig 4 直接呈现由 CerebralDoc 和手工处理生成的图像； 两图都得出相同结论：CerebralDoc 和人类输出之间没有统计学上的显著差异（图 3a和 4）。   VR 和 MIP 中 AI 与人工处理的比较； 人工处理中右侧大脑中动脉闭塞，未重建侧支循环（a），AI 成功重建侧支循环（b）； AI 比人工处理的图像更清晰，尤其是海绵窦段（c-d） 在 CerebralDoc 中，由于算法中的自动骨分割，所有的 MIP 图像都被评分为3分（被认为有中度骨残留但不影响血管观察）。  CerebralDoc 的临床应用效果\n 所有接受头颈部 CTA 检查的患者，包括动脉粥样硬化性溃疡、动脉瘤、烟雾病、颈部支架植入术的患者，均使用 CerebralDoc。   2019 年 7 月至 11 月，宣武医院临床共 3430 例患者行头颈部 CTA 扫描；经经验丰富的技术人员评价，成功重建 3122 例(91.0%)。最后，将 2649 张重建图像应用于临床(图7a)。而且，随着医生信任度的提高，重建图像被推送到诊所的概率从 243 逐渐增加到 753 (图7b)。  比较 CerebralDoc 与手工处理的性能\n两位技术人员手工处理一个病例的平均耗时为：13.48 ± 3.67、14.95 ± 3.65 分钟，使用 CerebralDoc 后耗时为 14.22 ±3.64、4.94 ± 0.36 分钟。点击次数的差异更为巨大。\nDiscussion 提出了 CGPM 来修正血管分割错误，避免部分缺失血管。此外，作者们在宣武医院将 CerebralDoc 应用于临床场景，利用人工智能重建直接用于头颈部血管疾病诊断的 CTA 图像。\n结果表明：3D-CNN 深度学习算法可以在各种增强 CTA 扫描中以高灵敏度和特异性自动完成骨骼和血管分割（包括主动脉、颈动脉和颅内动脉）。\nCerebralDoc 重建的图片质量由两名临床经验丰富的放射科医师进行评估，合格率为 92.1%。\n以往将深度学习用于血管分割任务的研究大多集中在算法优化和模型结构方面，这些工作促使我们开发了一种有针对性的自动化临床血管分割工具(补充表2)。\n Y-Net：对 49 例磁共振血管造影（MRA）数据进行颅内动脉三维分割，测试集的精度为 0.819； HalfU-Net：脑血管病患者(TOF)-MRA 血管分割，DSC=0.88； MS-Net：全分辨率分割算法，提高了分割精度和分割精度，并显著降低了监督成本；  大部分的工作主要集中在特定血管区域的分割，而完整的头颈部 CT 扫描包括三种不同大小的血管（主动脉、颈动脉和颅内血管），这使得模型很难捕捉到跨尺寸级别的血管特征。\n另一方面，以前的大多数研究都是由相对较小的有限的数据集得来的，缺乏来自真实临床数据的验证。\n因此，通过优化的基于生理解剖学的 3D-CNN，在保证最大 patch 大小（256 × 256 × 256 体素）的前提下，建立一个全面的管道，实现头颈部 CTA 自动后处理（分割重建），具有重要的临床意义。\nU-Net 于 2015 年提出，之前的研究使用了这种策略，将大图像裁剪成小碎片，导致整体血管的结构特征丧失。考虑到血管完整性的重要性，本片论文的策略采用裁剪的方式将原始的 3D 切片输入到 3D CNN 中，并使用残差块 bottleneck-ResNet (BR) 对 U-Net 架构进行修改，从而自动找到优化的模型参数。\n连接三个不同大小层次的血管只仍然是一个挑战。基于生理解剖信息，我们将整个体积划分为三个区域（主动脉、颈动脉和颅内区域）用于网络，以更好地提取不同大小血管的特征。\n此外，还提出了 CGPM，通过对没有标注的原始 CTA 图像的输入数据、ResNet1、ResNet2 和 ResNet3 产生的当前分割结果以及标注的 CTA 图像进行学习，修正血管分割的错误，有效避免部分血管的缺失。\n由于从主动脉到颅骨范围广，且脑动脉血管曲折分支，很少有研究从头颈部 CTA 扫描中对血管分割进行研究（文中又列出一些）。本文的研究是一项多中心、大样本的研究，按照临床后处理过程，完成主动脉至颅内动脉的骨分割和血管分割的自动框架。\nCerebralDoc 在的评估结果在不同疾病中的整体合格率和表现与手工加工没有显著差异。\n3D-CNN 可以学习血管特征，更准确地改善和捕获血管信号。\nCerebralDoc 仍存在一定的局限性：\n（1）头颈部动脉严重异常起源（除左椎体直接产生于主动脉和严重狭窄起源外的异常血管常规）的患者图像不包括在训练和验证集中。因此，需要进一步的临床收集和测试来评估各种形式的颅骨和颈部血管的临床准确性。由于这种畸形的发生率相对较低，所以这个问题并不影响最终的整体结论；\n（2）未进行噪声图像进行测试；\n（3）CerebralDoc 目前只适用于 head and neck CTA images。\n总结，CerebralDoc 是一个实用的头颈部 CTA 重建系统。与现有的 CTA 图像优化重建技术相比，它提供了一种省时、不依赖主观性的方法，节省了成本，提高了效率。基于人工智能的标准化自动 U-Net 和可视化后处理图像的生成，CerebralDoc 有潜力融入目前的放射手术工作流程。\nMethods 数据的准备\n数据集包含 18766 例头颈部 CTA 原始造影增强图像，有中国 5 家三甲医院提供。所有头颈部 CTA 图像扫描包括主动脉、颈动脉和颅内动脉，Z 轴的切片为 561~967 slices。\n人工初步筛查排除了 507 例低质量的数据。所有患者 CTA 图像均为 DICOM 格式。\n本研究的初步获取了 2018 年 1 月至 2019 年 2 月期间，4 家不同的 CT 制造商的 18259 例患者的 14461128 张头颈部 CTA 扫描图像。\n训练集：16433 例患者\n测试机：1826 例患者\n训练时，将 16433 例患者分给两个模型，Model 1 进行骨分割（6387 例），Model 2 进行血管分割（10046）。\n在进行临床评估时，从宣武医院获取 2019 年 5 月至 2019 年 6 月期间额外的 152 例进行评估。这些数据依然是有标注的数据，两名经验丰富的临床医生，随机采用 5 分制评估影像质量是否满足诊断要求（医生不被告知来源手工标注还是 AI 输出）。\n最后，搜集临床应用集，主要用于验证模型在 7 ~ 11 月真实临床应用场景下的性能。\n放射科医生标注\n为了减少人为误差，采样分层标注的方法。具体来说：\n（1）首先使用 ITK-SNAP 对 18259 例样本进行预标注，分别标记骨、主动脉、颈动脉和颅内动脉区域，它们被分为四类。\n（2）由 10 名 2+ 年经验的技术人员矫正预标注图像；\n（3）由 2 名 5+ 年经验的放射科医生进行复核。如果两名医生意见不一致，则请 10+ 年经验的医生做最后决策；\n每例数据所需的平均处理时间约为 22 分钟。\n数据预处理和图像增强\n尽管标注非常专业和严谨，但是仍然会存在零散噪声。解决的方法是连通分量检测算法（a connected component detection algorithm），消除了散射噪声，提高了数据的鲁棒性。\n数据增强策略：\n 水平翻转 不高于 25° 的旋转 不高于 20 pixel 的水平/垂直平移 选择一个矩形区域，随机擦除像素、随机遮挡  经过增强后，数据量由 N 增大到 5N。为了增强鲁棒性，在训练时还加入了高斯噪声。\n模型开发\n自动分割框架包含三个级联的 ResU-Net，完成去骨、提取血管。ResU-Net2 是专门为 Bone edge optimization 设计的，CGPM 用于消除血管分割错误，有效避免部分或缺失的血管节段。\nResU-Net1 和 ResUNet2 模型负责在相同的高分辨率下使用语义分割对整个骨骼进行分割，并根据动脉的大小学习将血管划分为主动脉、颈动脉和颅内动脉的生理解剖结构特征。\n根据生理解剖结构，采用 ResU-Net3 模型实现血管分割。\n最后，提出的 CGPM 被应用于固定任何破裂的血管，并确保它们可以在形态学水平上被可行地预测和识别。\nCGPM 基于 3D-CNN，输入数据包括：\n original CTA image without labeling 由 ResU-Net1,2,3 生成的分割结果 标注的 CTA 图像  CGPM 能准确定位部分缺失的位置，完成对缺失血管的补充。\n消融学习，证明管道中每个部件的贡献。\n模型性能评估指标\n（1）DSC\n  Y: ground truth\n  $\\hat{Y}$: binary predictions from the neural networks\n  $|Y| + |\\hat{Y}|$: indicates the sum of each pixel value after calculating the dot product between the ground truth and the prediction.\n  （2）V-score\n、\n V-score 是根据血管的解剖和形态位置来计算的，用于揭示血管分割的连续性和完整性。 V-score 越高，血管位置越关键  （3）Recall\n被正确预测为阳性的病例所占的比例。\n临床评估\n3 =血管描绘良好，无间断，血管侧支重建良好，血管图像清晰；\n2 =正常血管轮廓，部分中断，狭窄程度与无遗漏的源图像相比有偏差；\n1 =整个血管中断，与源图像相比有严重的狭窄遗漏。\n2019年7月至11月，在宣武医院对 CerebralDoc 的临床应用价值进行了评估，主要从以下几个方面进行：\n 所有接受头颈部CTA检查的患者的覆盖率； 整体、每月、每天的后处理数量，以及成功推送到 PACS 的数量； 后处理所需的平均时间和 后处理的平均时间和 CerebralDoc 和技术员的点击次数（针对100个随机选择的病人）； 测试月份的临床生产力； 模型的错误率（探讨了原因）。  ===\n一些缩写\nvolume rendering (VR)\nmaximum intensity projection (MIP)\ncurve planar reconstruction (CPR)\ncurved multiple planar reformation (MPR)\nconnected growth prediction model (CGPM)\ncomputed tomography angiography (CTA)\nbottleneck-ResNet (BR)\ntime-of-flight(TOF)-MRA\nabsolute volume difference (AVD)\n===\n","permalink":"http://landodo.github.io/posts/20210912-rapid-vessel-segmentation/","summary":"论文简介 基于三维卷积神经网络的头颈部血管图像快速分割与重建 Fu, F., Wei, J., Zhang, M., Yu, F., Xiao, Y., Rong, D., Shan, Y., Li, Y., Zhao, C., Liao, F., Yang, Z., Li, Y., Chen, Y., Wang, X., \u0026amp; Lu, J. (2020). Rapid vessel segmentation and reconstruction of head and neck","title":"Rapid vessel segmentation and reconstruction of head and neck angiograms using 3D convolutional neural network."},{"content":"Structure Boundary Preserving Segmentation for Medical Image\nwith Ambiguous Boundary\n边界模糊医学图像的结构边界保持分割\n简介 韩国一个大学的文章。\n CVPR 2020 与医学图像分割有关 Cited by: 5 没有代码，没啥意思😭，想去看看它的一些细节！  摘要 医学图像分割存在两个关键性问题：\n 医学图像域结构边界的模糊 没有专业领域知识的细分领域的不确定性。  为了解决这两个问题，提出保存边界的分割框架（boundary preserving segmentation framework）。\n 边界关键点选择算法 边界保留模块（BPB）  提出了一种新型的形状边界感知评价器（SBE），它具有专家指出的地面真实结构信息。SBE 可以根据结构边界关键点向分割网络提供反馈。\nIntroduction 本论文主要解决如下两个问题：\n 大多数医学图像由于较低的图像分辨率和模糊的纹理，而存在边界模糊问题。不同于自然图像中的对象，医学图像由于分辨率较低，可能没有明显的构造边界。 在没有专家知识的情况下，很难自动预测正确的目标区域  提出了一个新型的全自动医学图像分割框架，保留目标区域的结构边界。\n 边界点选择算法：自动选择最适合目标区域的关键点，这些点放置于目标对象的结构边界上； 边界保留块（BPB）：将点编码到网络中，进一步利用结构边界信息； Shape Boundary-aware Evaluator(SBE)：将专家知识嵌入分割模型；  在训练阶段：它试图利用关键图来评估分割图中有多少结构边界得到了很好的保存； 根据专家标注的 Ground truth，将预测区域反馈给分割网络。    任何分割模型都可以融合 BPB 和 SBE 来更精确的分割目标区域。\n论文的贡献总结如下：\n Boundaries key point selection algorithm SEB：将分割图与边界关键点图是否重合反馈给分割网络。 SOTA  Boundary Key Point Selection Algorithm 首先使用传统的边缘检测算法从 ground truth 分割图中得到目标物体的边界，随机选择 n 个点；然后依次连接 n 个点构造边界区域。为了获得 ground truth 的边界关键点图，测量边界区域之间的重叠区域的数量，通过 IoU 得到 ground-truth 的分割图。\n最后，选择使得 IOU 值最大的边界点作为结构边界关键点。\n为什么需要 4 个 BPB。\n论文有 4 个关键点：\n（1）Boundary Key Point Selection Algorithm\n（2）Boundary Preserving Block (BPB)\n（3）Boundary Key Point Map Generator\n（4）Shape Boundary-aware Evaluator (SBE)\nExperimental Results 在两个医学图像分割数据集上进行验证，\n PH2 + ISBI 2016 dataset：皮肤病灶分割 Transvaginal Ultrasound (TVUS) dataset：子宫内膜分割   什么是传统的边界检测算法（conventional edge detection algorithm）？\n我觉得作者应该是从标注的 Mask 获取到边界，不然的话整张图像的边界点太多了。\nA computational approach to edge detection.\n Canny, J. (1986). A Computational Approach to Edge Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8, 679-698.\nCited by: 38734（这惊人的引用量，ResNet 也就 3K）\n 彩色图像转为灰度图像的计算公式，RGB2Gray\n$$Gray = R \\times 0.299 + G \\times 0.587 + B \\times 0.114$$\nmatlab rgb2gray 0.299 * R + 0.587 * G + 0.114 * B 没有想到边缘检测（Edge Detection）也是一个很有意思的方向\n 一个开放的数据集（检索）下载地址：https://gas.graviti.com/open-datasets\nhttps://gas.graviti.cn/open-datasets\n  YOLOP：这是第一个可以在嵌入式设备 Jetson TX2上以 23 FPS 速度实时同时处理目标检测/可行驶区域分割和车道线检测这三个视觉感知任务并保持出色精度的工作。\n NVIDIA Pascal™ GPU 架构配有 256 个 NVIDIA® CUDA® 核心和高达 8GB 的内存。\n我可以考虑在这个嵌入式设备上部署 YOLOP。师兄的这个应该是 NVIDIA® Jetson Nano™，https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-nano/education-projects/。我下载了它的文档。\nYOLOP 可以同时完成 3 个任务： traffic object detection（交通目标检测）、 drivable area segmentation（可驾驶区域分割）和 lane detection（车道检测）。\n 本周看 YOLO 系列的论文，并且做好笔记！\n问题一：怎么知道物体的中心落在 grid cell 中？如何计算物体的中心？\n😘 中心点其实非常好求，去看看源代码，主要去看怎么处理 label 的那部分。\ncenter_x= (bbox[0]+bbox[2])*0.5 center_y= (bbox[1]+bbox[3])*0.5 https://zhuanlan.zhihu.com/p/183261974\n非极大值抑制，抑制的是与之重合的框。具体操作如下：\n 网络输出很多的预测框，选择置信度最高的框，肯定包含目标，其作为第一个框； 利用 IoU，把与第一个框重合的其他框抑制掉； 剩下还没有被抑制掉的框，取置信度最高的，得到第二个框；抑制重合框； 剩下的没有被抑制的框，取最高，得到第三个框，抑制重合框； ……直到没有剩下的框，结束。  多目标检测时，使用非极大值抑制。\nYOLOv1 输出为 shape=$7 \\times 7 \\times 30$。所以标签 label 的 shape 应该为 [batch, 7, 7, 30]。\nlabel 和 output 的 size 为：[batch_size, 7, 7, 30]。每个 output[bi, wi, hi] 是一个 30 维向量。\nYOLOv1 的检测头就是最好的 2 个全连接层，参数量很大，存在很大的改进空间。YOLOv1 一共预测 49 个目标，一共 98 个框。\nYOLOv2 归一化后的预测值为一个很小的偏移量，有利于神经网络的学习，并且使用偏移量会使得训练过程更加稳定。\nAnchor 是从数据集中统计得到的。\n什么是 DarkNet？\n答：这是 YOLO 作者自己写的一个深度学习框架。\nYOLOv3，3 个分支，32 倍下采样（大目标）、16 倍下采样、8 倍下采样（小目标）。\nLoss = 定位损失 + 置信度损失 + 分类损失\n什么是 geo_loss？\ngeo_loss + confidence_loss + class_loss。\n为什么 Head 变得越来越复杂了？\n答：因为特征提取网络变强了，能够支撑检测头做更加复杂的操作。\n可视化模型：https://netron.app/\n深入浅出Yolo系列之Yolov3\u0026amp;Yolov4\u0026amp;Yolov5\u0026amp;Yolox核心基础知识完整讲解：https://zhuanlan.zhihu.com/p/143747206\nyolo系列之yolo v3【深度解析】：https://blog.csdn.net/leviopku/article/details/82660381\n目标检测难的地方在于 pipeline 很长，细节很多，“the devil is in the detail”。\nYOLO 复现还得看大厂写的代码：https://github.com/Tencent/ObjectDetection-OneStageDet\n可以看看 DarkNet 的源码实现！\n1.5K 的解读，有非常多的中文注释：https://github.com/hgpvision/darknet\n17.2K，在官方的基础上添加了很多的新特性、新算法，新 backbone，是最流行的目标检测开源项目之一：https://github.com/AlexeyAB/darknet\n21.3K（作者源码）：https://github.com/pjreddie/darknet\n","permalink":"http://landodo.github.io/posts/20210831-structure-boundary-preserving-segmentation/","summary":"Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary 边界模糊医学图像的结构边界保持分割 简介 韩国一个大学的文章。 CVPR 2020 与医学图像分割有关 Cited by: 5 没有代码，没啥意思😭，想去看看它的","title":"Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary"},{"content":"目标检测任务（1） ⛳本报告主要是针对目标检测任务中正负样本不平衡、难易样本不平衡这两个问题进行简要讨论。\n论文简介  Lin, T., Goyal, P., Girshick, R.B., He, K., \u0026amp; Dollár, P. (2017). Focal Loss for Dense Object Detection. 2017 IEEE International Conference on Computer Vision (ICCV), 2999-3007.\n   Focal Loss\n  ICCV 2017, Best student paper.\n  https://arxiv.org/abs/1708.02002\n  1. 背景 以 2014 年为分界，目标检测的发展历程可以分为两大部分：传统目标检测时期、基于深度学习的目标检测时期。\n Zou, Z., Shi, Z., Guo, Y., \u0026amp; Ye, J. (2019). Object Detection in 20 Years: A Survey. ArXiv, abs/1905.05055.\n 在基于深度学习的目标检测算法中，又可以分为单阶段（One/Single-stage）和两阶段（Two-stage）两种大类。当然还有多阶段（Multi-stage），但是其速度和精度都比较低，已经被淘汰了。\n1.1 One-Stage and Two-stage One-stage 和 Two-stage 的主要区别在于受否存在 Region Proposal（可能包含待检物体的预选框）操作。以 Two-stage 方法中的代表 Faster RCNN 为例，算法会先生成候选框（Region proposals，可能包含物体的区域），然后再对每个候选框进行分类和修正位置；而 One-stage 算法会直接在网络中提取特征来预测物体分类和位置。\n两种方法都存在各自的优缺点。一般来说，One-stage 方法在速度上存在优势，但是在精度上会差于 Two-stage，主要原因可以总结为：正负样本不平衡（和难易样本不平衡）造成了 One-stage 方法在精度上的劣势。具体分析如下：\n  One-stage 网络最终学习的 Anchor 有很多，但是只有少数 Anchor 对最终网络的学习是有利的，而大部分 Anchor 对最终网络的学习都是不利的，这部分的 Anchor 很大程度上影响了整个网络的学习，拉低了整体的准确率；\n  Two-stage 网络最终学习的 Anchor 虽然不多，但是背景 Anchor 也就是对网络学习不利的 Anchor 也不会特别多，它虽然也能影响整体的准确率，但是肯定没有 One-stage 影响得那么严重，所以它的准确率比 One-stage 肯定要高。\n  对于正负样本不平衡问题，是比较好解决的，也存在不少的现有方法。Focal Loss 的提出，主要针对难易样本的不平衡问题。有了 Focal Loss，训练过程关注对象的次序为：（正/难） \u0026gt; （负/难） \u0026gt; （正/易） \u0026gt; （负/易）。该损失函数通过抑制那些容易分类样本的权重，将注意力集中在那些难以区分的样本上，有效控制正负样本比例，防止失衡现象。\n============================================================\n在开始介绍 Focal Loss 之前，我补充一些目标检测的基础知识，我也是才刚开始学习目标检测。\n（1）目标检测中的各种“框”\n ground truth：标注框 Anchor：人为设置的初始先验框 proposal：RPN 的输出（可能包含物体的候选框），即对 Anchor 第一次做回归得到的结果 RoI：RPN 阶段输出的 Proposal 经过排序取 topK，然后做 NMS 取一定数量的框，用于第二阶段的再次精修 bounding box：proposal 经过再次精修后的预测框，由于计算 AP，AP 指的是 bounding box AP。  （2）目标检测任务的评估指标 mAP\nhttps://www.zhihu.com/question/53405779/answer/993913699\nAP：PR 曲线下面积，先考虑计算 AP，即一个类别。\n 7 张图像（假设是一个 Batch），15 个 ground truth，24 个预测框 （1）计算预测框是 TP or FP（计算 bbox 与 Ground truth 的 IoU，根据阈值判断），如果一个 Ground Truth 有多个预测框，则 IoU 最大为 TP，其他为 FP； （2）根据置信度从大到小排序所有的预测框； （3）计算 Precision = TP / (ACC_TP + ACC_FP)、Recall = TP / (all grouth truth)；  ACC 表示累加，all ground truth 是一个固定值   （4）绘制 PR 曲线； （5）计算曲线下的面积，11 个点 [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] 的插值进行计算（10 个矩形的面积之和） （6）所有类别的 AP 计算都分别出来，然后求取平均得到 mAP。  2. Focal Loss 先从二分类的交叉熵开始：\n $y \\in {\\pm 1}$ 代表 ground truth（真实值） $p \\in [0, 1]$ 表示模型输出标签为 1 的概率（预测值）  定义一个 $p_t$：\n则交叉熵可以写为 $CE(p, y) = CE(p_t) = -log(p_t)$\n交叉熵损失存在一个问题，如 Figure 1.，即使那些很容易 easily classified 的样本（$p_t \u0026raquo; 0.5$），仍会造成很显著的损失值。当这些 easy example 数量庞大时，其累计起来的损失可能会远远大于（overwhelm）那些 rare class。\n 我认为这很类似于政治上的民主暴政。\n 2.1 Balanced Cross Entropy 解决正负样本不平衡的常用方法是对类别 1 引入权重因子 α∈[0,1]，对类别 -1 引入权重因子 1-α。得到 α-balanced CE loss：\n即，\n2.2 Focal Loss Easily classified negatives comprise the majority of the loss and dominate the gradient. α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples.\n因此，Focal Loss 的主要目标是 down-weight easy example，使得能够 focus training on hard example。Focal Loss 定义如下：\n $(1 - p_t)^{\\gamma}$ 是 modulating factor（调节因子） $\\gamma$ 是一个可调节的超参数，focusing parameter（聚焦参数）  Focal Loss 有两个性质：\n（1）当一个样本出现了错误分类，且 $p_t$ 非常小，则 $(1 - p_t)^{\\gamma}$ 非常接近 1，Loss 不受影响；当 $p_t$ 接近于 1 时，$(1 - p_t)^{\\gamma}$ 接近 0，对于 well-classified examples 的 Loss 将会降低权重（down-weighted）。因此模型的 Loss 就集中在那些错误分类样本上了（hard example）。\n（2）Focusing parameter $\\gamma$ 用于调整简单的样本（easy examples, well-classified examples）的 Loss 降低权重（down-weighted）的速率。$\\gamma=0$ 时，FL == CE；$\\gamma = 2$ 是实验得到的最好值。\nIntuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss.\n例如：\n $\\gamma = 2$，一个样本的分类结果为 $p_t = 0.9$（这就是一个 easy example），则 $(1 - 0.9)^{2} = 0.01$。Focal Loss 比 CE 小 100 倍； $p_t = 0.968$ 时，FL 比 CE 小近 1000 倍。 同理，$p_t = 0.4$ 时（hard example），则 $(1 - 0.4)^{2} = 0.36$。相当于变相给错误分类的难样本的 Loss 增加了权重。“increases the importance of correcting misclassifified examples”  为了平衡正负样本，使用 α 权重，得到最终的 Focal Loss 表达式：\nFL 更像是一种思想，其精确的定义形式并不重要。\n在 Two-stage 方法中，对于正负样本不平衡问题，主要是通过如下方法缓解：\n （1）object proposal mechanism：reduces the nearly infifinite set of possible object locations down to one or two thousand. （2） biased sampling：1:3 ratio of positive to negative examples.  同时，在模型初始化时，可以加入一下先验知识，可以缓解训练初期的不稳定现象。FL 通过直接通过损失函数解决！\nFocal Loss 的代码可以参考 MMdetection：https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/losses/focal_loss.py\n3. RetinaNet 为了验证 Focal loss 的有效性，设计了一个叫 RetinaNet 的网络进行评估。实验结果表明，RetinaNet 能够在实现保持 one-stage 速度优势的基础上，在精度上超越所有（2017 年）two-stage 的检测器（ achieves state-of-the-art accuracy and run time on the challenging COCO dataset）。\nRetinaNet 的卷积过程用的是 ResNet，上采样和侧边连接还是 FPN 结构。通过主干网络，产生了多尺度的特征金字塔。然后后面连接两个子网，分别进行分类和回归。\n4 实验结果 作者做了很多消融学习，可以总结如下：\n 作者有很多卡； $\\alpha$ 和 $\\gamma$ 这两个超参数是互相影响的； Focal loss 的威力还是很大的； $\\gamma=2, \\alpha=0.25$ 时，ResNet-101+FPN 作为 backbone 的结构有最优的性能；  下图是收敛模型中不同 γ 值的正负样本归一化损失的累积分布函数。\n改变 γ 对正样本的损失分布的影响很小。然而，对于负样本，增加 γ 会使得模型几乎所有的注意力从负样本上离开，实现了 down-weight easy example。\nFocal Loss 使得 One-stage 方法在精度上超越了 Two-stage 方法。\nFocal Loss 的缺点：在速度上还存在很大的改进空间。\n总结   One-stage 方法相比于 Two-stage 方法，在精度稍有劣势。研究发现，是正负样本不平衡和难易样本不平衡这两个问题所导致的；\n  Focal Loss 函数通过抑制那些容易分类样本的权重，将注意力集中在那些难以区分的样本上，有效控制正负样本比例，防止失衡现象。\n  Focal Loss 的主要目标是 down-weight easy example，使得模型能够 focus training on hard example。\n  具体做法是，对于难易样本不平衡问题，引入 modulating factor $(1 - p_t)^{\\gamma}$， Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss.\n  为了验证 Focal Loss 的有效性，设计了 RetinaNet 用于实验评估。\n  RetinaNet 能够在实现保持 one-stage 速度优势的基础上，在精度上超越所有（2017 年）two-stage 的检测器（ achieves state-of-the-art accuracy and run time on the challenging COCO dataset）。\n  RetinaNet 也存在缺点，其在速度上仍有很大的改进空间。\n  扩展学习 Focal Loss 存在缺点：\n 让模型过多关注那些特别难分的样本肯定是存在问题的，样本中有离群点（outliers），可能模型已经收敛了但是这些离群点还是会被判断错误，让模型去关注这样的样本，可能对最后的结果造成不利的影响； $\\alpha$ 和 $\\gamma$ 互相影响，全凭经验得到（不同的数据集都要寻找的最佳的 $\\alpha, \\gamma$，代价昂贵）； 速度上仍存在改进空间。  （1）GHM(gradient harmonizing mechanism) 解决了上述前两个问题。Focal Loss 是从置信度 p 的角度入手衰减 Loss，而 GHM 是一定范围置信度p的样本数量的角度衰减 Loss。\n（2）Generalized Focal Loss，不会带来额外的 Cost，提升 1% 的 AP。\n","permalink":"http://landodo.github.io/posts/20210827-focal-loss/","summary":"目标检测任务（1） ⛳本报告主要是针对目标检测任务中正负样本不平衡、难易样本不平衡这两个问题进行简要讨论。 论文简介 Lin, T., Goyal, P., Girshick, R.B., He, K., \u0026amp; Dollá","title":"Focal Loss for Dense Object Detection"},{"content":"anchor 什么是 anchor？\n anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此使得图像上密集铺满了许多 anchor，从而覆盖到包含物体的所有位置区域。\n 早期的方法：金字塔多尺度 + 滑动窗口，逐尺度逐位置判断“这个尺度的这个位置处有没有认识的目标”，非常的耗时。\n基于 anchor 的方法：预设一组不同尺度、不同位置的固定参考框，覆盖几乎所有位置和尺度，每个参考框检测与其交并比大于阈值的目标。anchor 将问题转换为“这个固定的参考框中有没有认识的目标，目标偏离参考框多远”。\nRPN是一个conv3x3+两个并列的conv1x1，一边预测anchor中是否包含目标，一边预测目标框偏离固定anchor多远。\nfeature map上每个位置设置9个参考anchor，这些大约能覆盖边长70~768的目标。下图是Faster R-CNN论文中各个 anchor 形状训练后学习到的平均proposal大小。\n特征提取部分：\nimport torch import torchvision.models.vgg as vgg import torch.nn as nn  if __name__ == \u0026#39;__main__\u0026#39;:   model = vgg.vgg16(pretrained=False)  featrue = list(model.features)[:30]  featrue_extra = nn.Sequential(*featrue)   input = torch.randn((4, 3, 256, 256))  output = featrue_extra(input) input.shape = torch.Size([4, 3, 256, 256])\noutput.shape = torch.Size([4, 512, 16, 16])\nbackbone 提取的特征图（Featuremap）相对于网络输入图像尺寸缩小了 16 倍。因此，featuremap 中的 1 个像素点就相当于输入图像的 16 个像素点。或者说，featuremap 中的 1x1 区域覆盖了输入图像的 16x16 区域。即 featuremap 中的每个像素点都对应的覆盖了输入图像的区域。\n一个点对应 9 个 anchor。（3 种尺度、3 种宽高比的排列组合）\nanchor 坐标有可能出现负数的情况。\n计算 featuremap 左上角(0,0) 对应的 9 个 anchor 的中心点坐标和长宽之后，其余的点坐标/长宽可以通过平移的方式得到。\n分类：\n 3x3 卷积，（融合周围 3x3 的区域，更鲁棒？） 1x1 卷积，（融合通道） ？  回归：\n 1x1 卷积将通道映射到 36=4x9。4 表示？  对分类和回归结进行后处理，生成 RoI（Region of Interest），也称为了 Proposal layer，对 RPN 输出的分类和回归结果进行后处理（如 非极大值抑制 等），得到网络任务包含物体的区域——RoI。\nRoI Pooling 的作用是将不同尺寸的各个 RoI 都映射到相同大小。\nRoI Pooling：如何划分 7x7 个 bin？当无法整除的情况，有人提出了 RoI Align 和 Precise RoI Pooling。\n存在的缺点：每个 bin 中只有一点贡献了梯度，忽略了大部分点的信息。\n将 RoI Pooling 后的结果 flatten 成为 vector，输入全连接层进行分类和回归，对应输出的神经元个数分别为物体类别数（n_classes）和每个类别对应的 bbox（n_classes x 4）。\nRoIHead 的输出不是预测结果的最终形态，还需要进行一些后处理。\n 将网络输出缩放至原图尺寸（不是网络输入的尺寸）； 对回归的结果去归一化（乘 std, 加 mean），结合 RoIs 的位置和大小计算出 bbox 的位置（左上角坐标和右下角坐标），并且裁剪到原始尺寸范围内； 选择置信度大于阈值的矩形框，最后再使用非极大值抑制剔除重叠度高的 bbox 得到最终的结果。  还是搞不太懂，留一段时间吧。先看看 Focal Loss\n \u0026mdash; 分割线 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\n😡两个问题：\n （1）正负样本不平衡 （2）难易样本不平衡  二分类问题的标准 Loss 是交叉熵：\n$$L_{ce} = -ylog \\check{y} - (1-y)log(1 - \\check{y})$$\n 当 y = 1 时，$L = -log\\check{y}$。（如果网络的输出预测值 $\\check{y}$ 越接近于 1，则 loss 越小） 当 y = 0 时，$L = -log(1-\\check{y})$（如果网络的预测值 $\\check{y}$ 越接近于 0，则 loss 越小）  -log 的函数图像：\n“硬截断”的 Loss：$L^* = \\lambda(y, \\check{y}) \\cdot L_{ce}$，其中：\n (y == 1) 且 $\\check{y} \u0026gt; 0.5$ 或 (y == 0) 且 $\\check{y} \u0026lt; 0.5$ 时，$\\lambda(y, \\check{y}) = 0$ 其他情况，为 1  正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，就都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。\n上面的 Loss 存在问题，即它只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5。\n进一步的优化是将“硬截断”的 Loss “光滑化”，可导。\nGHM(gradient harmonizing mechanism) 是基于 Focal Loss 的改进。\n 公式(2)解决了正负样本的不平衡：\n  公式(3)解决了难易样本的不平衡：\n  结合 (2)(3) 得到 Focal Loss：\n 😍GHM：不应该过多关注易分样本，但是特别难分的样本（噪声、离群点）也不应该过多的关注。\n梯度密度 $GD(g)$：单位梯度模长g部分的样本个数。\n🤑关于目标检测任务中正负样本不平衡、难易样本不平衡问题的学习笔记\n 5分钟理解Focal Loss与GHM——解决样本不平衡利器\nhttps://zhuanlan.zhihu.com/p/80594704\nAAAI 2019：把Cross Entropy梯度分布拉‘平’，就能轻松超越Focal Loss\nhttps://zhuanlan.zhihu.com/p/55017036\n Focal Loss （RetinaNet）\none-stage 为什么会差于 tow-stage：\n   one-stage 网络最终学习的anchor有很多，但是只有少数anchor对最终网络的学习是有利的，而大部分anchor对最终网络的学习都是不利的，这部分的anchor很大程度上影响了整个网络的学习，拉低了整体的准确率；\n  two-stage 网络最终学习的anchor虽然不多，但是背景anchor也就是对网络学习不利的anchor也不会特别多，它虽然也能影响整体的准确率，但是肯定没有one-stage影响得那么严重，所以它的准确率比one-stage肯定要高。\n     MxN 大小的图像经过 Conv layers 得到 (M/16)x(N/16) 的 Feature map 上面分支 1x1 卷积，得到的输出为 WxHx18（W=M/16，H=N/16）   FPN：Feature pyramid networks for object detection.\n对于每张图像，detectors 评估 10^4 - 10^5 个候选位置，但只有少数位置包含对象。这种不平衡会造成两个问题：\n （1）训练是低效的，因为大多数位置都是简单的负样本，没有提供有用的学习信号。 （2）简单的负样本可以压倒训练，并导致退化模型。  R-CNN -\u0026gt; Fast R-CNN -\u0026gt; Faster R-CNN\nR-CNN: Regions with CNN features\n😁RCNN 算法流程可以分为 4 个步骤：\n 候选区域的生成：Selective Search 算法 对每个候选区域，调整为相同的大小（warped region），使用深度网络提取特征 特征送入每一类的 SVM 分类器，判定类别 使用回归器精细修正候选框位置  R-CNN 框架（4 部分）\n  Region Proposal(Selective Search)\n  Feature extraction(CNN)\n  Classification(SVM)\n  Bounding-box regression(regression)\n  R-CNN 存在的问题：\n 测试速度慢：候选框之间存在大量重叠，提取特征冗余 训练速度慢：过程繁琐 训练所需空间大  Fast R-CNN 与 R-CNN 相比，训练时间快 9 倍，测试推理时间快 213 倍，准确率从 62% 提升至 66%。\n😁Fast R-CNN 算法流程的 3 个步骤：\n 一张图像生成 1K~2K 个候选区域（Selective Search） 将图像输入网络得到相应的特征图，将 SS 算法生成的候选框投影到特征图上获得相应的特征矩阵（1k-2k 的候选框各自的 feature map） 将每个特征矩阵通过 ROI pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。  Fast-RCNN 将整张图像送入网络，紧接着从特征图像上提取相应的候选区域。这些候选区域的特征不需要再重复计算。\nROI Pooling 不限制输入图像的尺寸。\nMulti-task loss\nL = 分类损失 + 边界框回归损失\nFast CNN 框架（2 部分）\n  Region proposal(Selective Search)\n  Feature extraction/Classification/Bounding-box regression(CNN)\n  Faster RCNN：将 Region proposal 也融合进网络中，成为端到端的架构。\nFaster R-CNN 推理速度 5fps（包括候选区域的生成）\n😁Faster R-CNN 算法流程的 3 个步骤\n 将图像输入网络得到相应的特征图 使用 RPN 结构生成候选框，将 RPN 生成的候选框投影到特征图上获得相应的特征矩阵， 将每个特征矩阵通过 ROI pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。  RPN + Fast R-CNN\nRegion Proposal Network 替代 SS 算法。\n","permalink":"http://landodo.github.io/posts/20210824-object-detection-anchor/","summary":"anchor 什么是 anchor？ anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此","title":"关于 Object Detection Anchor 的学习笔记"},{"content":"医学影像基础 现行医学影像设备所得到的医学影像在许多方面还存在不尽如人意之处。例如，某些影像的==空间分辨率不够高==，某些影像的==信噪比不够好==，某些影像==需要多模态融合==才能更好地进行诊断，等等。\n医学影像学部分主要涵盖了 X 线、CT、MRI、超声、核素显像五类医学影像。\n0 医学图像中的一些专业术语   X-ray：X 射线成像\n  Ultrasound：超声成像\n  Positron emissions tomography, PET：正电子发射断层扫描成像\n  Computed tomography, CT：计算机断层扫描成像\n  Magnetic Resonance Imaging, MRI：磁共振成像\n   缺点：贵、慢、吵\n     Computer-aided Diagnosis, CAD：计算机辅助诊断\n  MRI T1, T2\n   MRI 可以实现多变的图像对比度，该过程的实现是通过使用不同的脉冲序列和改变成像参数对应纵向松弛时间（T1）和横向松弛时间（T2），T1 加权和 T2 加权成像的信号强度与特定组织的特征有关。MR 成像中，图像的对比度依赖于相位对比脉冲序列参数，最常见的脉冲序列是 T1 加权和 T2 加权自旋回波序列．身体的 MR 成像是为了观察大脑、肝脏、胸、腹部和骨盆的结构细节，这有利于诊断检测或治疗。\n     Medical Image Computing and Computer Assisted Intervention, MICCAI：医学图像计算与计算机辅助干预\n  Emission Computed Tomography, ECT：发射型计算机断层成像技术\n  Positron Emission Tomography, PET：正电子发射型断层成像\n   属于核素显像\n     Optical Coherence Tomography, OCT：相干光断层成像\n   眼底血管图像\n     Cone-Beam Computed Tomography, CBCT：锥状射束计算机断层扫描\n  1. X 射线 X 线是由高速运行的电子群撞击物质突然受阻时产生的。撞击发生能量转换，其中仅约 1% 的能量形成 X 线，其余 99% 左右的能量则转换为热能，由散热设施散发。\nX 线与临床医学成像有关的主要特性如下。\n（1）穿透\n 波长越短，X 线的穿透作用越强 X 线对人体各种组织结构穿透力的差别是X线成像的基础。  （2）荧光\n X 线作用于荧光物质，使波长短的 X 线转换成波长较长的可见荧光。  （3）感光\n（4）电离\n（5）生物效应\n 生物细胞在一定量的 X 线照射下，可产生抑制、损害甚至坏死，称为 X 线的生物效应，是放射治疗学的基础  X 线成像原理\n1）X 线能使人体在荧光屏上或胶片上形成影像，必须具备三个基本条件：① X 线要具备一定的==穿透力==；② 被穿透的组织结构必须==存在密度和厚度的差异==，从而导致穿透物质后剩余 X 线量的差别；③ 有差别的剩余 X 线量，仍为不可见的，必须经过载体（如X线片、荧屏等）的过程才获得有黑白对比、层次差异的X线影像。\n2）人体组织结构根据其密度的高低及其对X线吸收的不同可分三类：① ==骨骼==比重高、吸收 X 线量多，X 线片上骨骼部位显示白色，称为高密度影像；② ==软组织==包括皮肤、肌肉、结缔组织等，彼此之间密度差别不大，X 线片上显示灰白色，称为中等密度影像；脂肪及气体，脂肪组织较一般软组织密度低，在 X 线片上显示灰黑色；③ ==气体==吸收 X 线最少，在 X 线片上呈深黑色，称为低密度影像。\n⛳吸收 X 射线越多（密度越高），CT 图像越亮！\nX 线成像技术\n 普通 X 线摄影 数字化 X 线摄影技术：计算机 X 线摄影（computed radiography，CR）和数字 X 线摄影（digital radiography，DR） 特殊 X 线摄影  X 线成像的临床应用\n 胸部病变 腹盆部病变 骨关节病变  2. CT 诊断 CT 解决了普通 X 线摄影不能解决的很多问题。CT 图像是真正的断层图像。CT 图像相对空间分辨率高，解剖关系明确，病变显影更好。\n基本概念\nCT 图像是真正的断面图像，它显示的是人体某个断面的组织密度分布图。CT 以 X 线作为投射源，由探测器接收人体某断面上的各个不同方向上人体组织对 X 线衰减值，经模/数转换输入计算机，通过计算机处理后得到扫描断面的组织衰减系数的数字矩阵，然后将矩阵内的数值通过数/模转换，用黑白不同的灰度等级在荧光屏上显示出来。CT 图像具有图像清晰，密度分辨率高，无断面以外组织结构干扰等特点。\nCT 图像是人体某一部位有一定厚度的体层图像。\nCT 值：体素的相对 X 线衰减度（即该体素组织对 X 线的吸收系数），表现为相应像素的 CT 值，单位名称为Hu（Hounsfield Unit，Hu）。规定以水的CT值为0Hu，骨皮质最高，为1000Hu。人体组织的CT值界限可分为2000个分度，上界为骨的CT值（1000Hu），下界为空气的CT值（-1000Hu）\n人体组织CT值范围有2000个分度（-1000～+1000），如在荧屏上用2000个不同灰阶来表示2000个分度，由于灰度差别小，人眼不能分辨（一般仅能分辨16个灰阶）。为了提高组织结构细节的显示，使CT值差别小的两种组织能够分辨，则要采用不同的==窗宽==来观察荧屏上的图像。\n伪影：是指在被扫描物体中并不存在的而图像中却显示出来的各种不同类型的影像。病人不自主运动及病人躁动可产生伪影。另外，病人体内高密度的异物也可形成伪影。\n成像原理\n（1）X线扫描数据的收集和转换\nX线射入人体后，因被人体吸收而衰减，其衰减的程度与受检层面的组织、器官和病变的密度（原子序数）有关，密度越高，对X线衰减越大。\n（2）扫描数据处理和重建图像\n（3）图像的显示及贮存\nCT 临床应用\n 中枢神经系统病变 头颈部病变 胸部病变 腹盆部病变  3. MRI 诊断 MRI 技术原理和基本概念\n通过对静磁场中的人体施加某种特定频率的视频脉冲，使人体组织中的氢质子受到激励而发生磁共振现象，当终止射频脉冲后，质子在弛豫过程中感应出MR信号；经过对MR信号的接收、空间编码和图像重建等处理过程，即产生MR图像，这种成像技术就是MRI技术。\n人体内氢核丰富，而且用它进行磁共振成像的效果最好，因此，目前MRI常规用氢核来成像。\nMRI 图像特点\n（1）多参数成像：成像参数主要包括 T1、T2 和质子密度等，\n（2）多方位成像：MRI可获得人体轴位、冠状位、矢状位及任意倾斜层面的图像，有利于解剖结构和病变的三维显示和定位。\n（3）流动效应\n（4）质子弛豫增强效应与对比增强\nMRI 的优点和限制\nMRI的优点：（1）无X线电离辐射，对人体安全无创；（2）图像对==脑和软组织==分辨率极佳，解剖结构和病变形态显示清晰；（3）多方位成像，便于显示体内解剖结构和病变的空间位置和相互关系；（4）多参数成像；（5）除可显示形态变化外，还能进行功能成像和生化代谢分析。\nMRI 的限制：（1 ）对带有心脏起搏器或体内有铁磁性物质的患者不能进行检查；（2）需要监护设备的危重患者不能进行检查；（3）对钙化的显示远不如CT，难以对以病理性钙化为特征的病变作诊断；（4）常规扫描时间较长，对胸腹检查受限；（5）对质子密度低的结构如肺和骨皮质显示不佳；（6）设备昂贵，尚未普及。\nMRI成像中的伪影\nMRI图像中的假影像为伪影，常见的伪影有装备伪影、运动伪影、金属异物伪影等。\nMRI的临床应用\n 中枢神经系统病变 头颈部病变 胸部病变 腹盆部病变 骨关节病变  4. 超声诊断 利用超声在人体器官组织传播过程中产生透射、折射、反射等的信息，加以接收、放大和处理形成曲线的方法，称为超声诊断。超声波在生物组织中的传播规律是超声诊断的基础，对超声诊断最重要的生物组织是软组织和血液。当超声经过不同性质的软组织和血液或当组织发生病理变化时，其在组织器官中的传播发生相应的改变，最终体现为超声曲线或图像上的差异。\n超声临床应用\n 乳腺病变 甲状腺超声诊断 腹部病变超声诊断  5. 核素显像诊断 核医学（nuclear medicine）显像是显示放射学核素标记的放射性药物在体内的分布图。放射性药物根据自己的代谢特点和生物学特性，能特异地分布于体内特定的器官或病变组织，并参与体内的代谢，标记在放射性药物分子上的放射性核素由于放出射线能在体外被检测。\n显像类型与特点\n（1）根据影像获取的状态分为静态显像（static imaging）和动态显像（dynamic imaging）\n（2）局部显像（regional imaging）和全身显像（whole body imaging）\n（3）早期显像（early imaging）和延迟显像（delay imaging）\n（4）热区显像（hot spot imaging）和冷区显像（cold spot imaging）\n（5）静息显像（rest imaging）和介入显像（interventional imaging）\n临床应用\n 骨骼系统 内分泌系统 泌尿系统  PET 成像 PET显像是利用人体正常结构组织含有的必需元素，葡萄糖、氨基酸、胆碱、胸腺嘧啶、受体的配体及血流显像剂等药物为显像剂，以解剖图像方式从分子水平显示机体及病灶组织细胞的代谢、功能、血流、细胞增殖和受体分布状况等，为临床提供更多的生理和病理方面的诊断信息\n 向人体内注射带有放射性核素的药剂（例如18F-氟代脱氧葡萄糖）后，在一段时间内肿瘤区域的放射性核素浓度便会明显高于正常组织。通过捕捉这些放射性核素发出的信号，我们便能准确地定位肿瘤的区域与大小，方便医生开展进一步的诊疗工作。\n PET显像所用的放射性药物是一类采用正电子核素标记的显像剂（正电子核数多为机体组成的基本元素）。\nPET/CT：由于CT提供的是人体解剖结构的信息，因此称为结构成像；PET提供的主要是目标区域的功能信息，称为功能成像。两者的融合能够得到更多的病理信息。\n","permalink":"http://landodo.github.io/posts/20210806-medical-image-basic/","summary":"医学影像基础 现行医学影像设备所得到的医学影像在许多方面还存在不尽如人意之处。例如，某些影像的==空间分辨率不够高==，某些影像的==信噪比不","title":"医学图像基础"},{"content":"节点聚类问题的总结和思考 摘要 聚类作为经典的无监督学习算法，在数据挖掘/机器学习等领域有着广泛地应用。聚类是针对给定的样本，依据它们的特征的相似度或距离，将其归并到若干“类”或“簇”中。其目的是通过得到的类或簇来发现数据的特点，或对数据进行处理。本文的主要内容分为两个部分，在第一部分，对 K-均值聚类、凸正则化聚类、非凸正则化聚类、数据驱动的正则化聚类、子空间聚类等聚类优化方法进行简要总结。在第二部分，研究利用图神经网络强大的结构捕获能力来提升聚类算法的精度，即采用深度神经网络来学习聚类友好表征。最后，对这两部分进行简要的总结和思考。\n1 聚类优化方法 物以类聚，人以群分。经过了几十年的研究，学者们提出了大量的聚类方法。聚类分析计算方法可以作如下分类，以提供一个相对有组织的描述。\n 划分式聚类算法 层次化聚类算法 基于密度的聚类算法 基于网格的聚类算法 基于模型的聚类算法  接下来对 K-均值聚类、凸正则化聚类、非凸正则化聚类、数据驱动的正则化聚类、子空间聚类等聚类优化方法进行简要总结。\n1-1 K-均值聚类 K-均值聚类方法最早由 Steinhaus 于 1955 年提出，在不同的学科领域被广泛研究和应用，K-均值聚类方法是数据发掘领域的十大经典算法之一。对于给定的数据样本点 $a_1, a_2, \u0026hellip;, a_n$，K-均值聚类通过求解如下优化模型实现：\n1.2 凸正则化聚类方法 Pelckmans 于 2005 年首次提出凸正则聚类方法，其优化模型如下：\n其中，$\\lambda$ 是正则化参数， $\\Vert \\cdot \\Vert_q$ 是 $l_q$ 范数。当数据量较大时，计算正则化项的值需要消耗巨大的计算量，因此，研究人员基于kNN 策略提出了带权重的正则化聚类模型：\n常见的权重 $w_ij$ 的计算方法有：k-最近邻和k-最近邻+高斯核。\n1.3 非凸正则化聚类方法 Pan 等人于 2013 年提出使用截断 Lasso 惩罚函数作为正则化的非凸正则化聚类方法，以消除凸正则化函数导致的估计偏差。其优化模型如下：\n其中 $\\tau$ 是给定的常数，截断 Lasso 可以当做是 $\\ell_0$ 范数的非凸连续松弛。\n1.4 数据驱动的正则化聚类 数据可以分为高维数据、含离群点的数据、缺失数据和其他类型数据。\n（1）高维数据驱动的正则化聚类方法\nWang 等人对高维数据集提出基于自适应组 Lasso 惩罚的正则化聚类模型：\n模型的最后一项的作用是迫使 X 的某一列为 0，从而达到剔除相应的特征的效果。\n（2）含离群点数据驱动的正则化聚类方法\n对离群点（异常点）的处理主要有三类方法：矩阵分解法、度量学习法、稳健损失函数法。\n（3）缺失数据驱动的正则化聚类方法\nPoddar 和 Jacob 基于数据间的部分距离（partial distance）提出可用于含缺失值数据的正则化聚类方法，其优化模型为：\n其中，$S_i$ 为采样矩阵。因为数据矩阵有缺失值，所以权重 $w_I$ 只能基于样本之间的部分距离进行近似计算。\n（4）其他类型数据驱动的正则化聚类方法\n其他的数据类型包括：二元数据（Binary data）、成分数据（Compositional data）、图结构数据（Graph-structure data）、直方图值数据（Histogram-valued data）。\n1.5 子空间聚类方法 很多高维数据往往分布在低维结构中，从高维数据中恢复这些低维子空间，不仅可以降低算法的计算成本和内存需求，还可以降低高位噪声的影响，进而提升算法的性能。子空间聚类方法是将特征选择技术和聚类技术相结合，并在维度空间或数据空间中搜索可能存在的子空间，从而得到聚类结果簇及其对应的子空间。现有的子空间聚类算法可以分为四大类：迭代方法、代数方法、统计方法和基于谱聚类的方法。子空间聚类方法可以归纳为如下统一模型：\n   聚类方法 优点 缺点     K-均值聚类 (1) 算法原理简单，运行速度快，收敛快 (2) 算法可解释性强 (1) 依赖于初始的聚类中心，会陷入局部最优解； (2) 需要提前给定聚类类别数K   凸正则化聚类 (1) 取得了丰富的研究成果 (1) 正则化函数可能导致估计量的偏差严重   非凸正则化聚类 (1) 消除了凸正则化函数导致的估计偏差 (1) 理论研究，高效一阶、二阶优化算法的设计还需要深入研究   数据驱动的的正则化聚类 (1) 特征选择，剔除了大量冗余的特征 (2) 消除了离群点（异常点）的干扰 (3) 缺失数据点可以通过采样数据间的部分距离解决 (4) 应用于多种数据类型    子空间聚类方法 (1) 低算法的计算成本和内存需求 (2) 降低高维噪声的影响     2 深度聚类（Deep Clustering） 图神经网络已经成为深度学习领域最热门的方向之一，那么，如何利用图神经网络强大的结构捕获能力来提升聚类算法的精度呢？深度聚类是聚类方法的一种，它采用深度神经网络来学习聚类友好表征。对于图聚类问题，其中的关键是如何捕捉结构关系和节点内容信息。很多近期的研究通过深度学习方法学习到节点的嵌入（Embedding），再利用简单的聚类算法（如 K-Means）进行聚类，以增强传统聚类算法的性能。\n图神经网络聚类与子空间聚类存在着联系与交集。研究发现，随着数据维度的上升，任何高维空间里的两点之间的距离（相似度）几乎没有区别，这就使得基于距离的相似度聚类方法失效。对于高维数据的聚类任务，最常用的处理方法就是降维，然后将降维后的特征应用于聚类算法。\n一般来说，这个降维后的特征可以称之为节点的嵌入（Embedding），相比于原来的高维空间的特征，降维后的低维度特征空间是聚类友好的特征空间（Clustering-friendly space）。所以聚类任务的其中一个目标是学习得到对聚类任务友好的特征空间的节点嵌入，再在此基础之上应用聚类算法，以取得良好的聚类结果。\n下面我介绍图聚类中的三篇经典文献：图注意力网络（Graph Attention Network）、深度聚类网络（Deep Clustering Network）和一种深度注意力的嵌入方法（A Deep Attentional Embedding Approach）。第一篇文献使用注意力机制学习节点的嵌入，第二篇介绍非目标导向的单阶段聚类方法，第三篇文献介绍以目标为导向的单阶段聚类方法。\n2.1 Graph Attention Network[2] 图注意力网络应用注意力机制将邻居节点的特征聚合到中心节点（一种Aggregate运算），注意力系数的使用，使得节点特征之间的相关性被更好的融合到模型中。\n左图为计算注意力系数（Attention coefficient）。即节点 $i$ 和其邻居节点 $j \\in N_i$ 之间的相关性，是通过可学习的参数W和映射 $\\overrightarrow{a}$ 实现的。右图为加权求和，将邻居节点的特征聚合到中心节点上，更新特征，$\\overrightarrow{h\u0026rsquo;}_l$ 就是融合了所有邻居节点的新特征表达。图注意力网络学习得到的节点嵌入融合了节点内容和图的结构信息，相当于学到了对聚类任务友好的特征空间（Clustering-friendly space）。\n2.2 two-stage方法 两阶段（two-stage）方法将聚类任务分为两个独立的阶段。第一阶段，先学习得到对聚类友好的节点嵌入（如神经网络、GAT 等 Aggregate 函数）；第二阶段，应用传统的聚类方法（如 K-means）进行聚类。两阶段方法存在一个明显的缺点，它是非目标导向的方法，我们最终的目标是得到好的聚类结果，而不是好的聚类空间节点嵌入，而且我们也无法知道所学到的聚类空间是否最优的。因此，将两阶段方法优化为单一阶段进行统一优化是非常必要的。\n2.3 single-stage方法[4] 论文[3]将数据降维和 K-means 聚类这两项任务联合起来，克服了两阶段方法的缺点，但其仍然是非目标导向的。论文[4]则提出一种以目标为导向的深度学习方法：Deep Attentional Embedded Graph Clustering (DAEGC)。这种方法包含三个主要核心点：\n（1）注意力机制的图自编码器（Graph Attentional Autoencoder）\n（2）自训练的图聚类（Self-optimizing Embedding）\n（3）自训练过程与图嵌入共同学习和优化（Joint Embedding and Clustering Optimization）\n为了实现以目标为导向，针对聚类这个无监督学习任务，提出了一个自训练的聚类组件，从“置信”的分配中生成软标签，以监督嵌入的更新。聚类损失和自动编码器重建损失被联合优化，以同时获得图嵌入和图聚类的结果。\n3 总结 聚类优化问题是不断发展的，图神经网络强大的结构捕获能力给传统的聚类方法注入了新的能量，在一定程度上缓解了传统聚类方法存在的不足，进一步提升了聚类性能。图神经网络可以聚合邻居信息来充分挖掘结构信息，为了同时实现对特征的降维抽取和对结构信息的挖掘。高阶结构信息（多层 GNN）也可以提升聚类的效果。从传统聚类到深度聚类以及现在图神经网络赋能的聚类, 各种各样的聚类算层出不穷,也在很多领域得到了广泛的应用。\n参考资料 [1] 孔令臣. 第五讲：聚类优化方法. 2021.07\n[2] Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio’, P., \u0026amp; Bengio, Y. (2018). Graph Attention Networks. ArXiv, abs/1710.10903.\n[3] Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos, and Mingyi Hong. 2017. Towards K-means-friendly spaces: simultaneous deep learning and clustering. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 (ICML'17). JMLR.org, 3861–3870.\n[4] Wang, C., Pan, S., Hu, R., Long, G., Jiang, J., \u0026amp; Zhang, C. (2019). Attributed Graph Clustering: A Deep Attentional Embedding Approach. IJCAI.\n","permalink":"http://landodo.github.io/posts/20210804-clustering-method/","summary":"节点聚类问题的总结和思考 摘要 聚类作为经典的无监督学习算法，在数据挖掘/机器学习等领域有着广泛地应用。聚类是针对给定的样本，依据它们的特征的相","title":"节点聚类问题的总结和思考"},{"content":"摘要 目前的方法严重依赖于大量的有标注数据。因此，在训练过程中加入未标注的数据是很常见的，这样可以用更少的标注达到相同的结果。本片综述主要是专注于应用无标注数据的分类任务。\n未来研究的三个趋势：\n 理论上，SOTA 的方法可以扩展到现实世界的应用程序，但没有考虑类不平衡、健壮性或模糊标签等问题； 实现与全监督相近的结果所需要的监督程度正在下降； 所有的方法都有一些共同的思想和独立的方法，通过结合不同的想法可以获得更好的性能。  1 Introduction 有标注的图像数量会影响深度神经网络的性能，ImageNet 就包含了大量的有标注数据。但是在现实中，很难获取到那么多的有标注数据（尤其是医学图像）。一个通常的解决方法是迁移学习，但是仍然存在问题：与人类不同，监督学习需要大量的有标注数据。\n对于一个给定的问题，我们通常可以获取到大量的未标注的数据集，如何将这些未标注的数据应用于神经网络是近几年研究的一个热点。\n 2016 年，研究了无监督深度学习图像聚类策略； 从那时起，对未标注数据的使用进行了多种方式的研究，并创造了无监督、半监督、自监督、弱监督或度量学习等研究领域；  无监督：使用未标注的数据 半监督（Semi-）：使用有标注和无标注的数据 自监督（Self-）：学习自己生产有标注的数据 弱监督（Weakly-）：仅使用标签的部分信息 度量学习（Metric）：学习一个好的距离度量    这些方法有一个统一的思想，即在训练过程中使用未标注的数据是有益的。它要么使标签较少的训练更健壮，要么在一些罕见的情况下甚至超过监督情况。\n无标注数据带来的益处，多研究人员都在 Semi-，Self- 和 unsupervised 领域开展工作。主要的目标是缩小半监督与监督学习之间的差距，甚至超越监督学习。因此，在这一领域有很多研究正在进行。本综述主要目标是跟踪半监督学习、自我学习和无监督学习的主要和最近的研究进展。只基于图像分类任务对这些方法进行比较。\n1.1 Related Work  无监督聚类算法在深度学习突破之前就有研究，目前仍被广泛使用； 已经有大量的调查描述了没有深度学习的无监督和半监督策略； 有许多新的综述只关注于 self-，semi- 和 unsupervised learning； Clustering deep neural network (CDNN)-based deep clustering Cheplygina 等人在医学图像分析的背景下提出了多种方法 本综述还包含：metric learning、meta learning、general adversarial networks、graph networks，应用方面：pose estimation、segmentation，数据类型：videos、sketches，其他主题：few-shot、zero-shot  1.2 Outline   Section 2：对用到的一些术语进行定义和解释，如方法、培训策略和公共的想法\n  Section 3：对各种方法进行了简要的描述，并给出了训练策略和共同思想\n  Section 4：在四种常用的图像分类数据集上，比较了基于它们所使用的思想和性能的方法；还包括对数据集和评估指标的描述。\n  最后，确定了三个趋势和研究机会。\n  Figure 2：术语及其依赖关系的可视化\n2 Underlying Concepts 训练策略（training strategy）：在训练期间使用无监督数据的一般类型/方法。\n根据它们的训练策略对方法进行粗略的排序，但根据使用的共同思想对它们进行详细的比较。\n  $X$：图像数据集，$X = X_l \\cup X_u$\n  $X_l$：标注的数据\n  $X_u$：未标注数据\n  $LS_X$：Learning Strategy，dataset X\n  训练的连续阶段被分成几个阶段，学习策略不同时，连续数据集 Xi 和 Xi+1 的阶段会发生变化。由于这个定义，在训练过程中只能出现两个阶段，图 4 显示了七种可能的组合。\n C 为标签 Z 的类别数 $f$ 是具有任意权值和参数的任意神经网络 Input $x \\in X$，output is $f(x)$  2.1 Training Strategies 文献中经常使用半监督学习、自我监督学习和非监督学习等术语，但某些方法的定义存在重叠。对于那些难以分类的边缘重叠情况，本综述根据训练期间的阶段来定义一个新的分类法，以精确区分这些方法。\n==无监督==学习是训练时没有任何的标注数据。它的目标可以是聚类或良好的表示（good representation）。一个良好的表示对聚类任务是有帮助的。\n==自监督==：这种无监督训练是通过生成自己的标签来实现的，因此这些方法被称为自监督方法。K-means 就是经典的无 self-supervised 的无监督方法。通常情况下，在相同或不同数据集的 pretext task 上实现自我监督，然后在下游任务上对预先训练的网络进行微调。\n本综述关注的是图像分类，因此大多数自监督或表示学习方法都需要对标记数据进行微调。由于使用了外部标记信息，预训练和微调的结合既不能称为无监督，也不能称为自我监督。\n==半监督==学习：使用有标注和无标注数据的方法。半监督和无监督的分类是比较困难的。\n在图像分类中，大多数方法要么是非监督的，要么是半监督的。当训练过程中使用标记或未标记的数据时，本综述仅根据粗略的区别来分离方法。把所有半监督(学习)策略、自我(学习)策略和非监督(学习)策略统称为减少监督(学习)策略（reduced supervised(learning) strategies）。\n将 stages 定义为使用不同学习策略有监督(X = Xl)、无监督(X = Xu)或半监督(Xu∩Xl =∅) 时训练的不同阶段/时间间隔。\n  two stages：使用自监督在 $X_u$ 上进行预训练，然后在有标注的相同图像上进行微调。\n  one stage：训练阶段使用不同的算法、损失、数据集，但是仅仅使用 $X_u$。\n  因此，可以将监督方法分类为训练策略：One-Stage-Semi-Supervised、One-Stage-Unsupervised、Multi-Stage-Semi-Supervised。\n2.1.1 Supervised Learning 仅仅使用已标注的数据 $X_l$，它对应的标签为 Z。其目标是最小化网络的输出 $f(x)$ 和期望的标签 $z_x \\in Z$ 之间的损失。\n2.1.2 One-Stage-Semi-Supervised Training 使用 $X_l, X_u, Z$ 在一个阶段完成，所有监督学习策略的主要区别在于使用额外的未标注数据 $X_u$。集成未标记数据的一种常见方法是在监督损失的基础上增加一个或多个非监督损失。\n2.1.3 One-Stage-Unsupervised Training 所有采用单阶段无监督训练策略的方法，只使用未标注样本 $X_u$ 在一个阶段进行训练。无监督学习存在多种损失函数，如自动编码器的重构损失（self-supervised）。与自监督方法相比，单阶段无监督方法给出图像分类，而不需要进一步使用标注数据。\n2.1.4 Multi-Stage-Semi-Supervised Training 训练分为两个阶段，第一阶段使用 $X_u$，第二阶段使用 $X_l$，或 $X_u$。许多被作者称为自我监督的方法都属于这一策略。通常使用 pretext task 来学习未标注数据的表示。第二阶段，这些表示被微调到 $X_l$ 上的图像分类。\n2.2 Common ideas 按字母顺序对这些想法进行了排序，并区分了损失函数和一般概念。\nLoss Functions\n交叉熵 Cross-entropy(CE)\n交叉熵是图像分类中的一个常见的损失函数，由于衡量 $f(x)$ 与其对应标签 $z_x$ 之间的差异。优化的目标是最小化其差异。\nP 是所有类的概率分布，近似于神经网络 $f(x)$ 或给定标签 $z_x$ 的 (softmax-) 输出。H 是概率分布的熵；KL 是 Kullback-Leibler 散度。\n需要注意的是，交叉熵是 $z_x$ 上的熵与 $f(x)$ 和 $z_x$ 之间的 Kullback-Leibler 散度的总和。\n交叉熵损失也可以使用与基于 ground truth 标签的 P 不同的概率分布，这些分布可能基于伪标签或自我监督 pretext task 中的其他目标。\n对比损失 Contrastive Loss(CL)\n对比损失尝试区分正例和负例（positive and negative pairs），positive pairs 可以是同一图像的不同视图，negative pairs 可以是批处理中的所有其他成对组合。\n通过对比去学习表示。近些年，这一思想在自监督视觉表示学习方法中得到了扩展。对比损失函数的例子有 NT-Xent 和 InfoNCE，两者都基于交叉熵。 NT\u0026ndash;Xent 的计算是通过含有 N 和样本的子集 X 中的所有 positive pairs 实现的。使用归一化的点积衡量输出之间的相似度。\n N 和 image pairs  InfoNCE 损失的动机与其他对比损失一样，是通过最大化不同视图之间的一致性/共同信息（agreement/mutual information）来实现的。\n熵最小化 Entropy Minimization(EM)\n在半监督学习中，预测的分布倾向于分布在许多或所有的类别，而不是在一个或几个类别。因此，研究人员建议锐化输出预测，或者换句话说，通过最小化熵迫使网络做出更有信心的预测。\n基于神经网络的输出 $f(x)$ 最小化概率分布的熵 $H(\\cdot|f(x))$，这个最小化导致了更尖锐更置信的预测。如果这个损失被用作唯一的损失，网络/预测将退化到一个微不足道的极小值。\nK-L 散度 Kullback-Leibler divergence(KL)\nK-L 散度可以整合为交叉熵的一部分，KL 描述的是两个给定分布之间的差异，通常被用作一个 auxiliary loss。\nMean Squared Error(MSE)\nMSE 衡量两个向量之间的欧几里得距离，不是一个概率度量。类似于熵的最小化，如果把这个损失作为网络输出的唯一损失，就会导致网络的退化。\nMutual Information(MI)\n定义了两种概率分布 P, Q 作为联合分布与边缘分布之间的 Kullback Leiber (KL) 散度。 在许多降监督的方法中，其目标是最小化分布之间的相关信息。\nVirtual Adversarial Training(VAT)\n","permalink":"http://landodo.github.io/posts/20210803-semi-self-unsupervised/","summary":"摘要 目前的方法严重依赖于大量的有标注数据。因此，在训练过程中加入未标注的数据是很常见的，这样可以用更少的标注达到相同的结果。本片综述主要是专","title":"20210803 Semi Self Unsupervised"},{"content":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。\n1. 聚类问题简介 聚类是针对给定的样本，依据它们的特征的相似度或距离，将其归并到若干“类”或“簇”中。其目的是通过得到的类或簇来发现数据的特点或对数据进行处理。聚类作为经典的无监督学习算法，在数据挖掘/机器学习等领域有着广泛地应用。\n两种最常用的聚类算法：层次聚类和 K 均值聚类。\n 层次聚类：将每个样本各自分到一个类；之后将相聚最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化类别结构。 K-Means 聚类：选择 K 个类别的中心，将样本逐个指派到与其最近的中心的类中，得到一个聚类结果；然后更新每个类的样本的均值，作为类的新的中心；重复以上步骤，直到收敛为止。  K-Means 被选为数据挖掘十大经典算法之一。\n图神经网络已经成为深度学习领域最热门的方向之一，那么，如何利用图神经网络强大的结构捕获能力来提升聚类算法的精度呢？深度聚类是聚类方法的一种，它采用深度神经网络来学习聚类友好表征。\n2. 相关综述 综述文献：[1][2]\n深度聚类算法（Deep Clustering Algorithm）可以分解为三个基本组成部分：\n 深度神经网络 网络损失 $L_n$ 聚类损失 $L_c$  3. 论文精读 论文名称：Attributed Graph Clustering: A Deep Attentional Embedding Approach [3]\n发表期刊：International Joint Conference on Artificial Intelligence (IJCAI-19)\n论文地址：https://arxiv.org/abs/1906.06532\n摘要 最近的研究大多集中在使用深度学习方法来学习一个紧凑的图形嵌入（embedding），在此基础上应用经典的聚类方法（如 K-means）来完成聚类任务。这种两阶段的方法通常无法取得更好的结果，因为其图嵌入不是以目标为导向的，即此深度学习方法并不是为聚类任务而设计的。\n本篇论文提出一种以目标为导向的深度学习方法：Deep Attentional Embedded Graph Clustering (DAEGC)。这种方法包含三个主要核心点：\n（1）注意力机制的图自编码器（Graph Attentional Autoencoder）\n（2）自训练的图聚类（Self-optimizing Embedding）\n（3）自训练过程与图嵌入共同学习和优化（Joint Embedding and Clustering Optimization）\nDAEGC 在 Cora、Citeseer、Pubmed 数据集上都取得了最好的聚类结果。\nIntroduction 对于图聚类问题，其中的关键是如何捕捉结构关系和节点内容信息。很多近期的研究通过深度学习方法学习到节点的 Embedding，再利用简单的聚类算法（如 K-Means）进行聚类。\n很显然，这是一种两个阶段的方法（非目标为导向），其存在着如下的缺点：学习到的 Embedding 可能不是最适合随后的图聚类任务，并且图聚类任务对图的嵌入学习没有帮助。\n传统的以目标为导向的方法大多针对的是分类任务（监督学习，如图卷积实现分类）。\n本论文提出了一种以目标为导向的图注意力自动编码器的图聚类框架。Figure 1 显示了其与两阶段方法的不同，模型学习嵌入并同时在一个统一的框架内进行聚类，从而获得更好的聚类性能。\nModel 如 Figure 2 所示模型主要由两个模块构成：\n（1）Graph attentional autoencoder\n自动编码器将节点属性值和图结构作为输入，并通过最小化重建损失来学习潜在的嵌入。\n（2）Self-training clustering\n自训练模块根据学习到的表征进行聚类，反过来，根据当前的聚类结果来操作潜在的表征。\n在一个统一的框架内学习图的嵌入和聚类，这样每个部分都能使对方受益。接下来对模型的细节进行分析。\n（1）Graph Attentional Autoencoder Graph Attention Encoder\n使用一个图注意力网络（GAT）的变体作为 Graph Encoder，其核心是通过关注其邻居来学习每个节点的隐藏表征，将节点特征与潜在表征中的图结构相结合。注意力机制对邻居的表示给予不同的权重。\n $z^{l+1}_{i}$​：节点 $i$​ 的输出表征（新特征）； $N_i$​：节点 $i$​ 的所有邻居节点； $\\alpha_{ij}$​：注意力系数，衡量节点 $j$​ 对节点 $i$​​ 的重要程度； $\\sigma$​​：非线性激活函数。  注意力系数 $\\alpha_{ij}$​ 可以表示为一个单层前馈神经网络，$x_i$ 和 $x_j$ 表示节点 $i$ 和 $j$ 的特征向量。\n $c_{ij}$​ 是一个标量，衡量节点 $j$​ 对节点 $i$​ 的重要程度。  图注意网络（GAT）只考虑了一阶邻居， 由于图具有复杂的结构关系，本篇论文的编码器中利用高阶邻居，通过考虑图中的 t 阶邻居节点来获得一个接近矩阵：\n $B$ 是一个转移矩阵（非负，各行元素之和等于 1），如果节点 $i$ 和 $j$ 之间存在边，则 $B_{ij} = 1/d_i$，否则 $B_{ij} = 0$。$d_{i}$ 为节点 $i$​​ 的度； $M_{ij}$​ 表示节点 $i$ 和节点 $j$ 之间的拓扑相关性（$t$​ 阶邻居）； $N_i$ 指 $M$ 中 $i$ 的邻居节点，如果 $M_{ij} \u0026gt; 0$，则表示 $j$ 是 $i$​​​ 的邻居节点； $t$：可以针对不同的数据集灵活地选择t，以平衡模型的精度和效率。  注意力系数通常在所有邻域 $j∈N_i$​​ 中用一个 softmax 函数进行归一化，以使它们易于在各节点间进行比较。\n加上拓扑权重 M 和激活函数 δ（LeakyReLU），注意力系数可以表示为：\n$x_i = z^{0}_{i}$​ 作为输入，堆叠两个图注意力层：\n图注意力编码器将结构和节点特征编码成一个隐藏的表示，得到：$z_i = z^{(2)}_{i}$。\nInner Product Decoder\n解码器（Decoder）可以进行如下分类：重建图结构、重建节点特征属性、两种都重建。由于公式（7）获取到的潜伏嵌入（latent embedding）已经包含了内容和结构信息，因此本篇论文选择采用一个简单的内积解码器来预测节点之间的联系：\n重建损失 Reconstruction Loss\n通过衡量 $A$ 和 $\\hat{A}$ 之间的差异性来最小化重构损失：\n（2）Self-optimizing Embedding 图聚类任务是无监督的，因此在训练期间无法获得关于所学嵌入是否得到很好优化的反馈。即 GAE 所学习到的节点表示只是为了更好的重构网络结构，和聚类并没有直接联系。针对此问题，本篇论文提出了 一种自优化的嵌入算法作为解决方案，对 GAE 所学习到的 embedding 进行约束和整合，使其更适合于聚类任务。\n除了优化重建误差，还将隐藏嵌入（hidden embedding）输入到一个自优化的聚类模块中，该模块最小化了以下目标：\n $q_{iu}$ ：衡量节点的 embedding $z_{i}$​ 和聚类中心 embedding $\\mu_u$ 的相似度  使用学生分布（Student\u0026rsquo;s $t$​​-distribution）来衡量，以处理不同规模的集群。（11）式中，聚类中心 embedding 为 $\\mu_u$​​​，则节点 $i$​​​​ 属于某个类别的概率为 $q_{i u}$:\n T 分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值。\n $q_{i u}$ 可以被看作是每个节点的软聚类分配分布。为了引入聚类信息来实现聚类导向的节点表示，我们需要迫使每个节点与相应的聚类中心更近一些，以实现所谓的类内距离最小，类间距离最大（二次方后，分布会变得更加尖锐，也更置信）。定义目标分布 $p_{iu}$​ ：\n 类别 $k$  目标分布 P 将 Q 提高到二次方，以强调这些“自信的分配”的作用。然后，聚类损失迫使当前分布 Q 接近目标分布 P，以便将这些“有信心的分配”设置为软标签（“works as ground-truth labels”），监督 Q 的嵌入学习。\n将聚类损失降到最低，以帮助自动编码器利用嵌入物自身的特性操纵嵌入空间。\n（3）Joint Embedding and Clustering Optimization 共同优化自动编码器嵌入和聚类学习，总目标函数定义为：\n  $L_r$：Reconstruction loss\n  $L_c$：Clustering loss\n  $\\gamma \u0026gt;= 0$：控制两者的平衡\n  可以直接从最后一个优化的 Q 中获得聚类结果，即对节点 $i$ 的所处的簇为：\nDeep Attentional Embedded Graph Clustering Algorithm\nExperiments 三个数据集\n总结  DAEGC: an unsupervised deep attentional embedding algorithm; 在一个统一的框架内，联合进行图聚类和学习图嵌入； 学习到的图嵌入整合了结构和内容信息，并专门用于聚类任务； 针对聚类这个无监督学习任务，提出了一个自训练的聚类组件，从“置信”的分配中生成软标签，以监督嵌入的更新； 聚类损失和自动编码器重建损失被联合优化，以同时获得图嵌入和图聚类的结果； 将实验结果与各种最先进的算法进行比较，验证了 DAEGC 的图聚类性能。  References [1] Aljalbout, E., Golkov, V., Siddiqui, Y., \u0026amp; Cremers, D. (2018). Clustering with Deep Learning: Taxonomy and New Methods. ArXiv, abs/1801.07648.\n[2] Erxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing Cui, and Jun Long. A Survey of Clustering with Deep Learning: From the Perspective of Network Architecture. DOI: 10.1109/ACCESS.2018.2855437, IEEE Access, vol. 6, pp. 39501-39514, 2018.\n[3] Wang, C., Pan, S., Hu, R., Long, G., Jiang, J., \u0026amp; Zhang, C. (2019). Attributed Graph Clustering: A Deep Attentional Embedding Approach. IJCAI.\n[4] https://deepnotes.io/deep-clustering\n Can deep neural networks learn to do clustering? Introduction, survey and discussion of recent works on deep clustering algorithms.\n [5] Papers List (1.2k Stars) https://github.com/zhoushengisnoob/DeepClustering\n Deep Clustering: methods and implements\n [6] 图神经网络时代的深度聚类：https://zhuanlan.zhihu.com/p/114452245\n","permalink":"http://landodo.github.io/posts/deep-clustering-notes/","summary":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。 1. 聚类问题简介 聚类是针对给定的样本，依据它们的","title":"深度聚类 Deep Clustering"},{"content":"GAT  地址：https://arxiv.org/abs/1710.10903 期刊：International Conference on Learning Representations (ICLR) 2018 被引用次数：4116  Abstract Graph Attention Networks(GATs) 利用 masked self-attention 为邻域中的不同节点指定不同的权重，这个操作不需要任何高成本的矩阵操作（如矩阵的逆）或取决于预先知道的图结构，解决了先前基于图卷积或其近似值的方法的缺点。\n GAT：应用注意力机制将邻居顶点的特征聚合到中心顶点上（一种 aggregate 运算），注意力系数的使用，使得顶点特征之间的相关性被更好地融入到模型中。\n GAT 很适用于 inductive 和 transductive 问题，在 Cora、Citeseer、Pubmed citation network datasets 和 an inductive protein-protein interaction dataset 上取得了 SOTA。\n1 Introduction 图像分类、语义分割或者机器翻译等任务的数据表示具有一种类似网格的结构（grid-like structure），卷积神经网络架构能够有效地重复使用参数可学习的局部卷积核，并将它们应用于所有输入位置（局部连接、参数共享）。\n对于不能用网格状的结构来表示的数据，通常表示为图结构。早期的工作有使用 RNN 来处理有向无环图的数据，图神经网络 GNNs 作为 RNN 的一种泛化，可以直接处理更普遍的图类。\n 💢TODO: 了解最初 GNN 的具体做法。\n 将卷积推广到图域，主要有两类：谱方法（spectral methods）和空间方法（spatial methods）。\n谱方法：卷积操作是在傅里叶域中通过计算图拉普拉斯的特征分解来定义的，这导致潜在的密集计算和非空间定位的过滤器。针对这两个问题：（1）Henaff 等人（2015）引入了一个具有平滑系数的光谱过滤器参数化，以使过滤器在空间上得到定位。（2）Defferrard 等人（2016）提出通过图拉普拉斯的切比雪夫展开来近似滤波器，消除了计算拉普拉斯特征向量的需要，产生了空间定位的滤波器。（3）semi-GCN 通过限制过滤器在每个节点周围的 1 步邻域内运行，进一步简化了之前的方法。\n利用谱方法学习到的滤波器取决于拉普拉斯特征基，而拉普拉斯特征基取决于图的结构。因此，一个在特定结构上训练的模型不能直接应用于不同结构的图。\n空间方法：它直接在图上定义卷积，对空间上的近邻组进行操作。这个方法的挑战是如何定义一个能够处理不同大小的邻域（可以理解为感受野）并保持 CNN 的权重共享的算子。针对这个挑战，有如下的尝试：（1）为每个节点的度学习特定的权重矩阵；（2）使用 transition matrix 来定义领域；（3）提取和规范化包含固定数量节点的邻域；（4）混合模型 CNN（MoNet）；（5）~GraphSAGE~ 通过对每个节点的固定大小的邻域进行采样，然后对其进行特定的聚合（如所有采样邻域的特征向量的平均值）。\n在许多基于序列的任务中，注意力机制几乎已经是标配。注意机制的好处之一是，它允许处理大小不一的输入，集中于输入中最相关的部分来做决定。self-attention 通常被用来计算一个单一序列的表示。\nGAT 引入了一个基于注意力的架构来进行图结构数据的节点分类。基本的想法是：通过关注其邻居节点，按照 self-attention 的策略，计算图中每个节点的隐藏表征。这个 Attention 架构的优点： （1）计算高效，并行。 （2）可以通过为邻居节点指定任意的权重来适用于具有不同度的图节点。 （3）该模型可直接适用于归纳学习问题（inductive learning problems）。\nGAT 在 four challenging benchmarks（ Cora, Citeseer, Pubmed citation networks, an inductive protein-protein interaction dataset）上取得了 SOTA。\n2 GAT 架构 2.1 Graph Convolution Layer 以 Cora 数据集为例，h 的 shape=(2708, 1433)，即有 N=2708 个节点，每个节点有 F=1433 个特征。F’ = 64 or 7，表示经过 GATConv 后节点的特征数量。\n [2708, 1433] —GATConv1—\u0026gt; [2708, 64] —CATConv2—\u0026gt; [2708, 7]。\n 为了获得足够的表达能力，将输入特征转化为更高层次的特征，至少需要一个可学习的线性转化。权重矩阵 W ∈ ℝ(F’ × F) 应用于每个节点。然后对节点进行 self-attention（一种共享的注意机制 a），把拼接后的高维特征映射到一个实数上，这个实数就是注意力系数： 注意力系数 e_{ij} 是一个标量，表示表示节点 𝑗 的特征对节点 𝑖 的重要性。对于一般的表示，𝑗 为图中除 𝑖 外的所有节点，这个忽略了结构信息（如邻居节点）。\n本篇论文则通过 masked-attention，只计算节点 𝑗 ∈ N𝑖 的注意力系数e_{𝑖𝑗}，其中 N𝑖 是节点 𝑖 在图中的某个邻居节点。这是一种考虑了图的结构信息的注意力机制。为了使系数在不同的节点之间容易比较，使用 softmax 函数对所有 𝑗 的选择进行归一化。 (1) 式中的 𝑎 是一个单层的前馈神经网络，由权重向量 𝑎→∈ ℝ(2F’) 参数化。然后应用 LeakyReLU 非线性，(3) 式的示意图为 Figure 1 Left。 归一化的注意力系数 𝛼𝑖𝑗 被用来计算与之对应的特征的线性组合，作为每个节点的最终输出特征。 ℎ→∈ℝ(N, F’)。 为了使 self-attention 的学习过程更加稳定，将 GAT 的 masked-attention 机制扩展到采用 multi-head attention 是有益的。  PyG 实现的 GAT，第一个 GATConv 的 head=8，即 (5) 式的 K = 8。第二个 GATConv head=1。\n 最后一层采用平均法是更好的选择。 multi-head masked-attention 的示意图如 Figure 1 Right。三种颜色的线表示 head = 3。 3 实验评估 如下是 4 个数据集的基本信息。 我只对其中的 Cora 数据集进行介绍。Cora 在图网络中的地位可以类比于 MNIST 之于图像分类任务。\nCora Dataset (https://relational.fit.cvut.cz/dataset/CORA)\n 相当于图网络中的“MNIST”。\n Cora：2708 个节点，5429 条边，1433 个特征，7 个类别（y）。训练数据集中有的有标签🏷，有的没有标签，Semi-Supervised Classification。\n下面介绍 Transductive learning 和 Inductive Learning 的区别： Transductive：以使用 Cora 数据集为例，把节点分成 train/val/test 集。在训练时，只使用来自训练集节点的标签，但是在 forward 时，根据 GNN 的工作方式，将从邻居聚合特征向量，而邻居中有一些节点可能属于 val 或 test 集。（训练和测试都基于同样的图结构）  GAT 模型学到的节点注意力权重非常接近均匀分布（换言之，所有的邻节点都获得了同等重视）。这在一定程度上解释了为什么在 Cora 上 GAT 的表现和 GCN 非常接近（在上面表格里我们可以看到两者的差距平均下来不到 2%）。由于没有显著区分节点，注意力并没有那么重要。\n Inductive：train set、val set、test set 相互独立。不会出现邻居属于 val/test set 的情况。（用完全没见过的图进行测试） 4 总结 图注意力网络 Graph Attention Networks (GATs) 直接在图结构的数据上进行操作，采用 masked self-attention，是一种新颖的卷积神经网络。\n这种注意力机制允许在处理不同大小的邻域时给邻域中的不同节点分配不同的重要性，并且不依赖于预先知道整个图结构。\n未来的研究方向，可以是利用注意力机制的优势，对模型的可解释性进行彻底分析。\n此外，将该方法扩展到图分类而不是节点分类，从应用的角度来看也很有意义。\n","permalink":"http://landodo.github.io/posts/20210711-gat/","summary":"GAT 地址：https://arxiv.org/abs/1710.10903 期刊：International Conference on Learning Representations (ICLR) 2018 被引用次数：4116 Abstract Graph","title":"Graph Attention Networks(GATs) "},{"content":"#DeepLab# 中的空洞卷积和条件随机场 简要总结 DeepLab V1 和 DeepLab V2，DeepLab 是语义分割的经典网络架构。其有两个核心要点：\n（1）空洞卷积（Atrous Convolution/‘hole’ algorithm/Dilated Convolution）\n（2）条件随机场（Fully-Connected CRF）\nDeepLab V1 的提出时间是 2014 年 12 月，基干网络选择了 2014 年 10 月份所提出 VGG-16。专为图像分类任务而设计的 VGG 在语义分割任务上的表现并不出色，这是由于：\n （1）最大池化和 “Stride”会使得分辨率降低。 （2）语义分割任务仍需要空间转换的不变性（语义分割仍是一个分类问题），这从本质上限制了 DCNN 模型的空间准确性。  这两个原因造成的。DeepLab V1 针对这两个问题使用的解决方案是：空洞卷积、CRF。\nDeepLab V2 的提出时间是 2016 年 06 月，基干网络选择了 2015 年 12 月所提出的 ResNet。在 DeepLab V1 的基础之上，V2 强调了空洞卷积 ‘Atrous Convolution’ 作为语义分割任务的一项强大工具，并提出了 atrous spatial pyramid pooling (ASPP) 提取融合多尺度信息，在多个数据集上进行了丰富的实验评估。\nDeepLab V1可以总结为以下几点：\n 基于 VGG-16，全连接层转换为 1×1 卷积层 删除了最后两个 max-pooling（或者说是最后两个池化层不改变特征图的大小） 空洞卷积“Atrous algorithm”调整感受野 使用双线性插值上采样 8× 得到原图大小 使用全连接 CRF 进行边界恢复 多尺度预测  DeepLab V2总结为以下几点：\n 使用 ResNet 作为基干网络 ASPP 模块 丰富的实验   DeepLab V1  Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., \u0026amp; Yuille, A. (2015). Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs. CoRR, abs/1412.7062.\n   期刊：ICLR 2015\n  论文地址（DeepLab V1）: https://arxiv.org/abs/1412.7062\n  作者简介：\n  http://liangchiehchen.com/   Google Research Scientist\n  摘要 DCNNs（Deep Convolutional Neural Networks）在图像分类和目标检测任务上取得了 SOTA 的成绩，但是在物体分割任务上表现得并不精确。论文的主要工作是结合 DCNNs（Deep Convolutional Neural Networks） 和概率图形模型的方法，以解决像素级分类（语义图像分割）的任务。\nDeepLab V1 在 PASCAL VOC-2012 语义图像分割任务中取得了 SOTA，在测试集中达到了 71.6% 的 IOU 准确性。\n1. Introduction 以端到端方式训练的 DCNN 取代了依靠人工精心设计的特征，如 SIFT 或 HOG。DCNN 的成功可以部分归因于其对局部图像转换的内在不变性（invariance），这也是它们学习数据的分层抽象的能力的基础。\n这种不变性有利于高层次的视觉任务（如图像分类），不利于低层次的任务（如语义分割）。在低层次的任务中，我们想要精确的定位，而不是空间细节的抽象。\n在 DCNN 应用于图像标记任务中，有两个技术障碍。 （1）最大池化和 “Stride”会使得分辨率降低。\n（2）语义分割任务仍需要空间转换的不变性（语义分割仍是一个分类问题），这从本质上限制了 DCNN 模型的空间准确性。\n本篇论文对这两个问题的解决方案分别是：\n ‘atrous’ (with holes) algorithm 全连接的条件随机场（CRF）  2. 相关工作（略） 3. 用于密集图像标记的卷积神经网络 3.1 ‘HOLE’ ALGORITHM 将 VGG-16 的全连接层转换为卷积层，作为特征图提取。5 个最大池化跳过了后两个，或者将最后两个最大池化 stride = 1，kernal = 3，这样得出的特征图大小不变。\n DeepLab V1：https://github.com/wangleihitcs/DeepLab-V1-PyTorch/blob/master/nets/vgg.py  # max pooling  nn.MaxPool2d(kernel_size=3, stride=1, padding=1), 最大池化层的改动，影响到了其后卷积层的感受野大小，通过引入 0 来修改（上采样）卷积核。 保持卷积核的完整，而对它们所应用的特征图分别使用 2 或 4 像素的输入跨度进行稀疏采样。如下图 1 所示，称为 hole algorithm（Atrous algorithm）。\n将最后的 1000-way 输出修改为 21-way，ground truth (subsampled by 8)，即最后网络的输出相对于原图像小 8 倍，然后使用双线性插值将其分辨率提高 8 倍。\n3.2 边界恢复：完全连接的 CRF 和 Muti-Scale 定位精度和分类性能之间的权衡似乎是 DCNN 所固有的：事实证明，具有多个最大池层的较深模型在分类任务中是最成功的，然而，增加的不变性和顶层节点的大感受野只能产生平滑的效果（Fig 2. 边界模糊）。\n如上图，DCNN 的 Score maps 能预测物体的存在和大致位置，但不能真正划定其边界。\n有两种方法解决这个问题： （1）利用卷积网络中的多层信息，以便更好的估计边界。 （2）采用超级像素表示法，基本上将精确定位任务委托给低级别的分割方法。\n本篇论文追求另一个方向，即把 DCNN 的识别能力和全连接 CRF 的细粒度定位精度结合起来。全连接 CRF 在解决定位挑战方面非常成功，产生了准确的语义分割结果，并在现有方法无法达到的细节水平上恢复了物体的边界。\n条件随机场（CRFs）被用来平滑嘈杂的分割图。一般来说，模型将相邻的节点结合在一起，倾向于将相同的标签分配给空间上近似的像素。\n而从 Fig 2 可以看出，DCNN 输出的 Score map 是相当平滑的，并产生同质化的分类结果。在这种情况下，我们需要恢复局部结构，而不是平滑它。\nEnergy Function：\nGaussian Kernel：\n这里见 PPT，我会从 Random Field Models，到 Filtering，再到 Dense Random Fields，依次介绍其优缺点，从宏观的角度去了解这个后处理操作。\n考虑多尺度信息能够提升最后分割的精度，与 FCN skip layer 类似，如下示意图所示。\n4. 实验结果 数据集：PASCAL VOC 2012，21 类，包含 20 个前景对象类和 1 个背景类。\n（a）\n CRF 带来 4% 的性能提升（分割效果如 Fig 7 所示） 多尺度 MSc 带来带来 1.5% 的性能提升（分割效果如 Fig 4）  采用的 “Atrous algorithm” 允许通过调整输入步幅来任意控制模型的感受野（Field-Of-View, FOV）。\n与 SOTA 方法进行比较。\n总结 DeepLab V1 可以总结为以下几点：\n 基于 VGG-16，全连接层转换为 1×1 卷积层 删除了最后两个 max-pooling（或者说是最后两个池化层不改变特征图的大小） 空洞卷积“Atrous algorithm”调整感受野 使用双线性插值上采样 8× 得到原图大小 使用全连接 CRF 进行边界恢复 多尺度预测   DeepLab V2  Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., \u0026amp; Yuille, A. (2018). DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40, 834-848.\n  论文地址 DeepLab V2: https://arxiv.org/abs/1606.00915 期刊：全称 IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)，简称为 IEEE PAMI 或者（PAMI）  摘要 论文使用深度学习解决了语义图像分割的任务，有三个主要贡献：\n（1）强调了空洞卷积 ‘Atrous Convolution’ 作为语义分割任务的一项强大工具。\n（2）提出了 atrous spatial pyramid pooling (ASPP)，考虑多尺度信息。\n（3）将最后一个 DCNN 层的响应与全连接的条件随机场（CRF）相结合来改善物体边界的定位。\n1. Introduction DCNN 在语义图像分割中应用的三个挑战：\n（1）特征分辨率的降低\n（2）多尺度物体的存在\n（3）由于DCNN的不变性而降低了定位精度\n第一个挑战分辨率的降低，这是由于 DCNNs 的最大池化层和降采样层的堆叠造成的，这个结构是为分类而设计的。特征图的空间分辨率降低对于分割任务来说是非常不利的。\n为了解决这个问题，DeepLab V2 移除了 max-pooling 层，使用升采样的卷积核（空洞卷积），最后使用双线性插值恢复到原始图像大小。与普通的卷积相比，空洞卷积在不增加额外计算量和参数量的情况下，有效增加了感受野大小。\n第二个挑战是由多个尺度的物体的存在引起的。传统的解决方案是在 DCNN 层计算输入图像的多个尺度比例的特征响应，进行汇总。受到 spatial pyramid pooling 的启发，本论文提出 “atrous spatial pyramid pooling” (ASPP)，通过使用用具有互补的感受野的多个卷积核来探测原始图像，从而在多个尺度上捕获物体和有用的图像背景。\n第三个挑战是：以物体为中心的分类器需要对空间变换保持不变，这本身就限制了 DCNN 的空间准确性。一种解决方案是使用跳跃连接（ResNet）；本论文则通过采用完全连接的条件随机场（CRF），提高了模型对精细细节的捕捉能力。Fully Connected CRF，具有计算效率高、能够捕捉到精细的边缘细节、满足长距离的依赖等优点。\nDeepLab V2 总体流程：\nDeepLab 具有如下三个优点：\n (1) Speed (2) Accuracy (3) Simplicity  与 DeepLab V1 相比，V2 有如下改变：\n（1）ASPP：在多个尺度上更好地分割物体\n（2）使用了 ResNet-101\n（3）进行了更多的实验评估\n2. Related Works（略） 3. Methods DeepLab V2 的核心有三个：空洞卷积、ASPP、全连接的 CRF。\n3.1 Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement 最大池化和卷积步幅使得特征图的分辨率降低，‘deconvolution’ 虽然可以缓解，但是需要引入额外的内存和时间消耗。\n空洞卷积最初是为小波变换开发的。空洞卷积算法使我们能够以任何理想的分辨率计算任何一层的输出，可以与预训练的模型无缝结合。\n一维的信号：x[i] 为输入的信号，w[k] 表示卷积核，y[i] 表述输出，r 扩张率，表示对输入信号进行采样的步长，r=1 表示标准的卷积。\n2-D 图像示例：\n先进行下采样，再进行卷积，最后升采样。 如果把得到的特征图植入原始图像的坐标，会发现我们只在图像的 1/4 位置获得了响应。\n如果使用空洞卷积，尽管卷积核的尺寸变大了，但是仅需要考虑非 0 值，参数的数量和每个位置的操作数保持不变。 这个方案使我们能够轻松而明确地控制神经网络特征反应的空间分辨率。\n接下来是关于空洞卷积的感受野，在不增加参数和计算量的情况下，k×k 扩大称为 k + (k-1)(r-1)。如 3×3,r=2，其感受野为 3 + (3 - 1)(2 - 1) = 5。因此，它提供了一个有效的机制来控制感受野，并在准确定位（小感受野）和上下文信息（大感受野）之间找到最佳平衡点。\n空洞卷积的实现有两种方式：\n（1） 第一种是通过插入空洞（0）来隐含地对卷积核进行升采样，或者说是对输入特征图进行稀疏采样。\n（2）第二种是对输入特征图进行子采样。\n3.2 Multiscale Image Representations using Atrous Spatial Pyramid Pooling (ASPP) 研究表明，考虑物体的尺度可以提高 DCNN 成功处理大物体和小物体的能力。有两种方法：\n（1）标准的多尺度处理 从多个重新缩放的原始图像中提取 DCNN 得分图，最后进行融合。（DeepLab V1）\n（2）ASPP 研究表明，任意尺度的区域可以通过对单一尺度提取的卷积特征进行重采样来准确有效地分类。使用了多个并行的非线性卷积层，具有不同的采样率，为每个采样率提取的特征在不同的分支中进一步处理，并融合以产生最终结果。\n3.3 全连接的 CRF 进行边界恢复 同 DeepLab V1。\n4. EXPERIMENTAL RESULTS DeepLab V2 进行了丰富的实验。\n基干网络使用 ResNet-101，数据集有 PASCAL VOC 2012, PASCAL-Context, PASCALPerson-Part, and Cityscapes。\n这里我只列出 PASCAL VOC 2012，方便与 DeepLab V1 进行比较。\nASPP\nDeepLab V2：79.7%，DeepLab V1：71.5%。\n基干网络 VGG-16 和 ResNet-101 比较：\n总结 DeepLab V2 总结为以下几点：\n 使用 ResNet 作为基干网络 ASPP 模块 丰富的实验  Reference [1] Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., \u0026amp; Yuille, A. (2015). Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs. CoRR, abs/1412.7062.\n[2] Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., \u0026amp; Yuille, A. (2018). DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40, 834-848.\n[3] https://www.cs.cmu.edu/~efros/courses/LBMV12/crf_deconstruction.pdf\n[4] http://vision.stanford.edu/teaching/cs231b_spring1415/slides/philipp_densecrf.pdf\n[5] https://web.cs.ucdavis.edu/~yjlee/teaching/ecs289g-winter2018/DeepLab.pdf\n","permalink":"http://landodo.github.io/posts/20210620-deeplab-v1-v2/","summary":"#DeepLab# 中的空洞卷积和条件随机场 简要总结 DeepLab V1 和 DeepLab V2，DeepLab 是语义分割的经典网络架构。其有两个核心要点： （1）空洞卷积（Atrous Con","title":"#DeepLab# 中的空洞卷积和条件随机场"},{"content":"Boundary loss for highly unbalanced segmentation  Csurka, G., Larlus, D., \u0026amp; Perronnin, F. (2013). What is a good evaluation measure for semantic segmentation? BMVC.\n(218 Citations.)\n  Kervadec, H., Bouchtiba, J., Desrosiers, C., Granger, É., Dolz, J., \u0026amp; Ayed, I.B. (2019). Boundary loss for highly unbalanced segmentation. Medical image analysis, 67, 101851 .\n(95 Citations)\n  论文 PDF : https://openreview.net/pdf?id=S1gTA5VggE 或者（https://arxiv.org/pdf/1812.07032.pdf） 期刊：Medical Imaging with Deep Learning (MIDL 2019 Conference), best paper award at MIDL 2019. 作者的 Presentation：https://www.bilibili.com/video/BV1U4411F76a（3:29:00 开始） source code：https://github.com/LIVIAETS/boundary-loss/blob/master/losses.py  0 Abstract Dice loss 和 cross-entropy 这种区域损失在类别高度不平衡的分割任务中，会影响到训练性能和稳定性。本篇论文提出了一种边界损失（boundary loss），其形式是轮廓（或形状）空间上的距离度量，而不是区域，\n 这可以减轻在高度不平衡的分割问题中区域损失的困难； 此外，边界损失提供的信息是对区域损失的补充。  在计算时，将形状空间上的非对称 L2 距离表达为区域积分，这就产生了一个用网络的区域 softmax 概率输出表示的边界损失，它可以很容易地与标准的区域损失相结合。\n在 the ischemic stroke lesion (ISLES) and white matter hyperintensities (WMH) 这两个高度不平衡数据集上，Dice + boundary 相比于单独使用 Dice，其 Dice score 提升了 8%。\n💢TODO：眼底视网膜分割数据集（DRIVE/CHASE_DB1），或许也可以归为不平衡的数据集，使用 Dice Loss + 本篇论文的 boundary loss 或许可以在原基础之上提升最后的性能，可以简单试一下实验结果。\n##1 Introduction\nDice or cross-entropy 都基于基于区域积分（regional integrals），这对训练深度神经网络很方便。\n标准区域损失包含前景和背景项，它们的值有很大差异，通常是几个数量级，这可能会影响性能和训练的稳定性，这在医学图像分析中相当常见。\n交叉熵 在高度不平衡的问题上，标准的区域 交叉熵（CE）有众所周知的缺点：\n 它假定所有样本和类别的重要性分布是相同的； CE 通常会导致不稳定的训练，并导致决策边界偏向于大多数类别。  常见的缓解类别不平衡策略：\n re-balance class prior distributions by down-sampling frequent labels. assign weights to the different classes, inversely proportional to the frequency of the corresponding labels.  Dice  Lee Raymond Dice 命名\nVNet 应用 Dice Loss\n Dice coefficient 等同 F1 score。Dice Loss 一种能够直接作用于 F1 Score 的损失函数。\n 在处理非常小的结构时可能会遇到困难，训练不稳定； Dice 对应于精确度和召回率之间的谐波平均值，这种损失主要适合于 FP 和 FN 同样糟糕的时候。  相比于 Dice 存在的缺点，Tversky 相似度指数可以在精确度和召回率之间提供一个更好的权衡。\n✅ Dice loss 比较适用于样本极度不均的情况，一般的情况下，使用 Dice loss 会对反向传播造成不利的影响，容易使训练变得不稳定。\n论文贡献：\n In this paper, we propose a boundary loss that takes the form of a distance metric on the space of contours (or shapes), not regions.\nWe argue that a boundary loss can mitigate the issues related to regional losses in highly unbalanced segmentation problems. Rather than using unbalanced integrals over the regions, a boundary loss uses integrals over the boundary (interface) between the regions.\nFurthermore, it provides information that is complimentary to regional losses.\n 2. Formulation 对于两个区域的情况，一般形式的积分：\n$$f = -log(\\cdot)$$\n广义 Dice 损失（GDL）：\n Ω 表示具有空间域的训练图像 g 表示对图像进行二进制的 Ground Truth 分割，g(p) = 1 表示像素 p 属于目标区域 G⊂Ω（前景区域），否则为 0 Ω\\G（背景区域） $s_\\theta$ 表示分割网络的 softmax 概率输出  广泛使用的分割损失函数涉及 Ω 中每个分割区域的区域积分，它衡量网络的概率输出所定义的区域与相应的地面实况之间的一些相似性（或重叠）。\n在医学图像分析中，极不平衡的分割非常常见，例如，目标前景区域的大小比背景大小小几个数量级，如此之大的差异影响了分割的性能和训练的稳定性。\nBoundary Loss Dist($∂_G, ∂_{S_\\theta}$ )：$∂_G$ 表示区域 G 的 Ground Truth 边界，$∂_{S_θ}$表示由网络输出定义的分割区域 G 的边界。\n （1）边界损失应该能够减轻上述不平衡分割的困难：它使用区域之间的边界的积分。此外，边界损失提供的信息与区域损失不同，因此是补充性的。 （2）并不清楚如何将 $∂_{S_θ}$ 上的边界点表示为区域网络输出 sθ 的可微调函数。（我的理解是：Boundary Loss 单独使用并不 work，无法使用反向传播更新）  本篇论文提出的 Boundary Loss 启发自 discrete (graph-based) optimization techniques for computing gradient flows of curve evolution (Boykov et al., 2006). curve evolution methods 需要一个评估边界变化（或变化）的措施。\nBoundary Loss Dist($∂_G, ∂_{S_\\theta}$ )：评估两个附近边界 ∂S 和 ∂G 之间的变化（Boykov 等人，2006）。\n 💢为什么 (2) 不能直接作为 Loss 进行优化？理解了这个很重要！\n  p∈Ω 是边界 ∂G 上的一个点 y∂S(p) 表示边界 ∂S 上的相应点（法线方向交点）。  公式 (2) 不能直接作为 ∂S= ∂Sθ 的损失！！！\n✅ 很容易表明，(2) 中的微分边界变化可以用积分方法来表达 (Boykov 等人，2006)，积分方法完全避免了涉及轮廓点的局部微分计算，并将边界变化表示为一个区域积分。得到公式 (3)。\n it is easy to show that the differential boundary variation in (2) can be expressed using an integral approach (Boykov et al., 2006), which avoids completely local differential computations involving contour points and represents boundary change as a regional integral:\n 其中，∆S 表示两条轮廓线之间的区域，DG : Ω→R+ 是相对于边界 ∂G 的距离图，即 $D_G(q)$ 评估了点 q∈Ω 与轮廓 ∂G 上最近的点 $z_{∂G}(q)$ 之间的距离。\n将 instance map $2D_{G}(q)$ 在连接 ∂G 上的点 p 和 $y_{∂S}(p)$ 的法线段上积分，就可以得到：\n因此，从公式（3）来看，公式（2）中轮廓线之间的非对称 L2 距离可以表示为基于边界 ∂G 的 level set 表示的区域积分之和。\n ❓什么是 level set？\n  s : Ω→{0,1} 是区域 S 的 binary indicator function，如果 q∈S 属于目标，s(q)=1，否则为 0。 $\\phi_G$ : Ω→R 表示边界 ∂G 的 level set 表示，如果 q∈G，$\\phi_{G}(q)=-D_G(q)$，否则 $\\phi_G(q)=D_G(q)$。  approximates boundary distance Dist(∂G,∂Sθ ):\n容易与标准的区域损失相结合：\nExperiment  Dice 相似系数（Dice Similariy Coefficient,DSC），取值范围 [0, 1]，1 为最好，0 为最差。 Hausdorff Distance 豪斯多夫距离：以集合 A 中任一点，半径取 HD 画圆，可以包含集合 B 的所有点。   boundary loss term brings a DSC improvement of around 2% on the WMH dataset, it achieves 8% better DSC on the ISLES segmentation task. The same trend is observed with the HD metric, where the gain is larger on the ISLES dataset than on WMH.\n 边界损失项有助于稳定训练过程，随着网络训练的收敛，产生了一个更平滑的曲线。Boundary Loss 只涉及两个矩阵之间的元素相乘，所增加的计算复杂度几乎可以忽略不计。\n总结 •单独使用 Boundary Loss 不 work，一般进行与 Dice 组合使用。\n•理论上，边界损失的全局最优对应于一个负值，因为一个完美的重叠只对距离图的负值进行求和。\n•边界损失项有助于稳定训练过程，随着网络训练的收敛，产生了一个更平滑的曲线\n•使用 Boundary Loss 带来的计算量几乎可以忽略不计。\n•利用了 S 和 G 空间上的一些信息，带来了一定的性能提升。\n•作者做得实验太少， Boundary Loss 是否能普遍提升 performance 还需要更多的实验验证。\n","permalink":"http://landodo.github.io/posts/20210530-boundary-loss/","summary":"Boundary loss for highly unbalanced segmentation Csurka, G., Larlus, D., \u0026amp; Perronnin, F. (2013). What is a good evaluation measure for semantic segmentation? BMVC. (218 Citations.) Kervadec, H., Bouchtiba, J., Desrosiers, C., Granger, É., Dolz, J., \u0026amp; Ayed, I.B. (2019). Boundary loss for highly unbalanced segmentation. Medical image analysis, 67, 101851 . (95 Citations) 论文 PDF : https://openreview.net/pdf?id=S1gTA5VggE 或者（https:","title":"Boundary loss for highly unbalanced segmentation"},{"content":"视网膜血管分割 Retinal vessel segmentation 1. 数据集（Datasets） 视网膜血管分割的公开集最常用的有 DRIVE、STARE 和 CHASE_DB。\n1.1 DRIVE DRIVE: Digital Retinal Images for Vessel Extraction (https://drive.grand-challenge.org/)\nDRIVE 数据库的图像来自荷兰的一个糖尿病视网膜病变筛查项目，筛查人群包括 400 名年龄在 25-90 岁的糖尿病患者。随机抽取了 40 张照片，其中 33 张没有显示任何糖尿病视网膜病变的迹象，7 张显示有轻度早期糖尿病视网膜病变的迹象（25_training、26_training、32_training、03_test、08_test、14_test、17_test）。\nDRIVE 数据集基本信息如下：\n 每张图片的的分辨率为 584 × 565 pixels，3 通道的彩色图片。 训练集 20 张图像，测试集 20 张图像。对于测试案例，有两个人工分割：一个被用作 gold standard，另一个可用于比较计算机生成的分割与独立的人类观察者的分割。 mask 表示 region of interest (RoI)。  DRIVE 数据库的建立是为了能够对视网膜图像中的血管进行分割的比较研究，被用于诊断、筛查、治疗和评估各种心血管和眼科疾病。\n此外，每个人的视网膜血管树都是独一无二的，可用于生物识别。\n1.2 STARE STARE: STructured Analysis of the Retina (https://cecas.clemson.edu/~ahoover/stare/probing/index.html)\nSTARE 数据基本信息：\n 每张图片的分辨率为 700×605 pixels。 共 20 张图片。  1.3 CHASE_DB1 https://blogs.kingston.ac.uk/retinal/chasedb1/\n  每张图片的分辨率为 700×605 pixels。\n  14 个学生的左眼和右眼图像，共 28 张。\n  LadderNet LadderNet：一种基于 U-Net 的多路径医学图像分割网络。\n作者：Juntang Zhuang（https://juntang-zhuang.github.io/），本科清华，目前在 Yale University Ph.D. in Biomedical Engineering。\n 论文时间：ArXiv 2018 年 10 月 论文地址：https://arxiv.org/abs/1810.07810  Abstract U-Net、Attention U-Net、R2-UNet 和 U-Net with residual blocks or blocks with dense con\u0002nections 的信息流的路径数量是十分有限的。本篇论文提出的 LadderNet， 由于有 skip connections、residual blocks，所以有更多的信息流路径，可以被看作是全卷积网络（FCN）的集合。\n在 DRIVE 和 CHASE_DB1 两个视网膜中的血管分割图像数据集上进行测试。\n目前在视网膜分割任务上，基于 GAN 的方法（RV-GAN）取得了 SOTA，DRIVE（AUC = 0.989），CHASE_DB1 （AUC = 0.991）。\nLadderNet 在 DRIVE 和 CHASE_DB1 上都可以排在前十，相比于 U-Net，AUC 提升了 0.01。\n1 Introduction 在各种分割网络的变体中，U-Net 是医学图像分析中使用最广泛的结构，主要是因为带有跳跃连接的 encoder-decoder 结构允许有效的信息流，并且在没有足够大的数据集的情况下表现良好。\n各种 U-Net 的变体，仍属于 encoder-decoder 结构，其中信息流的路径数量是有限的，这是本篇论文的背景。\n However, all these U-Net variants still fall into the encoder\u0002 decoder structure, where the number of paths for information flow is limited. 本篇论文提出了 LadderNet，一种用于语义分割的卷积网络，具有更多的信息流路径。LadderNet 可以被视为 FCN 的集合（即 FCN 是其一种特殊形式），并实验验证了 LadderNet 在视网膜血管分割任务中的优异表现。在视网膜图像中的血管分割任务上验证了其优越性能。\n2. Methons LadderNet 有更多的信息流路径（more paths of information flow）。\nA~E 表示不同的空间尺度的特征图；1、3 为 encoder 分支，2、4 为 decoder 分支。从一个级别到下一个级别，通道的数量增加一倍（例如，A 到 B）。\nLadderNet 和 U-Net 的联系 （1）LadderNet 可以视为 U-Net 的链，1 和 2 看做一个 U-Net，3 和 4 看做是另一个 U-Net。LadderNet 包含两个 U-Net，也可以连接更多的 U-Net 来形成复杂的网络结构。\nLadderNet 的 skip connection 使用的 sum，而 U-Net 使用的是 Concate。\n（2）LadderNet 也可以被看作是多个 FCN 的集合体，残差连接提供了多条信息流路径。\nLadderNet 信息流路径总数随着 encoder-decoder 对的数量和空间层次的数量呈指数级增长。我简单数了一下 LadderNet 的从输入到输出，共有 75 条路径。\nLadderNet 取得较高的精度的原因可以总结为：LadderNet has the potential to capture more complicated features and produce a higher accuracy.\nShared-weights residual block 更多的 encoder-decoder 分支将增加参数的数量和训练的难度。为了解决这个问题，本篇论文提出共享权重的残差块（Fig 1）。受 RCNN 的启发，同一区块中的两个卷积层可以看做是一个递归层。除了两个批处理规范化层是不同的。\n共享权重的残差块结合了 skip connection、recurrent convolution 和 dropout 正则化的力量，参数要比标准的残差块少得多。\n参数量情况 LadderNet vs U-Net，前者的参数量减少了 97%（使用 torchsummary 中的 summary）。\nTotal params: 921,902 Trainable params: 921,902 Non-trainable params: 0 Total mult-adds (M): 41.66 Total params: 31,031,810 Trainable params: 31,031,810 Non-trainable params: 0 Total mult-adds (G): 16.45 对于输入的一张 1×48×48 的特征图，最后得到 2×48×48 的输出。通道数作者的是实现是 (1, 10, 20, 40, 80, 126)，有其他的实现使用的 (1, 16, 32, 64, 128, 128)，level E 不加倍。\nExperiment DRIVE 数据集：40 张彩色图片，20 张训练集，20 张测试集，每张图片 565×584 pixels。为了增加训练的样本，随机采样 190,000 个 48×48 pixels 的 patch，10% 用作验证集。\nCHASE_DB1 数据集：28 张彩色图片，20 张做训练集，8 张做测试集，每张图片 700×605 pixels。随机采样 760,000 个 48×48 pixels 的 patch，10% 用作验证集。\n在数据预处理阶段，3 通道被转换为单通道。\n数据预处理 https://github.com/juntang-zhuang/LadderNet/blob/master/lib/pre_processing.py\n（1）提取彩色眼底图像血管与背景对比度较高的绿色通道；并利用双边滤波对其降噪。\n 在进行 RGB2gray 时，给予 g 通道更高的权重。  #convert RGB image in black and white def rgb2gray(rgb):  assert (len(rgb.shape)==4) #4D arrays  assert (rgb.shape[1]==3)  # 通道顺序为：B G R.  bn_imgs = rgb[:,0,:,:]*0.299 + rgb[:,1,:,:]*0.587 + rgb[:,2,:,:]*0.114  bn_imgs = np.reshape(bn_imgs,(rgb.shape[0],1,rgb.shape[2],rgb.shape[3]))  return bn_imgs （2）限制对比度直方图均衡化(CLAHE) 抑制噪声、提升对比度；全局锐化，抑制伪影、黄斑等噪声。\n（3）局部自适应 Gamma 矫正，抑制光照不均匀因素与中心线反射现象。\n（4）尺度形态学 Top-Hot 变换。\n#My pre processing (use for both training and testing!) def my_PreProc(data):  assert(len(data.shape)==4)  assert (data.shape[1]==3) #Use the original images  #black-white conversion  train_imgs = rgb2gray(data)  #my preprocessing:  train_imgs = dataset_normalized(train_imgs)  train_imgs = clahe_equalized(train_imgs)  train_imgs = adjust_gamma(train_imgs, 1.2)  train_imgs = train_imgs/255. #reduce to 0-1 range  return train_imgs 评估指标 Accuracy (AC), Sensitivity (SE), Specificity (SP) and F1-score.\nTrue Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN).\n实验结果  在实验时，对于 DRIVE 数据集，训练集 20 张 3×584×565 的图像，进行预处理，数据增强后，得到 100,000 个 1×64×64 patch。在华西的电脑上训练一个 Epoch 耗时：13 分钟。\n","permalink":"http://landodo.github.io/posts/20210516-retinal-vesse-segmentation/","summary":"视网膜血管分割 Retinal vessel segmentation 1. 数据集（Datasets） 视网膜血管分割的公开集最常用的有 DRIVE、STARE 和 CHASE_DB。 1.1 DRIVE DRIVE: Digital Retinal Images for Vessel Extraction","title":"视网膜血管分割 Retinal vessel segmentation"},{"content":"!ls Histogram and RoI.ipynb axon02.tif test.jpg Question 2.1 .ipynb cell_nucleus.tif axon01.tif roi.tif  from PIL import Image import numpy as np from PIL import ImageOps import matplotlib.pyplot as plt # 打开图片并转换成灰度图 im = Image.open(\u0026#39;axon01.tif\u0026#39;).convert(\u0026#39;L\u0026#39;) # 反转颜色 im = ImageOps.invert(im) # display in macOS /Applications/Preview.app # im.show()  # display in Jupyter Notebook im ​ ​\n# 将灰度图片转换为 Numpy Array im2array = np.array(im) im2array.shape (204, 1392)  # 将矩阵转化为向量 im2array = im2array.flatten() # 204*1392 = 283968 len(im2array) 283968  # 统计灰度值频率 freq = [0]*256 for i in range(len(im2array)):  freq[im2array[i]] += 1 # 绘制灰度频率直方图 plt.figure(figsize=(25,10)) for i in range(0, 256):  plt.bar(i, freq[i], color=\u0026#39;r\u0026#39;)  plt.grid(color=\u0026#39;gray\u0026#39;, linewidth=0.1) ​ ​\n# 左上角坐标(0,0)，右下角下标(100,100) # 裁剪出 100*100 的区域 box = (0, 0, 100, 100) region = im.crop(box) # 保存 region.save(\u0026#39;roi.tif\u0026#39;) # 读取保存的 RoI roi_im = Image.open(\u0026#39;./roi.tif\u0026#39;) roi_im.size (100, 100)  # 不转换为灰度图片 im = Image.open(\u0026#39;axon01.tif\u0026#39;) im2array = np.array(im) im2array = im2array.flatten()  # 统计像素值对应的像素个数 hist = [0] * (np.max(im2array)+1) for i in range(0, len(im2array)):  hist[im2array[i]] += 1 # 绘制直方图 plt.figure(figsize=(25,10)) for i in range(0, np.max(im2array)):  plt.bar(i, hist[i], color=\u0026#39;r\u0026#39;)  plt.grid(color=\u0026#39;gray\u0026#39;, linewidth=0.1) ​ ​\n使用 Pillow 库绘制直方图 # 求[0,255]像素频率 histogram = im.histogram() # 这个分布符合直觉，大部分都是黑色 print(histogram) [3341, 138883, 751, 581, 499, 536, 582, 11, 645, 687, 717, 774, 800, 895, 946, 1071, 1367, 6, 1131, 1178, 1212, 1256, 1384, 1407, 1491, 1548, 1649, 5, 1726, 1760, 1855, 1869, 1982, 2007, 1993, 2027, 2069, 3, 2161, 2230, 2316, 2194, 2337, 2293, 2304, 2316, 2427, 6, 2474, 2341, 2359, 2380, 2380, 2301, 3120, 2300, 2175, 1, 2208, 2151, 2059, 2089, 2068, 1994, 1891, 1957, 1775, 3, 1753, 1725, 1676, 1718, 1527, 1527, 1490, 1361, 1281, 4, 1718, 1254, 1101, 1092, 1027, 974, 894, 896, 796, 3, 820, 727, 720, 650, 670, 585, 548, 498, 487, 1, 442, 400, 387, 335, 339, 345, 293, 279, 220, 4, 235, 224, 227, 189, 196, 173, 165, 244, 147, 3, 118, 113, 109, 109, 109, 110, 98, 74, 76, 2, 75, 77, 74, 69, 73, 57, 75, 67, 56, 2, 57, 44, 63, 57, 47, 36, 38, 35, 40, 1, 43, 28, 41, 29, 34, 34, 26, 33, 21, 1, 35, 32, 8, 22, 38, 19, 34, 32, 33, 0, 20, 22, 10, 26, 16, 19, 11, 21, 12, 1, 23, 12, 29, 20, 10, 13, 7, 11, 19, 1, 18, 14, 6, 18, 13, 13, 9, 14, 10, 2, 10, 11, 13, 12, 10, 9, 11, 14, 4, 1, 15, 13, 12, 15, 10, 3, 11, 9, 9, 4, 14, 14, 15, 17, 16, 15, 19, 18, 16, 15, 19, 25, 34, 35, 32, 9, 45, 49, 54, 47, 65, 85, 101, 108, 92, 5, 102, 149, 141, 157, 185, 201, 207, 222, 275, 6, 318, 297]  # 最大值 138883 表示有 138883 个值为 1的像素 max(histogram) 138883  for i, value in enumerate(im.histogram()):  print(i, value) 0 3341 1 138883 2 751 3 581 4 499 5 536 6 582 7 11 8 645 9 687 10 717 11 774 12 800 13 895 14 946 15 1071 16 1367 17 6 18 1131 19 1178 20 1212 21 1256 22 1384 23 1407 24 1491 25 1548 26 1649 27 5 28 1726 29 1760 30 1855 31 1869 32 1982 33 2007 34 1993 35 2027 36 2069 37 3 38 2161 39 2230 40 2316 41 2194 42 2337 43 2293 44 2304 45 2316 46 2427 47 6 48 2474 49 2341 50 2359 51 2380 52 2380 53 2301 54 3120 55 2300 56 2175 57 1 58 2208 59 2151 60 2059 61 2089 62 2068 63 1994 64 1891 65 1957 66 1775 67 3 68 1753 69 1725 70 1676 71 1718 72 1527 73 1527 74 1490 75 1361 76 1281 77 4 78 1718 79 1254 80 1101 81 1092 82 1027 83 974 84 894 85 896 86 796 87 3 88 820 89 727 90 720 91 650 92 670 93 585 94 548 95 498 96 487 97 1 98 442 99 400 100 387 101 335 102 339 103 345 104 293 105 279 106 220 107 4 108 235 109 224 110 227 111 189 112 196 113 173 114 165 115 244 116 147 117 3 118 118 119 113 120 109 121 109 122 109 123 110 124 98 125 74 126 76 127 2 128 75 129 77 130 74 131 69 132 73 133 57 134 75 135 67 136 56 137 2 138 57 139 44 140 63 141 57 142 47 143 36 144 38 145 35 146 40 147 1 148 43 149 28 150 41 151 29 152 34 153 34 154 26 155 33 156 21 157 1 158 35 159 32 160 8 161 22 162 38 163 19 164 34 165 32 166 33 167 0 168 20 169 22 170 10 171 26 172 16 173 19 174 11 175 21 176 12 177 1 178 23 179 12 180 29 181 20 182 10 183 13 184 7 185 11 186 19 187 1 188 18 189 14 190 6 191 18 192 13 193 13 194 9 195 14 196 10 197 2 198 10 199 11 200 13 201 12 202 10 203 9 204 11 205 14 206 4 207 1 208 15 209 13 210 12 211 15 212 10 213 3 214 11 215 9 216 9 217 4 218 14 219 14 220 15 221 17 222 16 223 15 224 19 225 18 226 16 227 15 228 19 229 25 230 34 231 35 232 32 233 9 234 45 235 49 236 54 237 47 238 65 239 85 240 101 241 108 242 92 243 5 244 102 245 149 246 141 247 157 248 185 249 201 250 207 251 222 252 275 253 6 254 318 255 297  # 像素总数 283968 = 204*1392 np.sum(histogram) 283968  # 灰度级在 [0,255] len(histogram) 256  plt.figure(figsize=(50,30)) for i in range(0, 256):  plt.bar(i, histogram[i], color=\u0026#39;r\u0026#39;)  plt.grid(color=\u0026#39;gray\u0026#39;, linewidth=0.1) ​ ​\n","permalink":"http://landodo.github.io/posts/20210514-histogram-and-roi/","summary":"!ls Histogram and RoI.ipynb axon02.tif test.jpg Question 2.1 .ipynb cell_nucleus.tif axon01.tif roi.tif from PIL import Image import numpy as np from PIL import ImageOps import matplotlib.pyplot as plt # 打开图片并转换成灰度图 im = Image.open(\u0026#39;axon01.tif\u0026#39;).convert(\u0026#39;L\u0026#39;) # 反转颜色 im = ImageOps.invert(im) # display in macOS /Applications/Preview.app # im.show() # display in Jupyter Notebook im ​ ​ # 将灰度","title":"绘制频率直方图"},{"content":"MNIST (Mixed National Institute of Standards and Technology database) 参考链接：\nhttp://yann.lecun.com/exdb/mnist/\nhttps://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python\n下载 import glob path = glob.glob(\u0026#39;./../data/MNIST/raw/*.gz\u0026#39;) path ['./../data/MNIST/raw/t10k-images-idx3-ubyte.gz', './../data/MNIST/raw/train-images-idx3-ubyte.gz', './../data/MNIST/raw/train-labels-idx1-ubyte.gz', './../data/MNIST/raw/t10k-labels-idx1-ubyte.gz']  # train-images-idx3-ubyte.gz # 60000张训练集图片 # train-labels-idx1-ubyte.gz # 60000张训练集图片对应的标签 # t10k-images-idx3-ubyte.gz # 10000张测试集图片 # t10k-labels-idx1-ubyte.gz # 10000张测试集图片对应的标签 解压 # train-images-idx3-ubyte # train-labels-idx1-ubyte # t10k-images-idx3-ubyte # t10k-labels-idx1-ubyte Load data 下载下来的 MNIST 数据集，有 4 个压缩文件，如果读取？\nimport gzip f = gzip.open(path[0],\u0026#39;r\u0026#39;)  image_size = 28 num_images = 5  import numpy as np f.read(16) # 忽略前 16 字节  buf = f.read(image_size * image_size * num_images) data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32) data = data.reshape(num_images, image_size, image_size, 1) data.shape (5, 28, 28, 1)  import matplotlib.pyplot as plt image = np.asarray(data[2]).squeeze() plt.imshow(image) plt.show() ​ ​\n加载全部数据 The basic format is magic number size in dimension 0 size in dimension 1 size in dimension 2 ..... size in dimension N data import gzip import numpy as np   def training_images():  with gzip.open(\u0026#39;./../data/MNIST/raw/train-images-idx3-ubyte.gz\u0026#39;, \u0026#39;r\u0026#39;) as f:  # first 4 bytes is a magic number  magic_number = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(magic_number)   # second 4 bytes is the number of images  image_count = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(image_count)   # third 4 bytes is the row count  row_count = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(row_count)   # fourth 4 bytes is the column count  column_count = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(column_count)   # rest is the image pixel data, each pixel is stored as an unsigned byte  # pixel values are 0 to 255  image_data = f.read()  images = np.frombuffer(image_data, dtype=np.uint8).reshape((image_count, row_count, column_count))  return images X_train = training_images() 2051 60000 28 28  X_train.shape (60000, 28, 28)  def training_labels():  with gzip.open(\u0026#39;../data/MNIST/raw/train-labels-idx1-ubyte.gz\u0026#39;, \u0026#39;r\u0026#39;) as f:  # first 4 bytes is a magic number  magic_number = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  # second 4 bytes is the number of labels  label_count = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  # rest is the label data, each label is stored as unsigned byte  # label values are 0 to 9  label_data = f.read()  labels = np.frombuffer(label_data, dtype=np.uint8)  return labels y_train = training_labels() y_train.shape (60000,)  plt.figure() for i in range(1,11):  plt.subplot(2, 5, i)  plt.imshow(X_train[i-1, :, :])  plt.title(y_train[i-1]) ​ ​\n加载测试集同理 import gzip import numpy as np   def testing_images():  with gzip.open(\u0026#39;./../data/MNIST/raw/t10k-images-idx3-ubyte.gz\u0026#39;, \u0026#39;r\u0026#39;) as f:  # first 4 bytes is a magic number  magic_number = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(magic_number)   # second 4 bytes is the number of images  image_count = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(image_count)   # third 4 bytes is the row count  row_count = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(row_count)   # fourth 4 bytes is the column count  column_count = int.from_bytes(f.read(4), \u0026#39;big\u0026#39;)  print(column_count)   # rest is the image pixel data, each pixel is stored as an unsigned byte  # pixel values are 0 to 255  image_data = f.read()  images = np.frombuffer(image_data, dtype=np.uint8).reshape((image_count, row_count, column_count))  return images X_test = testing_images() 2051 10000 28 28  X_test.shape (10000, 28, 28)  plt.figure() for i in range(1,11):  plt.subplot(2, 5, i)  plt.imshow(X_test[i-1, :, :]) ​ ​\n","permalink":"http://landodo.github.io/posts/20210514-mnist/","summary":"MNIST (Mixed National Institute of Standards and Technology database) 参考链接： http://yann.lecun.com/exdb/mnist/ https://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python 下载 import glob path = glob.glob(\u0026#39;./../data/MNIST/raw/*.gz\u0026#39;) path ['./../data/MNIST/raw/t10k-images-idx3-ubyte.gz', './../data/MNIST/raw/train-images-idx3-ubyte.gz', './../data/MNIST/raw/train-labels-idx1-ubyte.gz', './../data/MNIST/raw/t10k-labels-idx1-ubyte.gz'] # train-images-idx3-ubyte.gz # 60000张训练集图片 # train-labels-idx1-ubyte.gz # 60000张训练集图片对应的标签 # t10k-images-idx3-ubyte.gz # 10000张","title":"MNIST (Mixed National Institute of Standards and Technology database)"},{"content":"数据预处理 import matplotlib.pyplot as plt import glob from PIL import Image import numpy as np import cv2 # 训练集图像 img_paths = glob.glob(\u0026#39;./../data/vesselseg/DRIVE/training/images/*.tif\u0026#39;) img_paths.sort() img_paths ['./../data/vesselseg/DRIVE/training/images/21_training.tif', './../data/vesselseg/DRIVE/training/images/22_training.tif', './../data/vesselseg/DRIVE/training/images/23_training.tif', './../data/vesselseg/DRIVE/training/images/24_training.tif', './../data/vesselseg/DRIVE/training/images/25_training.tif', './../data/vesselseg/DRIVE/training/images/26_training.tif', './../data/vesselseg/DRIVE/training/images/27_training.tif', './../data/vesselseg/DRIVE/training/images/28_training.tif', './../data/vesselseg/DRIVE/training/images/29_training.tif', './../data/vesselseg/DRIVE/training/images/30_training.tif', './../data/vesselseg/DRIVE/training/images/31_training.tif', './../data/vesselseg/DRIVE/training/images/32_training.tif', './../data/vesselseg/DRIVE/training/images/33_training.tif', './../data/vesselseg/DRIVE/training/images/34_training.tif', './../data/vesselseg/DRIVE/training/images/35_training.tif', './../data/vesselseg/DRIVE/training/images/36_training.tif', './../data/vesselseg/DRIVE/training/images/37_training.tif', './../data/vesselseg/DRIVE/training/images/38_training.tif', './../data/vesselseg/DRIVE/training/images/39_training.tif', './../data/vesselseg/DRIVE/training/images/40_training.tif']  查看不同通道 # 读取其中一张图像 rgb_img = Image.open(img_paths[0]) rgb_img.size (565, 584)  plt.imshow(rgb_img) \u0026lt;matplotlib.image.AxesImage at 0x7fb39515ed68\u0026gt;  ​ ​\nrgb_img = np.asarray(rgb_img) print(rgb_img.shape) (584, 565, 3)  # Blue 通道 plt.imshow(rgb_img[:,:,0], cmap=\u0026#34;gray\u0026#34;) \u0026lt;matplotlib.image.AxesImage at 0x7fb392b4b438\u0026gt;  ​ ​\n# Green 通道 plt.imshow(rgb_img[:,:,1], cmap=\u0026#34;gray\u0026#34;) \u0026lt;matplotlib.image.AxesImage at 0x7fb392c09518\u0026gt;  ​ ​\n# Red 通道 plt.imshow(rgb_img[:,:,2], cmap=\u0026#34;gray\u0026#34;) \u0026lt;matplotlib.image.AxesImage at 0x7fb39578a128\u0026gt;  ​ ​\n使用 Pillow 读取图片，并进行维度的转换 rgb_img = Image.open(img_paths[0]) print(rgb_img.size) # Convert the dimension of imgs to [N,H,W,C] rgb_img = np.expand_dims(rgb_img,0) print(rgb_img.shape) # Convert the dimension of imgs to [N,C,H,W] rgb_img = np.transpose(rgb_img,(0,3,1,2)) print(rgb_img.shape) (565, 584) (1, 584, 565, 3) (1, 3, 584, 565)  #convert RGB image in black and white def rgb2gray(rgb):  assert (len(rgb.shape)==4) #4D arrays  assert (rgb.shape[1]==3)  # 给 Green 通道对比度较高，给更大的权重  bn_imgs = rgb[:,0,:,:]*0.299 + rgb[:,1,:,:]*0.587 + rgb[:,2,:,:]*0.114  bn_imgs = np.reshape(bn_imgs,(rgb.shape[0],1,rgb.shape[2],rgb.shape[3]))  return bn_imgs gray_img = rgb2gray(rgb_img) # 明明已经是单通道了，为什么还是彩色的？ plt.imshow(gray_img[0,0,:,:]) \u0026lt;matplotlib.image.AxesImage at 0x7fb392c29160\u0026gt;  ​ ​\nplt.imshow(gray_img[0,0,:,:], cmap=\u0026#34;gray\u0026#34;) \u0026lt;matplotlib.image.AxesImage at 0x7fb395915d30\u0026gt;  ​ ​\n数据标准化 def dataset_normalized(imgs):  assert (len(imgs.shape)==4) #4D arrays  assert (imgs.shape[1]==1) #check the channel is 1  imgs_normalized = np.empty(imgs.shape)  imgs_std = np.std(imgs)  imgs_mean = np.mean(imgs)  imgs_normalized = (imgs-imgs_mean)/imgs_std  for i in range(imgs.shape[0]):  imgs_normalized[i] = ((imgs_normalized[i] - np.min(imgs_normalized[i])) / (np.max(imgs_normalized[i])-np.min(imgs_normalized[i])))*255  return imgs_normalized img_norm = dataset_normalized(gray_img) img_norm.shape (1, 1, 584, 565)  plt.imshow(img_norm[0,0,:,:], cmap=\u0026#34;gray\u0026#34;) \u0026lt;matplotlib.image.AxesImage at 0x7fb397b4b2b0\u0026gt;  ​ ​\n限制对比度直方图均衡化(CLAHE)，在抑制噪声的同时提升血管与背景的对比度 # CLAHE (Contrast Limited Adaptive Histogram Equalization) #adaptive histogram equalization is used. In this, image is divided into small blocks called \u0026#34;tiles\u0026#34; (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied def clahe_equalized(imgs):  assert (len(imgs.shape)==4) #4D arrays  assert (imgs.shape[1]==1) #check the channel is 1  #create a CLAHE object (Arguments are optional).  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))  imgs_equalized = np.empty(imgs.shape)  for i in range(imgs.shape[0]):  imgs_equalized[i,0] = clahe.apply(np.array(imgs[i,0], dtype = np.uint8))  return imgs_equalized img_clahe = clahe_equalized(img_norm) img_clahe.shape (1, 1, 584, 565)  plt.imshow(img_clahe[0,0,:,:], cmap=\u0026#34;gray\u0026#34;) \u0026lt;matplotlib.image.AxesImage at 0x7fb398bedf28\u0026gt;  ​ ​\n经 filter 滤波对图像进行全局锐化，抑制 CLAHE 增强后图像的伪影与黄斑等噪声影响，突显血管信息 # laplacian kernle K = [[0., 1., 0.],[1., -4., 1.], [0., 1., 0.]] # Image sharpening by laplacian filter  def laplacian_sharpening(img, K_size=3):   H, W = img.shape[2], img.shape[3]  # zero padding  pad = K_size // 2  out = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)  out[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)  tmp = out.copy()   # laplacian kernle  K = [[0., 1., 0.],[1., -4., 1.], [0., 1., 0.]]   # filtering and adding image -\u0026gt; Sharpening image  for y in range(H):  for x in range(W):  # core code  out[pad + y, pad + x] = (-1) * np.sum(K * (tmp[y: y + K_size, x: x + K_size])) + tmp[pad + y, pad + x]   out = np.clip(out, 0, 255)  out = out[pad: pad + H, pad: pad + W].astype(np.uint8)   return out img_sharp = laplacian_sharpening(img_clahe) img_sharp.shape (584, 565)  plt.imshow(img_sharp[:,:], cmap=\u0026#39;gray\u0026#39;) \u0026lt;matplotlib.image.AxesImage at 0x7fb39810f9b0\u0026gt;  ​ ​\n利用局部自适应 Gamma 矫正 def adjust_gamma(imgs, gamma=1.0):  assert (len(imgs.shape)==4) #4D arrays  assert (imgs.shape[1]==1) #check the channel is 1  # build a lookup table mapping the pixel values [0, 255] to  # their adjusted gamma values  invGamma = 1.0 / gamma  table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\u0026#34;uint8\u0026#34;)  # apply gamma correction using the lookup table  new_imgs = np.empty(imgs.shape)  for i in range(imgs.shape[0]):  new_imgs[i,0] = cv2.LUT(np.array(imgs[i,0], dtype = np.uint8), table)  return new_imgs img_gamma = adjust_gamma(img_clahe) plt.imshow(img_gamma[0,0,:,:], cmap=\u0026#39;gray\u0026#39;) \u0026lt;matplotlib.image.AxesImage at 0x7fb39993ba20\u0026gt;  ​ ​\n # Reading the image named \u0026#39;input.jpg\u0026#39; input_image = cv2.imread(img_paths[0]) input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY) # Getting the kernel to be used in Top-Hat filterSize =(3, 3) kernel = cv2.getStructuringElement(cv2.MORPH_RECT,  filterSize)  # Applying the Top-Hat operation tophat_img = cv2.morphologyEx(img_gamma[0,0,:,:],  cv2.MORPH_TOPHAT,  kernel) plt.imshow(img_gamma[0,0,:,:] - tophat_img, cmap=\u0026#39;gray\u0026#39;) \u0026lt;matplotlib.image.AxesImage at 0x7fb39ba39f28\u0026gt;  ​ ​\n","permalink":"http://landodo.github.io/posts/20210514-preprocessing/","summary":"数据预处理 import matplotlib.pyplot as plt import glob from PIL import Image import numpy as np import cv2 # 训练集图像 img_paths = glob.glob(\u0026#39;./../data/vesselseg/DRIVE/training/images/*.tif\u0026#39;) img_paths.sort() img_paths ['./../data/vesselseg/DRIVE/training/images/21_training.tif', './../data/vesselseg/DRIVE/training/images/22_training.tif', './../data/vesselseg/DRIVE/training/images/23_training.tif', './../data/vesselseg/DRIVE/training/images/24_training.tif', './../data/vesselseg/DRIVE/training/images/25_training.tif', './../data/vesselseg/DRIVE/training/images/26_training.tif', './../data/vesselseg/DRIVE/training/images/27_training.tif', './../data/vesselseg/DRIVE/training/images/28_training.tif', './../data/vesselseg/DRIVE/training/images/29_training.tif', './../data/vesselseg/DRIVE/training/images/30_training.tif', './../data/vesselseg/DRIVE/training/images/31_training.tif', './../data/vesselseg/DRIVE/training/images/32_training.tif', './../data/vesselseg/DRIVE/training/images/33_training.tif', './../data/vesselseg/DRIVE/training/images/34_training.tif', './../data/vesselseg/DRIVE/training/images/35_training.tif', './../data/vesselseg/DRIVE/training/images/36_training.tif', './../data/vesselseg/DRIVE/training/images/37_training.tif', './../data/vesselseg/DRIVE/training/images/38_training.tif', './../data/vesselseg/DRIVE/training/images/39_training.tif', './../data/vesselseg/DRIVE/training/images/40_training.tif'] 查看不同通道 # 读取其中一张图像 rgb_img = Image.open(img_paths[0])","title":"图像预处理"},{"content":" FCN：Fully Convolutional Networks for Semantic Segmentation\n https://arxiv.org/abs/1411.4038  U-Net：U-Net: Convolutional Networks for Biomedical Image Segmentation\n https://arxiv.org/abs/1505.04597  SegNet：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\n https://arxiv.org/pdf/1511.00561.pdf   概览 论文所要解决的问题：\nencoder layer 的最大池化和子采样可以实现更多的平移不变性（translation invariance），以实现鲁棒分类，但相应地也存在特征图的空间分辨率的损失。随着最大池化和子采样层的堆叠，边界细节丢失的有损特征图（空间分辨率损失），对边界划分至关重要的分割任务来说是不利的。因此，解决方案是：在 encoder 中进行降采样之前，先捕获并存储特征图中的一些信息。如果前向传播过程中的内存不受限制，那么可以存储所有的 encoder 每次降采样之间的所有特征图，这些特征图将用于指导升采样。\nFCN、U-Net 采用的是将 encoder 每次降采样前的特征图全部存储，在 decoder layer 进行浅层特征和深层特征合并时，U-Net 使用的是拼接方法（Concatenation），而 FCN 使用的是求和。\n由于内存的限制，本文提出了一种更有效的方式来存储这些信息。 它只存储最大池化索引（max-pooling indices），即为每个编码器特征图记忆每个池化窗口中最大特征值的位置索引。\n   encoder 和 decoder：\nSegNet 将网络前面提取特征的部分称为编码器（Encoder），后面上采样的部分称为解码器（Decoder）。从这篇文献开始，encoder-decoder 开始广泛使用在分割任务中。（可能）\nencoder-decoder Hinton 2006 年 提出。\nhttps://www.cs.toronto.edu/~hinton/science.pdf\n   decoder 与encoder 相对应。\n  SegNet 的 decoder 使用从相应 encoder 保存下来的最大池化索引（max-pooling indices），用这个来指导输入的特征图进行非线性上采样。\n  max-pooling indices 的优势：\n (i) it improves boundary delineation. (ii) it reduces the number of parameters enabling end-to-end training.（我表示怀疑！） (iii) this form of upsampling can be incorporated into any encoder-decoder architecture with only a little modification.    SegNet Architecture SegNet 由 3 个主要组件：\n（1）encoder network\n（2）decoder network\n（3）pixel-wise classification layer\n去掉全连接的 VGG16 有 13 个卷积层。可以用在大数据集预训练好的参数进行初始化。VGG16 的参数量是 134M，去掉全连接层后的参数量为 14.7M。\n\u0026#39;vgg16\u0026#39;: [64, 64, \u0026#39;M\u0026#39;, 128, 128, \u0026#39;M\u0026#39;, 256, 256, 256, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;], 每个编码器层都有相应的解码层，因此解码网络也有 13 层。最后的解码器的输出被送入 multi-class soft-max classififier 为每个像素独立产生类概率。\nmax-pooling indices 由于内存的限制，本文提出了一种更有效的方式来存储 encoder 降采样前的特征图信息。 它只存储最大池化索引（max-pooling indices）：每个池化窗口中最大值的位置索引。\n对于输入的特征图，decoder 使用其相对应的 encoder 存储下来的 max-pooling indices 进行上采样。\n上采样后得到稀疏的特征图，然后将这些特征图与可训练的 decoder 卷积核进行卷积，以产生密集的特征图。卷积之后是 Batch Normalization。\n需要注意：对应于第一个 encoder（最接近输入图像）对应的 decoder 会产生一个多通道特征图（最后一个 decoder），尽管其 encoder 输入有 3 个通道（RGB）。网络中的其他 decoder 产生的特征图与其 encoder 输入的大小和通道数相同。\n1 Boundary F1-measure (BF) mIoU 倾向于区域平滑性，而不评价边界精度，这一点 FCN 的作者最近也提到了。\nBoundary F1-measure (BF)：衡量边界轮廓的指标。对所有的 ground-truth 和 prediction 的轮廓（contour）点进行比较，计算准确率和召回率，得到 F1-score。轮廓不会完全精准，因此这里的准确指的是在一定容忍范围内的相等（tolerance distance）。\n The key idea in computing a semantic contour score is to evaluate the F1-measure [59] which involves computing the precision and recall values between the predicted and ground truth class boundary given a pixel tolerance distance. We used a value of 0.75% of the image diagonal as the tolerance distance. The F1-measure for each class that is present in the ground truth test image is averaged to produce an image F1-measure. Then we compute the whole test set average, denoted the boundary F1-measure (BF) by average the image F1 measures.\nCsurka, G., D. Larlus, and F. Perronnin. \u0026ldquo;What is a good evaluation measure for semantic segmentation?\u0026rdquo; Proceedings of the British Machine Vision Conference, 2013, pp. 32.1-32.11.\n     $B_{gt}^{c}$ ：be the boundary map of the binarized ground truth segmentation map for class c.\n  $B_{ps}^{c}$：is the contour map for the binarized predicted segmentation map $S_{ps}^{c}$.\n  θ is the distance error tolerance\n   IoU = TP / (TP + FP + FN)\nP = TP / (TP + FP)\nR = TP / (TP + FN)\n BF 的实现：http://mi.eng.cam.ac.uk/projects/segnet/computeBFmeasure.m\nExperiment Decoder Variant\n（1）SegNet-Basic：4 encoder，4 decoder。\n（2）SegNet-SingleChannelDecoder：卷积核只有一个通道，大大减少了可训练参数的数量和推理时间（分组卷积）。\n（3）FCN-Basic：FCN-Basic 是与 SegNet-Basic 对应的 FCN 版本，共享相同的 Encoder 网络，而 Decoder 网络采用 FCN 所提出的结构。最后一个 encoder 将通道数降为 K。\n（4）FCN-Basic-NoAddition：一个 FCN-Basic 模型的变体，它摒弃 encoder 特征图添加步骤，只学习上采样核。因此，不需要存储 encoder 的特征图。 TABLE 1 中的 n/a 表示：“表格中本栏目(对我)不适用”。\n（5）Bilinear-Interpolation：使用固定的双线性插值权重来研究上采样，因此上采样不需要学习。（不太了解这个）\n（6）SegNet-BasicEncoderAddition：在每一层将 64 个 encoder 特征图添加到 SegNet decoder 的相应输出特征图中，以创建一个内存消耗更大的 SegNet 变体。（仿照 FCN，因为值存 64 个，因此通道数要少于 FCN，后面卷积时，卷积核的通道数也少一些，因此参数量少。1.425M V.S. 1.625M）\n（7）FCN-Basic-NoDimReduction：一个更耗费内存的 FCN-Basic 变体，对 encoder 特征图不进行维度减少。\nTABLE 1 的出的结论：\n","permalink":"http://landodo.github.io/posts/20210418-segnet/","summary":"FCN：Fully Convolutional Networks for Semantic Segmentation https://arxiv.org/abs/1411.4038 U-Net：U-Net: Convolutional Networks for Biomedical Image Segmentation https://arxiv.org/abs/1505.04597 SegNet：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation https://arxiv.org/pdf/1511.00561.pdf 概览 论文所要解决的问题： encoder","title":"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"},{"content":"SegNet http://mi.eng.cam.ac.uk/projects/segnet/\n时间：2015 年\n Max Pooling 后特征图会变小，空间分辨率的下降对于边界的勾画是不利的，在文章的论述中，作者认为需要对这种边界信息进行存储保留，比如作者将每一个 feature map 中每一个 pooling 窗口中最大值的 location 记录了下来。\n  SegNet 在上采样过程中使用了在 encoder 端所获得的 pooling indices，用这个来指导上采样过程。\n  并不推荐在医学图像这样精度要求非常严格的领域内使用 SegNet，相对于 pooling indices，直接传送 feature map 的信息量还是高很多。\n 1 Introduction 最大池化和子采样降低了特征图的分辨率。\n在提取的图像表示中保留边界信息很重要。\nSegNet 的关键组成部分是 decoder 网络，它由一个层次的 encoder 器组成，每个 decoder 对应一个 encoder。\n其中，decoder 使用从相应 encoder 接收到的最大池化索引（max-pooling indices）对其输入的特征图进行非线性上采样。\nmax-pooling indices 的优势：\n（1）改善了边界划分\n（2）减少了实现端到端训练的参数数量\n（3）只需稍加修改，就可以加入到任何的 encoder-decoder 网络中\n2 Literature Review encoder 网络权重通常是在大型 ImageNet 对象分类数据集上进行预训练。\nFCN 中的 decoder 对其输入特征图进行上采样，并与相应的 encoder 特征图相结合，以产生下一个解码器的输入。它的 encoder 网络（134M）有大量的可训练参数，但 decoder 网络非常小（0.5M）。\n FCN8s 的 decoder 就不那么小了！decoder 也是进行 4 次 deconv，也 encoder 相对应。\n 忽略高分辨率的特征图肯定会导致边缘信息的丢失。\n多尺度的深层结构，结合不同层的特征图，能够提供全局和局部的上下文信息，使用特征图的早期编码层保留了更多的高频细节，可以帮助获取更清晰的类边界。推理的成本也很高，有多条卷积路径进行特征提取。\nSegNet 则使用解码器来获取特征，以实现精确的像素化分类。\n最近提出的解卷积网络及其半监督变体解耦网络利用编码器特征图的最大位置（pooling indices）在解码器网络中进行非线性上采样。\nSegNet 抛弃了 VGG16 编码器网络的完全连接层，这使得网络在相关的训练集上使用 SGD 优化来训练。\n对于每个样本，在池化过程中计算出的最大位置的索引被存储并传递给解码器。解码器通过使用存储的池化索引对特征图进行上采样。\n3 Architecture SegNet 有一个编码器网络和一个相应的解码器网络，然后是最后的像素分类层。\n（1）encoder network（2）decoder network（3）pixel-wise classification layer\n去掉全连接的 VGG16 有 13 个卷积层。可以用在大数据集预训练好的参数进行初始化。VGG16 的参数量是 134M，去掉全连接层后的参数量为 14.7M。\n每个编码器层都有相应的解码层，因此解码网络也有 13 层。最后的解码器的输出被送入 multi-class soft-max classififier 为每个像素独立产生类概率。\n FCN 也没有全连接层，为什么 SegNet 和 FCN 的模型大小差那么多？\nTODO：使用 summary 比较一下两者的参数！\n 编码器网络中的每个 encoder 都与一组卷积核进行卷积后，产生一组特征图，然后是 BN、ReLU。\n之后，进行最大池化（2×2，stride=2，非重叠），并将所得输出以 2 的系数进行子采样。\n 以 2 的系数进行子采样怎么理解？\n 最大池化用于实现输入图像中的小空间位移的平移不变性。子采样的结果是为特征图中的每个像素提供一个大的输入图像上下文（spatial window）。\n Max-pooling is used to achieve translation invariance over small spatial shifts in the input image. Sub-sampling results in a large input image context (spatial window) for each pixel in the feature map.\n怎么理解子采样？最大池化难道不就是子采样中的一种吗？\n 虽然几层最大池化和子采样可以实现更多的平移不变性（translation invariance），以实现鲁棒分类，但相应地也存在特征图的空间分辨率的损失。 While several layers of max-pooling and sub-sampling can achieve more translation invariance for robust classification correspondingly there is a loss of spatial resolution of the feature maps.\n 论文所要解决的问题：\n随着最大池化和子采样层的堆叠，越来越大的有损（边界细节丢失）特征图对边界划分至关重要的分割任务来说不利。因此，在进行子采样之前，有必要在编码器特征图中捕获并存储边界信息。如果推理过程中的内存不受限制，那么可以存储所有的编码器特征图（子采样后）。\n 存储所有的特征图，这不就是 UNet 吗？\nUnet 有一个 copy and crop！\n 由于内存的限制，本文提出了一种更有效的方式来存储这些信息。 它涉及到只存储最大池化索引（max-pooling indices），即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。\n每个 2×2 的池化窗口可以使用 2 个比特来完成，与记忆浮点精度的特征图相比，存储效率更高。\n这种较低的内存存储量导致精度略有下降，但仍适合实际应用。\n解码器网络中的解码器使用从相应编码器特征图中记忆的最大池化指数对其输入特征图进行上采样。\n对于输入的特征图，decoder 使用其相对应的 encoder 存储下来的 max-pooling indices 进行上采样。\n The appropriate decoder in the decoder network upsamples its input feature map(s) using the memorized max-pooling indices from the corresponding encoder feature map(s).\n 上采样后得到稀疏的特征图，然后将这些特征图与可训练的 decoder 卷积核进行卷积，以产生密集的特征图。\n卷积之后是 Batch Normalization。\n需要注意：对应于第一个 encoder（最接近输入图像）对应的 decoder 会产生一个多通道特征图（最后一个 decoder），尽管其 encoder 输入有 3 个通道（RGB）。\n网络中的其他 decoder 产生的特征图与其 encoder 输入的大小和通道数相同。\n最后一个 decoder 输出的特征图，输入到 soft-max classifier，最后的输出是一个 K 通道的概率图像，其中 K 是类的数量。 预测分割对应于每个像素处概率最大的类。\n将 SegNet 与 DeconvNet（更大的参数量）、UNet 进行比较。\nU-Net 并不重复使用 pooling indices，而是将整个特征图（以更多的内存为代价）转移到相应的解码器上，并将其连接成上采样(通过解卷积)解码器特征图。\n3.1 Decoder Variant 许多分割网络的架构都是用相同的 encoder， 它们只是在其 decoder 网络的形式上有所不同。\nSegNet-Basic：4 encoder，4 decoder。\nSegNet-Basic 中的所有 encoder 都会进行最大池化和子采样，相应的 decoder 使用接收到的最大池化索引对其输入进行上采样。\n在encoder 和 decoder 网络中，每一个卷积层之后都会使用 BN。\ndecoder 网络中不使用 ReLU。\n图 3 的左边是 SegNet（也是 SegNet-Basic）使用的解码技术，其中上采样步骤不涉及学习。得到的稀疏特征图可以通过可训练的卷积使其密集化。\nSegNet-SingleChannelDecoder：卷积核只有一个通道，大大减少了可训练参数的数量和推理时间（分组卷积）。\n图 3 中右边是 FCN（也是 FCN-Basic）解码技术。FCN 模型的重要设计要素是 decoder 特征图的降维步骤。\n ✅FCN-Basic（0.65M） 的参数量为什么要少于 SegNet-Basic（1.425M）？\nFCN-Basic 是与 SegNet-Basic 对应的 FCN 版本，共享相同的 Encoder 网络，而 Decoder 网络采用 FCN 所提出的结构。\n答：FCN-Basic 把 dimensionality reduction 了。看看 FCN-Basic-NoDimReduction，参数量、内存消耗就差不多了。\n FCN decoder 在推理过程中需要存储 encoder 特征图。这带来了一定的内存消耗，以 32 位浮点精度存储 FCN-Basic 第一层 180×240 分辨率的 64 个特征图需要 11MB。\nSegNet 对 max-pooing indices 的存储成本要求几乎可以忽略不计（如果使用每 2×2 个池化窗口 2 bits 存储，则需要 0.17MB）。\nFCN-Basic-NoAddition：一个 FCN-Basic 模型的变体，它摒弃 encoder 特征图添加步骤，只学习上采样核。因此，不需要存储 encoder 的特征图。 TABLE 1 中的 n/a 表示：“表格中本栏目(对我)不适用”。\nBilinear-Interpolation：使用固定的双线性插值权重来研究上采样，因此上采样不需要学习。（不太了解这个）\nSegNet-BasicEncoderAddition：在每一层将 64 个 encoder 特征图添加到 SegNet decoder 的相应输出特征图中，以创建一个内存消耗更大的 SegNet 变体。（仿照 FCN）\n ✅Storage 为什么会比 FCN-Basic 大那么多？FCN 存储的是全部的特征图，按理说应该更大的才对！\n答：因为 FCN-Basic 进行 dimensionality reduction 了。如果是 FCN-NoDimReduction，Storage 的参数量就上来了，两者是相等的，都是增加了 64 倍。\n FCN-Basic-NoDimReduction：一个更耗费内存的 FCN-Basic 变体，对 encoder 特征图不进行维度减少。与 FCN-Basic 不同，最终的 encoder 特征图在传递给 decoder 网络之前，并没有被压缩到 K 个通道。因此，每个 decoder 末端的通道数与对应的 encoder 相同（即 64 个）。\n ✅什么意思？FCN 的 encoder 使用的 VGG，通道数 3 \u0026ndash; 64 \u0026ndash; 126 \u0026ndash; 256 \u0026ndash; 512。意思是在第 4 次卷积是，将通道从 256 压缩到 K ？\n答：压缩的最后一个 encoder，即最后一个 encoder 输出的特征图的通道数为 K，K 为类别数。\n 3.2 Training 类别平衡（class balancing）：当训练集中每个类的像素数量变化较大时（如 CamVid 数据集中道路、天空和建筑像素占主导地位），那么就需要根据真实类的情况对损失进行不同的加权。\n  median frequency balancing\n  natural frequency balancing\n  3.3 Analysis 3 个性能指标（performance measures）：\n（1）Global accuracy (G)：衡量数据集中正确分类像素的百分比。\n（2）Class average accuracy (C)：所有类别预测精度的平均值。\n（3）mean Intersection over Union (mIoU)：所有类的平均 IoU。mIoU 度量是一个比类平均精度更严格的度量，因为它惩罚假阳性预测（FP）\n mIoU：在语义分割中，mIoU 才是标准的准确率度量方法。它是分别对每个类计算（真实标签和预测结果的交并比）IoU，然后再对所有类别的 IoU 求均值。\nIoU = TP / (TP + FP + FN)\n mIoU 指标又称 Jacard 指数，是基准测试中最常用的指标。这个指标并不总是符合人类对优质细分的定性判断（等级）。\nmIoU 倾向于区域平滑性，而不评价边界精度，这一点 FCN 的作者最近也提到了。\nBoundary F1-measure (BF)：衡量边界轮廓的指标。对所有的 ground-truth 和 prediction 的轮廓（contour）点进行比较，计算准确率和召回率，得到 F1-score。轮廓不会完全精准，因此这里的准确指的是在一定容忍范围内的相等（tolerance distance）。\n The key idea in computing a semantic contour score is to evaluate the F1-measure [59] which involves computing the precision and recall values between the predicted and ground truth class boundary given a pixel tolerance distance. We used a value of 0.75% of the image diagonal as the tolerance distance. The F1-measure for each class that is present in the ground truth test image is averaged to produce an image F1-measure. Then we compute the whole test set average, denoted the boundary F1-measure (BF) by average the image F1 measures.\nCsurka, G., D. Larlus, and F. Perronnin. \u0026ldquo;What is a good evaluation measure for semantic segmentation?\u0026rdquo; Proceedings of the British Machine Vision Conference, 2013, pp. 32.1-32.11.\n 结果分析：\n（1）没有学习的双线性差值法采样的效果是最差的。\n（2）\nSegNet-Basic 只存储 max-pooling indices，使用更少的 memory。 每个 decoder layer 有 64 个特征图，前向传播会慢一些。  FCN-Basic 存储所有的 encoder 特征图，多消耗了 11 倍的 memory。 使用了 dimensionality reduction，降维 11（CamVid 数据集有 11 类），传播快。  SegNet-Basic 的 decoder 要大于 FCN-Basic，总体上看 SegNet 的网络更大，Traning accuracy 更高。\n但是由于 FCN 存储了 encoder 的所有特征图，因此分割边界（BF）更精确。\n（3）SegNet-Basic 和 FCN-Basic-NoAddition\nSegNet-Basic 和 FCN-Basic-NoAddition-NoDimReduction\n得出结论：捕捉 encoder 特征图中存在的信息对于提高性能至关重要。\n（4）更大的模型，FCN-Basic-NoDimReduction 和 SegNet-EncoderAddition，都能有更好的精度。\n（5）FCN-Basic-NoDimReduction 的表现是最好的，这再次强调了分割架构中内存和精度之间的权衡。\nNatural frequency balancing 表示没有使用 class balancing。\n 这个值得去深入了解。\n 结论总结：\n4 Benchmark DeepLab-LargeFOV is most efficient model，配合 CRF （条件随机场）后处理可以产生有竞争力的结果。\nFCN 表示使用固定的双线性上采样方法（fixed bilinear upsampling），没有可学习的参数。\nTABLE 2 为 SegNet 和传统方法的比较。\n TABLE 4 的 BF 指标为什么那么差？\n✅ 整体性能较差的原因之一是在这个分割任务中，类的数量很多，其中很多类占据了图像的一小部分，而且出现频率不高。\n性能差的另一个原因可能在于这些深度架构（都是基于 VGG 架构）对室内场景的大变化无能为力。\n TABLE 5：类别越大，精度会更好一些。\n5 Discussion and Feature Work 从移动的汽车上捕捉到的户外场景图像更容易分割，深度架构表现稳健。indoor scene Segmentation 任务更具有挑战性。\n 代码部分 1. 如果在 encoder 中保存 max-pooling indices? 2. 如何在 decoder 中使用 max-pooling indices? 3. BF 指标？ ","permalink":"http://landodo.github.io/posts/20210413-segnet-notes/","summary":"SegNet http://mi.eng.cam.ac.uk/projects/segnet/ 时间：2015 年 Max Pooling 后特征图会变小，空间分辨率的下降对于边界的勾画是不利的，在文章的论述中，作者认为需要对这种边界信息进行存储保留，比如","title":"SegNet"},{"content":"Image Segmentation Using Deep Learning: A Survey 一篇图像分割的综述。\n arXiv：https://arxiv.org/abs/2001.05566  写这篇笔记时（2021.04.01），被引用量为 140。2020 年 01 月份发布的论文，对在此之前的文献进行了全面的回顾。\n Abstract 这篇 Survey 涵盖了语义和实例级分割的前沿工作（pioneering works for semantic and instance-level segmentation）\n (FCN) fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, (GAN) and generative models in adversarial settings   1 Introduction 早期的图像分割算法：thresholding, histogram-based bundling, regiongrowing, k-means clustering, watersheds, to more advanced algorithms such as active contours, graph cuts, conditional and Markov random fields, and sparsity-based methods.\n在过去的几年里，深度学习（DL）模型已经产生了新一代的图像分割模型，其性能得到了显著的提高——通常可以达到最高的准确率。\n图像分割（Image segmentation）可以表述为带有语义标签的像素的分类问题（semantic segmentation）或单个对象的分割问题（instance segmentation）。\n Semantic Segmentation： 语义分割用一组对象类别（如人、车、树、天空）对所有图像像素进行像素级标签。它一般比图像分类更难进行，因为图像分类预测整个图像的单一标签。 Instance Segmentation：实例分割通过检测和划分图像中的每一个感兴趣的对象（例如，各个人的分区），进一步扩大了语义分割的范围。  这篇综述包含到 2019 年图像分割的最新文献，讨论了一百多种基于深度学习的细分方法。\n将基于深度学习的工作按照其主要技术贡献进行如下划分：\n 2 OVERVIEW OF DEEP NEURAL NETWORKS 介绍最杰出的深度学习架构，包含，CNN、RNN、LSTM、GAN。\n之后的发展出来的 Transformer、GRU 等不进行介绍。\nTransfer Learning\n在图像分割的任务，许多人使用在 ImageNet（比大多数图像分割数据集更大的数据集）上训练的模型作为分割网络的 encoder 部分。\n 2.1 Convolutional Neural Networks (CNNs) Fukushima：“Neocognitron”\nHubel，Waibel：视觉皮层的层次性感受野模型。\nLeNet\nCNN 主要由三类层组成。\n Convolution layer Nonlinear layer Pooling layer  一些最著名的 CNN 架构包括：AlexNet、VGGNet、ResNet、GoogLeNet、MobileNet 和 DenseNet。\n 2.2 Recurrent Neural Networks (RNNs) and the LSTM 处理序列数据（sequential data）\nRNN 通常在长序列上存在问题，它们在许多情况下无法捕捉到长期的依赖性。LSTM 被设计用来解决这个问题，结构内部包含三个门（input gate, output gate, forget gate），它们调节信息流入和流出一个存储单元，该单元在任意时间间隔内存储数值。\n 2.3 Encoder-Decoder and Auto-Encoder Models encoder：compresses the input into a latent-space representation. feature(vector) representation\ndecoder：predict the output from the latent space representation.\n 2.4 Generative Adversarial Networks (GANs) generator and a discriminator.\n传统 GAN 中的生成器网络 G = z → y 学习从噪声 z（有一个先验分布）到目标分布 y 的映射，这与“真实”样本相似。\n判别器网络 D 试图将生成的样本与“真实”样本区分开来。\n自 GAN 发明以来，研究者们努力从多个方面改进/改造 GAN。\n 3 基于DL的图像分割模型 对 2019 年之前提出的百余种基于深度学习的细分方法进行了详细的评述。\n This section provides a detailed review of more than a hundred deep learning-based segmentation methods proposed until 2019\n  3.1 Fully Convolutional Networks 语义图像分割的第一个深度学习作品之一，使用完全卷积网络（FCN），只包括卷积层。\n通过使用跳层连接进行特征图融合，将语义信息（深层）和外观信息（浅层）结合起来，以产生准确的分割。\nmilestone\nParseNet：通过使用一层的平均特征来增强每个位置的特征，为 FCN 增加全局上下文。\n unpooled 和 concatenated  3.2 Convolutional Models With Graphical Models FCN 忽略了潜在有用的场景级语义上下文，为了整合更多的上下文，一些方法将概率图形模型，如条件随机场（CRFs）和马尔科夫随机场（MRFs）纳入 DL 架构。\n为了克服深度 CNN 的本地化特性差的问题（poor localization property of deep CNNs），（1）将最后 CNN 层的响应与全连接的 CRF 相结合。\n（2）还有方法通过联合训练 CNN 和全连接 CRF 进行语义图像分割。\n（3）另一方式是基于 contextual deep CRFs，探索了 “patch-patch” context (between image regions) 和 “patch-background” context，通过使用上下文信息来改善语义分割。\n（4）将丰富的信息纳入 MRF，包括高阶关系和标签上下文的混合（high-order relations and mixture of label contexts）。\n3.3 Encoder-Decoder Based Models ","permalink":"http://landodo.github.io/posts/20210401-image-segmentation-survey/","summary":"Image Segmentation Using Deep Learning: A Survey 一篇图像分割的综述。 arXiv：https://arxiv.org/abs/2001.05566 写这篇笔记时（2021.04.","title":"Image Segmentation Using Deep Learning: A Survey"},{"content":" arXiv: https://arxiv.org/abs/2004.08955 时间：2020 年 04 月  SENet（17.09）、SKNet（19.03） 和 ResNeSt（20.04） 可以放在一起进行阅读，这三篇都是注意力机制的经典文献。其体现了一种层层递进、升级强化的过程。\n最后一篇 ResNeSt 可以看做是 SENet 和 SKNet 的集大成之作。\nResNeSt 把 Group Convolution玩得算是炉火纯青，我认为这是它的一个最大的特点。它没有提出什么新的东西，只是把现有的技术（Attention Mechanism ）进行了完美的结合。\nAbstract  ResNeSt integrates the channel\u0002 wise attention with multi-path network representation.\n 本篇论文提出一个模块化的结构：将通道注意力应用到了多分支结构之上。\nResNeSt 中的 ”S“，表示的就是 Split。ResNeSt 由 Split-Attention Block 堆叠而成。\nResNeSt 的取得的成绩如下，有 3 个数据集取得了 SOTA（State of the Art）。\nResNeSt 对比的是 EfficientNet。\nRelate Work (1) CNN Architectures:\n AlexNet: shifted from engineering handcrafted features to engineering network architectures. Network in Network: first uses a global average pooling layer to replace the heavy fully connected layers, and adopts 1×1 convolutional layers to learn non-linear combination of the featuremap channels, which is the first kind of featuremap attention mechanism. （第一个注意力机制） VGG-Net: stacking the same type of network blocks repeatedly Highway Network: highway connection makes the information flow across several layers ResNet: one of the most successful CNN archi\u0002tectures  (2) Multi-path and featuremap Attention:\n GoogLeNet: Multi-path rep\u0002resentation ResNeXt: group convolution, converts the multi-path structure into a unified operation. SE-Net: channel-attention mechanism SK-Net: featuremap attention across two network branches.  总结 ResNeSt：integrates the channelwise attention with multi-path network representation.Split-Attention Network ResNeSt（Split-Attention Network）的核心是 Split-Attention Block。\n解析 Split-Attention Block Split-Attention Block 和核心是 Featuremap Group 和 Split Attention。\n（1）Featuremap Group\n从 Figure 2(Right) 可以看到，ResNeSt Block 的分支结构中又包含着分支，有一种套娃的感觉。我把它想象成二叉树的结构，从根节点到叶子节点的路径称为一个分支。ResNeSt Block 中超参数 K（cardinality）、R（radix）控制分支的数量，总分支数为 G=KR。\n有多少分支，就表示需要将输入的特征图通道数分成多少组。对于输入的特征图 64×16×16，K=2、R=2 表示将特征图分为 4（K*R）组，每组的特征图大小为 16×16×16。\n即：Featuremap Group 总数 为 K×R，每条分支的特征图的通道数等于原始特征图的通道数除以Featuremap Group 总数。可以应用一系列的变换 {F1，F2，\u0026hellip;\u0026hellip;FG} 到每一个单独的组。下图是 K=2、R=2情况，我一般称之为 2 条分支（K=2），每个分支内部有 2 个 Featuremap Group（R=2）。\n（2）Split Attention\nInput 表示的每个特征图组经过一系列变换后的输出，即 $Input = F(Featuremap\\ Group)$。\nSplit Attention 是 ResNeSt 的核心，它将通道注意力机制应用到了每个 Split 分支之上。\nSplit Attention 架构 NOTES：和 Figure 2(Right) 有所不同，但是其实是等价的，Figure 2 更符合人的直觉，对人来说友好。Figure 4 能够方便的使用计算机实现，对计算机友好。\n这里我花了特别长的时间来了解这两种结构为什么是等价的。总结下来，直接阅读源代码是最好的方式。\nSplit Attention Block 的实现 https://github.com/zhanghang1989/ResNeSt\n假设输入的图片大小是 (3×64×64)。SplAtConv2d 表示的就是 Split Attention Block，输入的特征图大小为 (64×16×16)。通过 summary 可以知道，Split Attention 不改变特征图的 shape。\n以 K=2、R=2 为例，对于 (64×16×16) 的特征图，Split Attention Block 做了如下的事情：\n（1）\n源码实现中，不使用 1×1 的卷积。只有一个 3×3 的分组卷积。\n# In: 64×16×16, Out: 128×16×16 # kernel_size=3×3, same_conv, groups=2*2=4 (cadinality*radix) self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,  groups=groups*radix, bias=bias, **kwargs) （2）\n# x=(128×16×16) splited = torch.split(x, int(rchannel//self.radix), dim=1)  # if radix=2 # ==\u0026gt; splited[0] = (64×16×16) # ==\u0026gt; splited[1] = (64×16×16) （3）\n# In: splited[0], splited[1], Out: 64×16×16 gap = sum(splited) （4）全局平均池化\n# Global Pooling # In: 64×16×16, Out: 64×1×1 gap = F.adaptive_avg_pool2d(gap, 1) （5）\n# channels=64, inter_channels=32, groups=2(cardinality=2) # In: 64×1×1, Out: 32×1×1 self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality) NOTES：\n# in_channels=64, radix=2, reduction_factor=4 ==\u0026gt; inter_channels=32 inter_channels = max(in_channels*radix//reduction_factor, 32) （6）\n# inter_channels=32, channels*radix=64*2=128 # In: 32×1×1, Out: 128×1×1 self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality) （7）\n# rSoftmax # 得到 128 维的注意力向量 atten = self.rsoftmax(atten).view(batch, -1, 1, 1) （8）\n# rchannel=128 attens = torch.split(atten, int(rchannel//self.radix), dim=1) # attens[0] = 64 # attens[1] = 64 （9）\n# (64×16×16) 的特征图乘以其注意力权重 (64×1×1) out = sum([att*split for (att, split) in zip(attens, splited)]) out 即为 Split Attention Block 的输出，其 shape = (64×16×16)。\n实验结果 （1）SOTA Datasets\n（2）ImageNet \u0026amp; COCO\n","permalink":"http://landodo.github.io/posts/20210325-resnest-review/","summary":"arXiv: https://arxiv.org/abs/2004.08955 时间：2020 年 04 月 SENet（17.09）、SKNet（19.03） 和 ResNeSt（20.04） 可以放在一起进行阅读，这三篇都是注意","title":"ResNeSt"},{"content":"（SKNet）Selective Kernel Network 解析 Selective Kernel Network 是注意力机制的一篇经典文献。\n其实我现在的论文阅读量不多，连基础的基干系列网络都还没看完，但是为什么跑到注意力机制这里来了呢？\n是这样的，在我之前的汇报过的论文中，有 AlexNet、VGG、Xception，我进入注意力机制里，就是从 Xception 开始的。\nXception 基于“卷积神经网络的特征图中的跨通道相关性和空间相关性的映射平可以完全解耦” 这一假设，提出了一种完全基于深度可分离卷积层的卷积神经网络架构。\n下图 Figure 1~Figure 4 都是来自 Xception 的论文。\n Chollet, F. (2016). Xception: Deep Learning with Depthwise Separable Convolutions (cite arxiv:1610.02357)\n Figure 1 是经典的 Inception block；Figure 2 是其一个简化版本；Figure 3 是 1×1 卷积（通道维度 ）后，在输出通道的非重叠段上卷积（空间维度），注意这个【非重叠】，加粗了。非重叠，意味着通道的分段。\n观察 Figure 3 之后，自然而然地提出了一个问题：对于输出的通道，分段的段数有什么影响？\nFigure 4 这就是 Inception 的一个极端的结构（Extreme Inception），即：1×1 卷积之后，有多少个通道就分成多少段，，均分，3×3 的通道数都是 1，进行非重叠卷积。\n所以，我一直对段数到底应该怎么分有一个疑惑。比如，能不能不均分？就是说每个 3×3 卷积时的通道数都可能是不同的，那就有很多种组合。\n从这里出发，我就找到了 SENet。\n Hu, J., Shen, L., \u0026amp; Sun, G. (2018). Squeeze-and-Excitation Networks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7132-7141.\n 但是 SENet 依旧没有解决我的问题。\nSENet 是这样的，它仍然和 Xception 有类似思想，有多少通道数，就分成多少段（global average pooling），然后通过注意力机制，使得每个通道的重要性程度不一样。\n也就是说，通过 SE block 之后，特征图每个通道的重要性程度就不再相同了。重要的信息被强调，不重要的信息就被弱化，进而提升网络的性能。\n看完了 SENet，心中疑惑仍在：1×1卷积后得到的特征图，按照通道数应该分成多少段？哪些核应该分到多点的通道？哪些核应该分到少一点的通道？\nSKNet 是 SENet 的孪生兄弟（作者说的），SKNet 的起名也是致敬了 SENet。既然是兄弟，有始有终，就顺便把 SKNet 给看了。\n Li, X., Wang, W., Hu, X., \u0026amp; Yang, J. (2019). Selective Kernel Networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 510-519.\n 我与 SKNet 就这样相识了。\n下面，是我对 SKNet 的解析。\n背景 在开始介绍 SKNet 之前，先来了解一下这篇论文是在一个什么样的背景下出现的。\n在神经科学领域，视觉皮层的神经元的感受野大小是受刺激调节的。但是，这一点在 CNN 中很少被考虑到。如下图，神经元一般拥有着固定的感受野大小。\nhttps://towardsdatascience.com/advanced-topics-in-deep-convolutional-neural-networks-71ef1190522d\n在视觉皮层中，同一区域中神经元的感受野大小是不同的，这使得神经元可以在同一处理阶段收集多尺度的空间信息。什么是多尺度？下图就表示一个目标对象的不同尺度。\n也就是说，视觉皮层中的神经元可以捕获具有不同比例的目标对象，根据输入自适应地调整其感受野大小。\n虽然在此之前，自适应感受野大小的机制还没有人提出，或者说很少被考虑到。但是有一点是存在共识的，那就是结合不同感受野大小能够提升神经元的适应能力。\n“结合不同感受野大小”，让神经元可以在同一处理阶段收集多尺度的空间信息的机制，已经被广泛采用了。\nInceptionNet 就是一个典型例子。它通过设计一个简单的级联（concatenation）来聚合来自内部 1×1、3×3、5×5、7×7 卷积核的多尺度信息。\n Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.E., Anguelov, D., Erhan, D., Vanhoucke, V., \u0026amp; Rabinovich, A. (2015). Going deeper with convolutions. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1-9.\n 但是 Inception 存在一个缺点：它虽然考虑到了神经元需要不同尺度的信息，但是它不同分支的多尺度信息的聚合方法比较简单，机制固定，可能还不足与为神经元提供强大的适应能力。\n而且，Inception module 结构非常复杂，人工的痕迹太重了。\n“复杂的东西往往抓不到背后的本质！”\nInception(GoogLeNet) 的提出时间为 2014 年 9 月（arXiv），正是发表是在 2015 年。\nVGG 在 arXiv 上的时间也是 2014 年 9 月份。\n Simonyan, K., \u0026amp; Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR, abs/1409.1556.\n 那个时候，还没有 “He Initialization”，也没有 “Batch Normalization”，ResNet 还没有提出。神经网络面临一个普遍的问题，随着网络的加深，越来越难以训练，而且存在网络退化的问题。\nInceptionNet 就是让网络变得“胖”一些。\n第二年 ResNet 横空出世，一下子就把网络的深度加深到了上千层。\n He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778.\n ResNet 发布后，Inception 从 ResNet 获得灵感，第二年 Inception-ResNet 发布。\n后来，单路卷积变成了多个支路的多路卷积，分组数很多，结构一致，进行分组卷积，ResNeXt 诞生。\n Xie, S., Girshick, R.B., Dollár, P., Tu, Z., \u0026amp; He, K. (2017). Aggregated Residual Transformations for Deep Neural Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 5987-5995.\n 上图中，(a) 是 ResNeXt 基本单元，如果把输出那里的 1×1 合并到一起，得到等价网络 (b) 拥有和 Inception-ResNet 相似的结构，而进一步把输入的 1×1 也合并到一起，得到等价网络 (c) 则和通道分组卷积的网络有相似的结构。\n这幅图相当于在说，Inception-ResNet 和通道分组卷积网络，都只是 ResNeXt 这一范式的特殊形式而已。\n“ResNeXt 抓住了背后的本质。”\n做了一个小总结：\n InceptionNet 的特点是多分支的 multiple Kernel 设计 ResNeXt 的特点是用分组卷积轻量化了 \u0026gt;1× 1 的 kernel  在此背景之下，SKNet 设计的出发点就是能否结合两者的特色。\nSKNet 设计的出发点 SKNet 设计的一大出发点就是结合 Inception\u0026amp;ResNeXt 的优点，改进其缺点。\n首先，先把 InceptionNet concatenation 汇聚的过程灵活化。目前流行的注意力机制就可以做到这一点。\n原始的 ResNeXt block 对于不同尺度的目标对象输入，无法获得不同尺度的信息。因此，在 ResNeXt 的基础之上，引入类似 Inception block 的多分支机构。就像下图这样，有 4 个分支（后续实验表明，分支数为 2 就好了，分支数太多参数量、计算量太大）。\n引入，我得出的总结为：在 ResNeXt block 的基础之上，加上多分支和 Attention 机制，就得到了 SK block。\n解析 Selective Kernel Unit（SK block） SK 模块所做的工作是：输入的特征图为 $X \\in \\mathbb{R}^{H \\times W \\times C}$，经过 SK Convolution 后，得到输出的特征图为 $V \\in \\mathbb{R}^{H \\times W \\times C}$。SK 卷积有 3 个步骤：Split, Fuse and Select.\n1 SK Convolution: Split(1/3) 对于输入的特征图 $X \\in \\mathbb{R}^{H \\times W \\times C}$，默认情况下进行两次转换：\n（1）$\\tilde{F}: X \\rightarrow \\tilde{U} \\in \\mathbb{R}^{H \\times W \\times C}$\n卷积核的大小为 $3 \\times 3$。\n（2）$\\hat{F}: X \\rightarrow \\hat{U} \\in \\mathbb{R}^{H \\times W \\times C}$\n卷积核的大小为 $5\\times 5$。为了进一步提高效率，$5 \\times 5$ 的卷积使用空洞大小为 2 的 $3 \\times 3$ 的卷积来替代。\n2 SK Convolution: Fuse(2/3) Fuse 操作和 Squeeze and Excitation block 一样。\n（1）integrate information from all branches.\n将分支进行 element-wise 的求和，得到 $U \\in \\mathbb{R}^{H \\times W \\times C}$。\n（2）global average pooling.\n得到 $s \\in \\mathbb{R}^{C \\times 1}$，s 是一个有 C 个元素的列向量。\n（3）compact feature by simple fully connected (fc) layer.\n使用全连接层，即 $s \\in \\mathbb{R}^{C \\times 1} \\rightarrow z \\in \\mathbb{R}^{d \\times 1}$，其中 $d \u0026lt; C$。\n $\\delta$ 使用 ReLU 函数， $B$ 表示 Batch Normalization， $W \\in \\mathbb{R}^{d \\times C}$，权重矩阵。 z 被称为 compact feature descriptor.   reduction ratio $r$ 是一个超参数， 一般设置 L = 32。  3 SK Convolution: Select(3/3) 这一步是 SK Block 的核心操作。\n A soft attention across channels is used to adaptively select different spatial scales of information, which is guided by the compact feature descriptor z.\n （1）Soft attention across channels.\n $A, B \\in \\mathbb{R}^{C \\times d}$，这两个矩阵也是端到端训练出来的。如果只有两个分支，那么矩阵 B 是冗余的，因为 Softmax 的输出 $a_c + b_c = 1$，可以通过 1 减去另一个得到。 $z \\in \\mathbb{R}^{d \\times 1}$，经过 softmax 操作后，得到的 $a \\in \\mathbb{R}^{C \\times 1}$.  z 被称为 compact feature descriptor. a 被称为 soft attention vector.    （2）The final feature map $V$ is obtained through the attention weights on various kernels.\n $\\tilde{U}, \\hat{U} \\in \\mathbb{R}^{H \\times W \\times C}$ $a, b \\in \\mathbb{R}^{C \\times 1}$ 执行的操作是 element-wise product. $a_c \\cdot \\tilde{U}_c$ 表示第 c 个通道的特征图上的每个点，都乘以数 $a_c$。  综上，就是 SK Convolution 的内部原理细节。\nSKNet 结构 SKNet 建立在 ResNeXt 的基础之上，把 ResNeXt 中 3×3 的核全部换成 SK Unit，这就得到了 SKNet。\n有两个多出来的超参数。\nM=2 表示分支数为 2。\nr 这个参数是用来调节 Fuse 中全连接层的参数个数的。在 Fuse 操作中，全局平均池化之后，得到向量 s，然后通过一个全连接层后得到向量 z，z 的通道数是小于 s 的，超参数 r 就是用来控制这个的。\n实验结果 Table 2~5 都是描述的 SKNet 与其他模型的比较结果。对 SKNet 只要有一个印象就行，与其他模型相比：\n 同等参数量下，SKNet 的 top-1 错误率最低！ 同等 top-1 错误率下，SKNet 的参数量最少！  重点放在 Ablation Studies 上。\n（1）Table 6\n表中 D 是空洞卷积的空洞大小，能在在不增加计算量的情况下，扩大感受野。\nSKNet 中，Split 操作有一分支是 3×3，另一分支是 5×5。对于 5×5 的支路，使用空洞大小为 2 的 3×3 卷积来替代。Table 6 说明了这样做的优势。\n各种 3×3 kernels with various dilations 明显优于与它具有相同感受野的 kernel。3×3, D=3 优于 7×7, D=1。\n（这只在 SKNet 分支中使用才成立，在其他地方使用就不好说了，Table 7 就是例子）\n（2）Table 7.\n前面的三栏是空洞卷积的知识，在不增加计算量的情况下增加感受野。表中可以得出结论，单纯的使用这种卷积虽然增大了感受野，但是 top-1 错误率却提升了。第一栏就是 ResNeXt 的精度。因此，空洞卷积它不能乱用！在 SK 块外使用就不行！\n关于空洞卷积的使用场景，这里我留一个 Flag，后面再看。\n 对于两条分支的 SK 卷积聚合始终比简单的聚合方法的 top-1 错误率更低。（SK:20.79%, 普通的 concat: 21.76%） (K3, K7)(K5, K7) 也是一样的，SK卷积聚合性能更优。稍微不可避免的增加了一点点的计算量。 SK Unit 随着分支数的增加，M=2 增到 M=3，top-1 错误率下降了。这也是以增加一定的参数量和计算量为代价的。 M=2，M=3 的 top-1 错误率差不多（20.79%，20.76%）。为了更好的平衡模型的性能和复杂度，M = 2 是更好的选择（两路分支）。  （3）Figure 3.\nFigure 3 特别有意思，我认为这是这篇文献的一个重点。\n一般来说，对于这种花里胡哨的图，脑海中不多的经验告诉我，这都是用来研究一个问题的，即神经网络的可解释性研究！\n下面我将用两个问题来总结这些图片想要说明的事情。（1）这些图做了什么事情？或者说为什么要画这些图？（2）这些图可以得出什么结论？\n首先来看问题（1）为什么要画这些图？\n对于我来说，看完前面的实验结果（Table 2~Table5），我就知道了一件事：SKNet 非常厉害！它为什么厉害，我不知道。\n再回忆一下这篇论文的背景，它的提出是用来解决一个什么问题的？\nSKNet 解决的是：对于不同尺度的目标对象输入，SKNet 能够实现自适应感受野大小，进而提升性能。\n所以，这些花里胡哨的图是用来说明，对于不同尺度的目标对象输入，网络中的不同感受野的卷积核的权重是动态变化的，即自适应感受野大小。SKNet 所带来的性能提升，是自适应感受野大小的结果。\n好的，第一个问题解决。\n来看第二个问题，（2）这些图可以得出什么结论？\n(a)(b) 两图的折线图，取了 SK_3_4 阶段的 soft attention vector b 进行可视化。b 向量是分支 5×5 分支的注意力权重。\n 至于为什么选择 SK_3_4，这是因为通过实验发现，SK_3_4 最能体现，当目标对象增大（1.0×、1.5×、2.0×），大核（5×5）的注意力权重增大。\n 以两个随机的样本为例，对于大多数通道，当目标对象增大（1.0×、1.5×、2.0×），大核（5×5）的注意力权重增加，图上的体现就是，红线大部分位于蓝绿两线之上。\n接下来看条形图。\n分两段看：2_3、3_4、4_6 是一个段，这是网络的前层和中间一点的层；5_3 是另一个段，表示为网络的很深的层。前一段呈现了这样一个现象：目标对象越大，则在网络浅层和中层阶段（例如，SK2_3，SK3_4, SK 4_6），“Selective Kernel” 机制将增加大核（5×5）的注意力权重，体现在条形图上就是：2.0× 的粉色 \u0026gt; 1.5× 绿色 \u0026gt; 1.0× 蓝色。\n后一段 SK 5_3 深层就没有这个规律了。(b) 图的 5_3，目标对象从 1.0× 放大到 1.5×，核 5×5 的注意力权重反而减少了，而从 1.5× 放大到 2.0×，5×5 核的注意力权重却又增加了。\n(c) 图表明，对于所有的验证集数据，都呈现出一样的现象。\n最后来回答第二个问题（2）这些图可以得出什么结论？\n得出的结论：目标对象从 1.0x 增加到 1.5x、2.0x，5x5 大核的重要性增加了！\n如果要加上一个定语的话，那就是，只限于在网络的浅层和深层，在深层没有这种现象。\n（4）Figure 4.\nFigure 图可以得到以下结论：\n 在 SK_2_3 和 SK_3_4 阶段（网络的浅层和中间一点的层），随着目标对象的放大（1.0× 增大到 1.5×），5×5 核的重要程度随着增加。  （浅层和中层橙色线始终位于蓝色线之上）\n 在网络的深层，目标对象的放大（1.0× 增大到 1.5×），橙色线和蓝色线是上下交错的，这表明 5×5 核的重要程度和不同尺度的目标对象相关性不强。  深层没有浅层的那种规律现象。作者也对这个现象进行了解释：深层的特征中的多尺度信息已经被分解掉了，所以对于 1.0x、1.5x 的目标对象，SKNet 没有明确的倾向去选择更大或更小的 kernel。\n总结 最后对 SKNet 进行一个总结：\n SKNet 从多种卷积核中聚集信息，以实现神经元的自适应感受野大小。 SKNet 是第一个通过引入注意力机制来明确关注神经元的自适应感受野大小。 设计思路：ResNeXt + Inception + SENet ≈≈ SKNet  实验发现，\n（1）SKConvolution 的大于 3×3 分支中，各种 3×3 kernels with various dilations 明显优于与它具有相同感受野的 kernel。例如：3×3, D=3 优于 7×7, D=1。\n（2）随着目标对象尺度的增大（1.0×、1.5×、2.0 × ），在前网络的浅/中层，感受野大的核的重要性增大。\n（3）在网络的深层，特征中的多尺度信息会被分解，所以对于 1.0×、1.5×、2.0× 等不同尺度的目标对象，SKNet 没有明确的倾向去选择更大或更小的 kernel。\n以上，就是昨天晚上的汇报，我想要表达的所有内容。\n","permalink":"http://landodo.github.io/posts/20210315-sknet-review/","summary":"（SKNet）Selective Kernel Network 解析 Selective Kernel Network 是注意力机制的一篇经典文献。 其实我现在的论文阅读量不多，连基础的基干系列网络都还没看完，但是为","title":"Selective Kernel Network 解析"},{"content":"Wide Residual Networks arXiv: 1605.07146v4\n法国\nAbstract 深度残差网络被证明能够扩展到数千层，不断的提高性能。\n但是，精度提高的每一小部分都要增加近一倍的层数，而训练很深的残差网络有一个特征重用递减（diminishing feature reuse）的问题，这使得这些网络的训练速度非常慢。\n为了解决这些问题，本文对 ResNet block 的架构进行了详细的实验研究，在此基础上，提出了一种新型的架构，即减少深度，增加残差网络的宽度（ decrease depth and increase width of residual networks. ）。\n本篇论文提出的网络被称为 wide residual networks (WRNs)，实验证明，一个简单的 16 层深的 WRN，精度和效率上优于以前所有的深残差网络，包括千层深网络。\nWRN 在 CIFAR、SVHN、COCO 数据集上取得 SOTA。\n1 Introduction 卷积神经网络在过去几年中，层数逐渐增加：AlexNet，VGG，Inception，ResNet。\n训练深度神经网络存在几个困难，包括梯度爆炸/消失和网络退化（exploding/vanishing gradients and degradation.）。训练深层的神经网络有如下技巧：\n well-designed initialization strategies better optimizers skip connections knowledge transfer layer-wise training  最近的 ResNet 在 ImageNet、CIFAR、PASCAL VOC、MS COCO 上取得了 SOTA。与 Inception 架构相比，ResNet 表现出了更好的泛化能力，这意味着特征可以在迁移学习中被利用，效率更高。\n研究表明，残差连接加快了网络的收敛。\nHighway Network 在 ResNet 之前被提出，Highway Network 中的残差链路是门控的，这些门的权重是可学习的。\n到目前为止，对残差网络的研究主要集中在 ResNet 块内的激活顺序和残差网络的深度上。本篇论文的的工作是探索一套更丰富的 ResNet 块的网络架构，并彻底研究除了激活顺序之外的其他几个不同方面如何影响性能。\nWidth vs depth in residual networks.（残差网络的宽度与深度）\n残差网络的作者试图将网络尽可能地做得更薄，而倾向于增加其深度和更少的参数，甚至引入了一个”瓶颈“块（«bottleneck» block），使得 ResNet 块更薄。\n作者们注意到，带有恒等映射（identity mapping）的残差块可以训练非常深的网络，这这是残差网络的优点，同时也是一个弱点。由于梯度流经网络时，没有什么东西可以强迫它通过残差块权重，它可以避免在训练过程中学习任何东西，所以有可能只有少数几个块可以学习到有用的表征，或者很多块共享的信息非常少，对最终目标的贡献很小。这个问题被称为递减特征重用（diminishing feature reuse），（Deep networks with stochastic depth）试图用在训练过程中随机禁用残差块的想法来解决这个问题。\n这篇论文建立在 “Identity mappings in deep residual networks” 之上，试图回答深度残差网络应该有多宽的问题，并解决训练中存在的问题。\n实验表明，与增加残差网络的深度相比，拓宽 ResNet 块提供了一种更有效的方法来提高残差网络的性能。wider deep residual networks 比 ResNet 有显著的性能提升，层数减少了 50 倍，速度快了 2 倍以上。\nwide 16-layer deep network 与 1000-layer thin deep network 的精度相同，参数数量也相当，不过前者训练速度要快几倍。这暗示了深度残差网络的主要力量在残差块，深度的影响是辅助性的。\n**Use of dropout in ResNet blocks. **\nDropout 较多应用于参数较多的顶层，以防止特征共适应和过拟合。\n后来，dropout 被 Batch normalization 代替，BN 的网络比有 dropout 的网络能达到更好的精度。（Batch normalization: Accelerating deep network training by reducing internal covariate shift.）\nkaiming He 研究表明，在 ResNet 中引入 dropout 会产生负面的作用。在宽残差网络（WRN）上的实验结果表明，引入 dropout 后，在一些数据集上取得了 SOTA。\n本篇论文的贡献总结如下：\n 对残差网络架构进行了详细的实验研究，彻底研究了 ResNet 块结构的几个重要方面。 提出了一种 widened architecture for ResNet blocks，使残余网络的性能得到显著提高。 提出了一种在深度残差网络中利用 dropout 的新方法。 WRN 在几个数据集上取得 SOTA。  2 Wide residual networks 恒等映射的残差块表示如下：\n 其中 $x_{l+1}$ 和 $x_l$ 为网络中第 $l$ 个单元的输入和输出，$F$ 为残差函数，$W_l$ 为块的参数。残差网络由依次叠加的残差块组成。  残差网络由两种类型的块组成：\n（1）basic：用两个连续的 3×3 卷积与批量归一化、ReLU 前面的卷积：conv3×3-conv3×3 图 1(a)\n（2）bottleneck：有一个 3×3 卷积，前后有降维和扩展的 1×1 卷积层：conv1×1-conv3×3-conv1×1 图1(b)\n与原架构（Deep residual learning for image recognition）相比，在（Identity mappings in deep residual networks）中，将 batch normalization、activation、convolution 的顺序由 conv-BN-ReLU 改为 BN-ReLU- conv。后者被证明训练速度更快，取得了更好的效果。\n所谓的 ”bottleneck block“ 最初是用来使 block 的计算成本降低，以增加层数。由于本篇论文要研究加宽的效果，而 ”bottleneck block“ 是用来让网络变薄的，所以不考虑它，而是关注 ”basic block“ 的残差架构。\n有三种简单的方法可以提高残差块的表示力。\n to add more convolutional layers per block ✅ to widen the convolutional layers by adding more feature planes to increase filter sizes in convolutional layers  引入两个参数：\n deepening factor l：l 表示一个区块中的卷积数。 widening factor k：k 倍的特征数量（特征图的片数，即卷积核的个数）  因此，ResNet baseline 中的 ”basic block“ 对应的是 l＝2，k＝1。图 1(a) 和图 1(c) 分别显示了 «basic» 和 «basic-wide» blocks。\nWRN 结构如 Table 1 所示：\n它由一个初始卷积层 conv1 组成，之后是 3 组（每组大小为 N）残差块 conv2、conv3 和 conv4，然后是平均池和最终分类层。\n在所有的实验中，conv1 的大小都是固定的，而引入的加宽因子 k 则对 3 组 conv2-4 中的残差块的宽度进行了缩放（例如，原来的 ”basic“ 架构相当于 k=1）。\n2.1 Type of convolutions in residual block 让 B(M) 表示残差块结构，其中 M 是一个列表，其中有块中卷积层的核大小。\n例如，B(3, 1) 表示具有 3×3 和 1×1 卷积层的残差块。请注意，由于我们不考虑前面解释的 \u0026ldquo;瓶颈 \u0026ldquo;块，所以在整个块中，特征平面的数量始终保持不变（所有 block 中，使用卷积核的数量相同）。\n论文想要研究的是，”basic block“ 残差架构的 3×3 卷积层中的每一个层的重要程度，是否可以用计算成本较低的 1×1 层，甚至是 1×1 和 3×3 卷积层的组合来代替。例如，B(1,3) 或 B(1,3)。这可以增加或减少块的表示能力。\n因此，试验了以下组合（注意，最后一个组合，即 B(3,1,1) 与Network in Network 架构相似）。\n2.2 Number of convolutional layers per residual block 还对区块加深因子 l 进行实验，看看它对性能的影响。比较必须在参数数量相同的网络中进行，所以在这种情况下，需要在确保网络复杂度保持大致不变的情况下，构建不同 l 和 d（其中 d 表示块的总数量）的网络。例如，这意味着只要 l 增加，d 就应该减少。\n2.3 Width of residual blocks  While the number of parameters increases linearly with l (the deepening factor) and d (the number of ResNet blocks), number of parameters and computational complexity are quadratic in k.\n 参数量随着 l（加深因子）和 d（ResNet 块数）的增加而线性增加，但参数量和计算复杂度是 k 的平方。\n关于更宽的残差网络的一个论点是，在残差网络之前，几乎所有的架构，包括最成功的 Inception 和 VGG，与 ResNet 相比都要宽很多。例如，残差网络 WRN-22-8 和 WRN-16-10 在宽度、深度和参数数量上与 VGG 架构非常相似。\n WRN-n-k：n 为总的卷积层数；k 为widening factor。\nfor example, network with 40 layers and k = 2 times wider than original would be denoted as WRN-40-2.\n 2.4 Dropout in residual blocks BN 虽然有正则化的效果，但是其需要大量的数据增强。\n图 1(d) 所示，在每个残差块之间的卷积和 ReLU 之后增加一个 dropout 层，以扰动下一个残差块中的批归一化，防止其过拟合。在很深的残差网络中，应该有助于处理在不同残差块中强制学习的递减特征重用问题。\n3 Experimental results （1）Type of convolutions in a block\nB(3,3) 是最好的，B(3,1) 和 B(3,1,3) 在精度上非常接近 B(3,3)，因为参数少，层数少。B(3,1,3) 比其他的快一点。\n（2）Number of convolutions per block\nB(3,3 )结果最好，而 B(3,3,3) 和 B(3,3,3,3) 的性能最差。\nB(3,3) 在每个块的卷积数方面是最优的，因此，在接下来的实验中，只考虑 B(3,3) 类型的块的 WRN。\n（3）Width of residual blocks\n当试图增加加宽参数 k 时，就必须减少总层数。为了找到一个最佳比例，在 k 从 2~12，深度从 16~40 的情况下进行了试验。结果如 Table 4 所示。\nK = 8 时，depth 分别是 16、22、40，错误率 4.56%、4.38%、4.66%，有一个先下降，再上升的过程。\ncompare thin and wide residual networks.\nWRN-28-10 在 CIFAR-10 上的表现比 ResNet-1001 要好 0.92%，在 CIFAR-100 上的表现比 ResNet-1001 要好 3.46%，层数少了 36 倍。\nWRN-28-10 和 WRN-40-10 的参数分别是 ResNet-1001 的 3.6 倍和 5 倍，分类错误率明显低于 ResNet-1001。\n（4）Dropout in residual blocks\n略，实验结果直接看论文效果更好。\n笔记只记录背景和原理就好。\n","permalink":"http://landodo.github.io/posts/20210313-wrn/","summary":"Wide Residual Networks arXiv: 1605.07146v4 法国 Abstract 深度残差网络被证明能够扩展到数千层，不断的提高性能。 但是，精度提高的每一小部分都要增加近一倍的层数，而训练很深的残差网络有一","title":"Wide Residual Networks"},{"content":"Dual Attention Network for Scene Segmentation arXiv: 1809.02983\n“用于场景分割的双注意力网络“\nAbstract 针对场景分割任务，基于 self-Attention 机制捕捉丰富的上下文依赖关系。\n与以往通过多尺度特征融合来捕捉上下文的工作不同，本篇论文提出了一种双注意力网络（Dual Attention Network, DANet）来自适应地整合局部特征与其全局依赖性。\ntwo types of attention modules：分别在空间维度和通道维度上建立语义相互依赖的模型。\n（1）position attention module\n 对所有位置的特征进行加权和，选择性地聚合每个位置的特征。  （2）channel attention module\n 通道关注模块有选择地强调相互依赖的通道图，通过整合所有通道图之间的相关特征。  将两个注意力模块的输出相加，以进一步实现改善特征表示，这有助于更精确的分割结果。\n在 Cityscapes、PASCAL Context 和 COCO Stuff 数据集上取得了 SOTA。\n1. Introduction 本篇论文提出了一种新颖的框架工作，称为作为双注意力网络（Dual Attention Network, DANet），用于自然场景图像分割，如 Figure 2。\n它引入了一种自注意力机制，分别捕捉空间和通道维度的特征依赖性。具体来说，在扩张的 FCN 之上附加了两个平行的注意力模块。一个是位置注意力模块（position attention module），另一个是通道注意力模块（channel attention module）。\n对于位置注意力模块，引入自关注机制来捕捉特征图中任意两个位置之间的空间依赖性。\n对于通道注意力模块，使用类似的自注意力机制来捕捉任意两个通道图之间的通道依赖性。\n最后，将这两个注意力模块的输出进行融合，以进一步增强特征表示。\n 图 1：场景分割的目标是识别每个像素，包括东西、不同的物体。物体/东西的各种尺度、遮挡和光照变化，使得解析每个像素具有挑战性。\n 2. Related Work “Attention is all your need” 率先提出了绘制输入的全局依赖性的自注意机制，并将其应用于机器翻译中。\n3. Dual Attention Network 在本节中，首先介绍网络的总体框架，然后介绍了两个注意力模块，它们分别在空间和通道维度上捕捉长程上下文信息。最后介绍如何将它们聚合在一起进行进一步的完善。\n3.1. Overview 去掉了向下采样操作，并在最后两个 ResNet 块中采用了空洞卷积，从而将最后的特征图的大小放大到输入图像的 1/8。它保留了更多的细节，而没有增加额外的参数。然后，残差网络的特征将被输入到两个平行的注意力模块中。\nFigure 2 在上部分是 spatial attention modules。首先应用卷积层来获得降维的特征。然后将这些特征反馈到位置注意力模块中，通过以下三个步骤生成新的空间长程上下文信息特征（spatial long-range contextual information）。\n（1）第一步是生成一个空间注意力矩阵，该矩阵对特征的任意两个像素之间的空间关系进行建模。\n（2）在注意力矩阵和原始特征之间进行矩阵乘法。\n（3）对上述乘法结果矩阵和原始特征进行元素求和操作，以获得反映远距离上下文的最终表示。\n同时，通道维度的远距离上下文信息由通道注意力模块捕获。捕捉通道关系的过程与位置关注模块类似，只是第一步，在通道维度上计算通道注意力矩阵。\n最后，将两个注意力模块的输出汇总，以获得更好的特征表示，用于像素级预测。\n3.2. Position Attention Module 给定一个局部特征图 $A \\in \\mathbb{R}^{C×H× W}$，首先将其送入卷积层，分别生成两个新的特征图 B 和 C，$(B, C) \\in \\mathbb{R}^{C \\times H \\times W}$。\n然后将 B、C reshape 为 $(B, C) \\in \\mathbb{R}^{C \\times N}$，其中 $N = H \\times W$，表示一个通道的特征图的像素总数。\n之后，矩阵乘法 $C^T \\cdot B$，并应用 softmax 层计算空间注意力图 $S \\in \\mathbb{R}^{N×N}$。\n $s_{ji}$ 衡量的是衡量第 i 个位置对第 j 个位置的影响。两种位置的特征表征越相似，有助于提高它们之间的相关性。  同时，需要将 A 经过一个卷积层得到特征图 D，$D \\in \\mathbb{R}^{C \\times H \\times W}$，然后将 D reshape 成 $D \\in \\mathbb{R}^{C \\times N}$。然后再 D 和 S 进行矩阵乘法后，将得到的结果 reshape 为 $\\mathbb{R}^{C \\times H \\times W}$。乘以比例参数（scale parameter）$\\alpha$ 后与特征图 A 进行 element-wise sum operation 得到最终的结果 $E \\in \\mathbb{R}^{C \\times H \\times W}$。\n由公式 (2) 可以推断，每个位置的结果特征 E 是所有位置的特征和原始特征的加权和。因此，它具有全局的上下文视图，并根据空间注意力图有选择地聚合上下文。相似的语义特征实现了相互增益，从而导入了类内紧凑和语义一致性。\n3.3. Channel Attention Module 通过利用通道图之间的相互依赖性，可以强调相互依赖的特征图，改善特定语义的特征表示。\n因此，构建一个通道注意力模块来明确建模通道之间的相互依赖关系。\n与位置注意力模块不同，这里直接通过原始特征图 $A \\in \\mathbb{R}^{C \\times H \\times W}$ 计算通道注意力图（channel attention map）$X \\in \\mathbb{R}^{C \\times C}$。\n细节上，先将 $A \\in \\mathbb{R}^{C \\times H \\times W}$ reshape 成 $A \\in \\mathbb{R}^{C \\times N}$，然后 A 与 A 的转置进行矩阵乘法，$A \\cdot A^T$。最后，应用 softmax 层得到通道注意力图 $X \\in \\mathbb{R}^{C \\times C}$：\n 其中，其中 $x_{ji}$衡量第 i 个通道对第 j 个通道的影响。  $X^T \\cdot A$ 并将结果 reshape 为 $\\mathbb{R}^{C \\times H \\times W}$，最后的操作和 Position Attention Module 类似，scale parameter 为 $\\beta$。\n从公式 (4) 可以看出，每个通道的最终特征是所有通道的特征和原始特征的加权和，它模拟了特征图之间的长程语义依赖关系。它有助于提升特征的可分辨性。\n利用所有相关位置的空间信息来模拟通道相关性。\n we exploit spatial information at all corresponding positions to model channel correlations.\n 3.4. Attention Module Embedding with Networks 本篇论文提出的注意力模块很简单，可以直接插入到现有的 FCN pipeline 中。它们不会增加太多参数，却能有效地加强特征表示。\n4. Experiments 4.2.1 Ablation Study for Attention Modules: Table 1, Figure 4, Figure 5.\n4.2.2 Study for Improvement Strategies: Table 2.\n4.2.3 Visualization of Attention Module: Figure 6.\n4.2.4 Comparing with State-of-the-art: Table 3.\n4.3. Results on PASCAL VOC 2012 Dataset: Table4, Table5.\n4.4. Results on PASCAL Context Dataset: Table 6\n4.5. Results on COCO Stuff Dataset: Table 7.\n5. Conclusion  presented a Dual Attention Network (DANet) for scene segmentation, which adaptively integrates local semantic features using the self-attention mechanism. position attention module and a channel attention module to capture global dependencies in the spatial and channel dimensions respectively. dual attention modules capture long-range contextual information effectively and give more precise segmentation results. Dual Attention Network (DANet) achieves outstanding performance consistently on four scene segmentation datasets, Cityscapes, Pascal VOC 2012, Pascal Context, and COCO Stuff.  ","permalink":"http://landodo.github.io/posts/20210311-dual-attention-network/","summary":"Dual Attention Network for Scene Segmentation arXiv: 1809.02983 “用于场景分割的双注意力网络“ Abstract 针对场景分割任务，基于 self-Attention 机制捕捉丰富的上下文依赖关系。 与以往通过多尺度特征融合来捕捉上下文的","title":"Dual Attention Network for Scene Segmentation"},{"content":"Attentive Inception Module based Convolutional Neural Network for Image Enhancement “基于 Attentive Inception Module 的卷积神经网络在图像增强中的应用”\n high quality：高质量 low distortion：低失真 singleimage superresolution (SISR)：单一图像超分辨率 JPEG compression artifact removal：消除 JPEG 压缩伪影 image distortions：图像失真 denoising CNN (DnCNN) which developed a successful single model for image denoising, superresolution and compression artifact removal.  摘要 本篇论文通过提出一种卷积神经网络，其内含注意机制的 Inception 模块，解决了以单幅图像超分辨率和压缩伪影形式进行图像增强的问题。\n方法是通过注意力机制对 Inception 多分支结构聚合的多尺度特征进行过滤，使学习的特征图加权，以减少冗余。\n In this paper, the problem of image enhancement in the form of single image superresolution and compression artifact reduction is addressed by proposing a convolutional neural network with an inception module containing an attention mechanism.\n I. INTRODUCTION 图像的增强方法可以从低级的图像处理算法到高级的机器学习和深度学习方法。\n这篇论文的主要工作：解决了单一图像超分辨率（SISR）和 JPEG 压缩伪影去除的问题。In image superresolution，一个高分辨率（HR）图像是由单个低分辨率（LR）图像或多个低分辨率图像构建而成。\n近年来，深度学习模型在图像增强领域取得了令人瞩目的成果。卷积神经网络（CNN）主要用于此类增强任务，当在自然图像的数据集上训练时，它试图通过迭代更新其参数来学习低质量和相应的高质量图像之间的映射。\n这种数据驱动的监督方法可以使模型近似于高质量图像的自然性，而基于回归的损失函数和正则化方法可以对训练过程施加特定的前因和约束。\n  One of the first networks for SISR (SRCNN)\n  The efficient sub-pixel convolutional network (ESPCN)\n  ARCNN: introduced for JPEG compression artifact removal\n  \u0026hellip;\n   ✅ In this work, we propose a convolutional neural network with an inception module having dilated filters for multi-resolution feature extraction, along with short and long skip attention modules.\n II. MODEL OVERVIEW CNN 模型是一个端到端的结构，包含一系列具有注意力的 Inception 块。为了创建训练数据集，将 DIV2K 数据集中的一组 RGB 图像转换为 YCbCr 图像，并提取 Y 通道，即亮度分量。\n （1）The low-quality input images for the compression artifact reduction task are generated by compressing them with different factors by the JPEG compression algorithm.\n通过 JPEG 压缩算法对低质量的输入图像进行不同因子的压缩.\n  （2）For the superresolution task, the low-quality images are generated by resampling the original images or patches of images by various resampling factors. Overlapping patches are cropped from the low and high quality images.\n通过对原始图像或图像的补丁按不同的重采样因子重采样生成的\n 从低质量图像（上面两种方式生成）和高质量图像（原图）中裁剪出重叠的 patch。低质量的 patch 被用作模型的输入，而高质量的 patch 被用作 ground truth。\n Overlapping patches are cropped from the low and high quality images. The low-quality patches are used as inputs to the model while the original patches are used as the ground truth examples. The following section describes the proposed network architecture and the training of the model.\n A. Network architecture 网络架构图如 Fig.1 所示。\n 有多个 Inception 模块堆叠而成 每个 Inception 包含 4 条平行的卷积分支卷积块， 卷积层中使用的卷积核具有相同数量的可学习系数，但具有不同的扩张因子（different dilation factors），因此增加了卷积核的感受野。  在低质量的图像中，某些区域或邻域具有非常相似的信息，对于给定的网络深度，一个小的卷积核无法从这些邻域中提取和重建足够好的特征。因此，空洞卷积核有助于聚合多分辨率信息，同时忽略冗余像素，有利于图像的最终重建。\n卷积层使用 3 × 3 × D 的核（D=64），四个分支的 dilation factors 分别为 0、1、2、4，其卷积核的数量或输出特征图的深度分别为 D、D/2、D/4 和 D/4。每一个卷积层之后都有一个批量归一化层（BN）。\n在批归一化层之后，在每个分支中还有一个额外的卷积和批归一化层，这个卷积是普通的卷积。\n分支的特征图 concatenated 后，得到深度为 2×D=128 的特征图。然后经过一个整流线性单元（ReLU）。\n在 Inception 模块的内部，引入了一个通道维度的注意力机制（channel-wise attention）。\n经过 Concat+ReLU 后，得到特征图 X，对其进行全局平均池化（global average pooling），得到一个长度为 2×D=128 的向量。\n 公式中的 i, j 表示特征图空间维度的索引（spatial indices） k 代表特征图的通道  然后将全局平均池化后得到的向量，输入到包含一个隐藏层神经元个数为 4×D 的神经网络中，最后得到一个长度为 D 的最终向量 y。然后在 y 向量上应用一个 sigmoid 函数得到向量 s。\n同时，在下面的分支，卷积层将通道数为 2D 的特征图 X 转换为通道数为 D 的特征图 Y，并与给定的注意力向量 s 元素进行通道的乘积（channel-wise product），得到最后的特征图 $Y^{att}$。\n另外，网络还包含一个长跳接的注意力。channel-wise 软注意力独立地权衡每个特征通道，并为下面各层学习每个特征图的重要性分配。最早的注意力向量与具体的输入图像相关性更强，因此引入长跳接的注意力连接意在增强这种基于 CNN 输入的动态性质。\nB. Loss Function and Hyperparameters C. Experimental Observations Fig.2 显示了不同的网络架构。\n Arch1 是一个简单的前馈网络，采用瓶颈结构，特征图在特征通道数上进行了扩展和压缩； Arch2 是 Inception 前馈架构； Arch3 是本篇论文提出的架构，引入了 Attention 模块  所有的架构都是残差性的，具有相同的层数和相似的特征量，从原始图像中以 192 的步幅，裁剪出 64px × 64px 的 patch 输入到网络中进行训练。\nTABLE I 显示了组合数据集上的结果，峰值信噪比（PSNR）和结构相似性指数指标（SSIM）以 dB 为单位。\n可以看出，inception blocks with dilation and attention mechanism 提升了性能。\n在不同的网络深度下，通过改变批量归一化 Inception 模块数量，对提出的网络（Arch3）进行实验。\n本篇论文的工作中的最终模型有 6 个 Inception 模块，但结果清楚地表明，随着网络深度的增加，性能有进一步改进的可能。\n最终提出的网络（Arch3）有近 1.6M 的参数，而其他没有注意机制的架构有近 1.3M 的参数。\nIII. EVALUATION   DIV2K dataset\n  Classic5、LIVE1、Set5、Set14 和 BSD100(Berkeley segmentation dataset) 数据集\n  低质量的压缩图像是由 Matlab JPEG 编解码器产生。\n最终网络由 6 个 Inception 模块组成，并进行批量归一化处理。\n对于 SISR 的定量评估，使用了 PSNR 和 SSIM 指标，PSNR- B 用于 compression artifact removal。\n针对超分辨率和 JPEG 伪影去除（for superresolution and JPEG artifact removal）。\nTABLE III 列出了本篇论文提出的网络在 PSNR、SSIM 和 PSNR-B 方面的结果。\n在两个数据集上，比 JPEG 结果平均提高了约 1.89dB PSNR，而比 MemNet 平均提高了约 0.095dB。\nTABLE IV，本篇论文提出的网络在 Set5 和所有超分辨率因子上比 bicubic interpolation（二次方插值）实现了近3.9dB 的 PSNR 平均改进，而在 Set14 和 BSDS100 上的平均改进分别接近 2.7dB 和 2dB。在所有数据集上，比 MemNet 的平均改进幅度接近 0.13dB。\nFig 3 是 compression artifact removal and SISR 的结果（压缩伪影去除和 SISR）。\n图 3 分别显示了 Classic5 和 LIVE1 测试数据集的两个例子，以及一对裁剪的部分。在图像中可以观察到，与其他 CNN 相比，本篇论文提出的模型执行的重建效果相对更好。DnCNN 和 ARCNN 在某些高纹理区域存在块状伪影和模糊现象，而本篇论文的模型的结果显示，相同的区域相对来说更加平滑。\n在上边一行的例子中，可以观察到本论文模型能够比其他方法更好地重建围巾上的图案并去除某些伪影。在下边一行的例子中，可以看到在右边的补丁上，本论文的模型相比于其他方法去除了大部分字母周围的块状伪影，而在左边的补丁上，边缘相对更清晰。然而，整个图像中的某些区域由于在压缩的 JPEG 图像中完全丧失了信息，所以根本没有重建。\nFig.4 可以观察到的是，本论文的模型产生的结果与原始图像有更好或更接近的相似度，并在模式变化过程中产生更平滑的过渡。在斑马的图像中，从补丁中可以看出，与其他方法相比，产生的伪影更少，图案更平滑。\n同样，在最下面一行的例子中，从补丁中可以看出，本论文的模型试图更可靠地重建图案。然而，对于某些图像，其他 CNN 似乎在某些区域产生了更清晰的纹理。\n总的来说，可以得出结论，所提出的模型在引入了带有注意力机制的 Inception 模块后，对于这两个增强任务的表现令人钦佩。该模型的深度比较适中，作为未来的努力方向，可以尝试降低 Inception 块内的特征图深度，而增加 Inception 模块的数量，因为实验表明随着网络深度的增加，该模型还有进一步改进的空间。由于文献中存在更多的注意力形式，可以对其进行详细研究并适当实现。另外值得注意的是，与 SISR 相应的相对改进相比，compression artifact（压缩伪影）去除的相对定量改进较为一致，未来应从网络架构和各个任务的损失函数的角度进行仔细研究。\nIV. SUMMARY AND CONCLUSION 实验采用不同的网络深度和模型架构来研究它们对整体重建质量的影响。在基准数据集上对所提出的网络进行评估，结果显示，与其他模型相比，attentive inception modules 网络在伪影抑制和重建质量上都有改善。\n","permalink":"http://landodo.github.io/posts/20210310-attentive-inception-module/","summary":"Attentive Inception Module based Convolutional Neural Network for Image Enhancement “基于 Attentive Inception Module 的卷积神经网络在图像增强中的应用” high quality：高质量 low distortion：低失真 singleimage superresolution (SISR)：单","title":"Attentive Inception Module based Convolutional Neural Network for Image Enhancement"},{"content":"注意力机制 Non-local Neural Networks 注意力机制的文献，arXiv:1711.07971v3\nFaceBook Research\nAbstract 论文提出了一种非局部操作（non-local operations）来捕获长范围的依赖性。非局部操作将某一位置的响应计算为所有位置特征的加权和。\nnon-local 模型在 Kinetics 和 Charades 数据集取得了很好的成绩（compete or outperform current competition winners）。\n1. Introduction 对于序列数据，递归操作（recurrent operations）是长程依赖性建模的主流解决方案。对于图像数据，长距离的依赖性是由卷积运算深度堆叠形成的大感受野来建模的。\n卷积运算和递归运算都是在空间或时间上处理局部邻域。因此，只有当这些操作被反复应用，在数据中逐步传播信号时，才能捕捉到长程依赖性。反复应用这些局部操作存在如下的局限性。\n（1）计算效率低\n（2）优化困难\n（3）当消息需要长距离的来回传递时，这种依赖性建模变得困难\n本篇论文提出的 non-local operation，用于捕捉深度神经网络的长范围依赖性，这个操作是一个高效、简单、通用的组件。非局域操作是计算机视觉中经典的非局域平均操作（non-local mean operation）的泛化。\n 非局域操作将某一位置的响应计算为所有位置的特征的加权和。位置可以是空间、时间或时空，适用于图像、序列和视频问题。  Figure 1：一个位置 $x_i$ 的响应是由所有位置 $x_j$ 的特征的加权平均值计算出来的。\n非局部操作的优点：\n（1）通过计算任意两个位置之间的相互作用，直接捕捉长程依赖性，而不管它们的位置距离如何；\n（2）高效，即使网络只有几层也能取得很好的效果；\n（3）非局域运算保持了可变的输入大小，并且可以很容易地与其他运算相结合。\n2. Related Work  Non-local image processing. Graphical models. Feedforward modeling for sequences. Self-attention. Interaction networks. Video classification architectures.:  3. Non-local Neural Networks 3.1. Formulation  i是一个输出位置（在空间、时间或时空）的索引 j 是列举所有可能位置的索引 x​ 是输入信号（图像、序列、视频；通常是它们的特征） y 是与 x 大小相同的输出信号 配对函数 f 计算 i 和所有 j 之间的关系，结果是一个标量 一元函数 g 计算输入信号在位置 j 的表示 响应由系数 C(x) 归一化。  3.2 Instantiations （1）Gaussian\n（2）Embedded Gaussian\n（3）Dot product.\n（4）Concatenation.\n3.3. Non-local Block Figure 2 是一个 Non-local block 的例子。\n4. Video Classification Models 2D ConvNet baseline (C2D)\n通过“膨胀”内核（“inflating” the kernels），将 Table 1 中的 C2D 模型变成一个 3D 卷积对应模型。\nInflated 3D ConvNet (I3D).\n one can turn the C2D model in Table 1 into a 3D convolutional counterpart by “inflating” the kernels.  Non-localnetwork.\n insert non-local blocks into C2D or I3D to turn them into non-local nets.  4.1. Implementation Details   Training.\n  Inference.\n  5. Experiments on Video Classification Figure 1、Figure 3 直观地展示了 Non-local 模型计算的非本地块行为的几个例子。网络可以学习寻找有意义的关系线索，而不考虑空间和时间的距离。\n 箭头的起点表示 $x_i$，终点表示 $x_j$。每个 $xi$ 的 20 个最高权重的箭头被可视化。 输入的 Video 有 32 帧，以步幅为 8，取其中的 4 帧进行可视化显示模型如何找到相关线索来支持其预测。  5.1. Experiments on Kinetics  (a) Instantiations (b) Which stage to add non-local blocks? (c) Going deeper with non-local blocks. (d) Non-local in spacetime. (e) Non-localnetvs.3DConvNet. (f) Non-local 3D ConvNet. (g) Longer sequences.  Figure 4 显示了 ResNet-50 C2D baseline 与带有 5 个块 Non-local C2D 的训练过程的曲线。\nComparisons with state-of-the-art results.\n5.2. Experiments on Charades 6. Extension: Experiments on COCO COCO: object detection/segmentation and human pose estimation (keypoint detection).\n(1) Object detection and instance segmentation.\n(2) keypoint detection.\n7. Conclusion 本篇论文提出了一类新的神经网络，它通过非局部操作来捕捉长程依赖性。\nNon-local Block 可以与现有的网络架构相结合，non-local modeling 在 video classification, object detection and segmentation, and pose estimation 任务中具有重要的意义。\n在如上所有的任务中，为 baseline 添加 Non-local Block 后，都能在 baseline 的基础之上提升模型性能。\n","permalink":"http://landodo.github.io/posts/20210308-non-local-neural-networks/","summary":"注意力机制 Non-local Neural Networks 注意力机制的文献，arXiv:1711.07971v3 FaceBook Research Abstract 论文提出了一种非局部操作（non-local operations","title":"Non-local Neural Networks"},{"content":"论文阅读 Selective Kernel Networks 注意力机制论文阅读，第二次汇报的论文为：\n 下载地址：（SKNet）Selective Kernel Networks (arXiv: 1903.06586) 发表时间（e-prints posted on arXiv）：2019 年 03 月.  Abstract 传统的卷积神经网络中的每一层中的神经元的感受野都是相同的大小。在神经科学领域，视觉皮层神经元的感受野大小是受刺激调节的，这一点在 CNN 中很少被考虑到。\n本篇论文设计了一种叫做 ”Selective Kernel Unit“ 的块，使用 softmax attention 融合具有不同卷积核大小的多个分支。 对这些分支的不同关注度使得在融合时神经元的有效感受野大小上的不同。\n多个 SK 块的堆叠得到 SKNet，这个名字也是为了致敬 SENet。\nSKNet 在 ImageNet、CIFAR 数据集上都取得了 SOTA。\n详细的实验分析表明，SKNet 中的神经元可以捕获具有不同比例的目标对象，实验验证了神经元根据输入自适应地调整其感受野大小的能力。\n1. Introduction 在视觉皮层中，同一区域中神经元的感受野大小是不同的，这使得神经元可以在同一处理阶段收集多尺度的空间信息。下图就是相同目标对象的不同尺度。\n让神经元可以在同一处理阶段收集多尺度的空间信息的机制，已经广泛被采用。InceptionNet 就是其中一个例子，它通过设计一个简单的级联（concatenation）来聚合来自内部 1×1、3×3、5×5、7×7 卷积核的多尺度信息。\n许多的实验证明，神经元的感受野大小不是固定的，而是受刺激调节的。InceptionNet 是一种在同一层具有多尺度信息的模型，但是这种机制比较的固定，不同分支的多尺度信息的汇聚方法也比较的简单，可能不足以为神经元提供强大的适应能力。\n本篇论文提出了一种方法，从多种卷积核中聚集信息，以实现神经元的自适应感受野大小。论文的作者将 SKNet 的核心描述为一句话：用 multiple scale feature 汇总的 information 来 channel-wise 地指导如何分配侧重使用哪个 kernel 的表征。\nSKNet 的核心是 “Selective Kernel”（SK）卷积，它由三种操作组成：Split、Fuse 和 Select。\n Split：生成具有各种内核大小的多个路径，这些大小对应于神经元的不同感受野大小。 Fuse：组合并汇总来自多个路径的信息，以获得选择权重的全局和全面表示。 Select：根据选择权重聚合大小不同的内核的特征图。  最后，为了验证所提出的模型能够自适应调节神经元感受野大小，论文作者通过放大自然图像中的目标对象并缩小背景以保持图像大小不变来模拟刺激。发现当目标对象变得越来越大时，大多数神经元从更大的核分支路径中收集的信息越来越多（权重越大）。这些结果表明，SKNet 中的神经元具有自适应的感受野大小。\n2. Related Work （1）多分支的卷积网络\n Highway、ResNet、InceptionNet\u0026hellip;  （2）Grouped/depthwise/dilated convolutions\n 分组卷积：AlexNet、ResNeXt\u0026hellip; 深度可分离卷积：Xception、MobileNetV1、MobileNetV2、ShuffleNet\u0026hellip; 空洞卷积  （3）注意力机制\nSKNet 第一个通过引入注意力机制来明确关注神经元的自适应感受野大小。（4）Dynamic Convolution\n Spatial Transform Networks\u0026hellip;  3. 解析 Selective Kernel Convolution  SKNet：用 multiple scale feature 汇总的 information 来 channel-wise 地指导如何分配侧重使用哪个 kernel 的表征。\n adaptively adjust their RF sizes. 自适应的调整其感受野的大小。   SK 模块所做的工作是：输入的特征图为 $X \\in \\mathbb{R}^{H \\times W \\times C}$，经过 SK Convolution 后，得到输出的特征图为 $V \\in \\mathbb{R}^{H \\times W \\times C}$。SK 卷积有 3 个步骤：Split, Fuse and Select.\n3.1 SK Convolution: Split(1/3) 对于输入的特征图 $X \\in \\mathbb{R}^{H \\times W \\times C}$，默认情况下进行两次转换：\n（1）$\\tilde{F}: X \\rightarrow \\tilde{U} \\in \\mathbb{R}^{H \\times W \\times C}$\n卷积核的大小为 $3 \\times 3$。\n（2）$\\hat{F}: X \\rightarrow \\hat{U} \\in \\mathbb{R}^{H \\times W \\times C}$\n卷积核的大小为 $5\\times 5$。为了进一步提高效率，$5 \\times 5$ 的卷积使用空洞大小为 2 的 $3 \\times 3$ 的卷积来替代。\n3.2 SK Convolution: Fuse(2/3) Fuse 操作和 Squeeze and Excitation block 一样。\n（1）integrate information from all branches.\n将分支进行 element-wise 的求和，得到 $U \\in \\mathbb{R}^{H \\times W \\times C}$。\n（2）global average pooling.\n得到 $s \\in \\mathbb{R}^{C \\times 1}$，s 是一个有 C 个元素的列向量。\n（3）compact feature by simple fully connected (fc) layer.\n使用全连接层，即 $s \\in \\mathbb{R}^{C \\times 1} \\rightarrow z \\in \\mathbb{R}^{d \\times 1}$，其中 $d \u0026lt; C$。\n $\\delta$ 使用 ReLU 函数， $B$ 表示 Batch Normalization， $W \\in \\mathbb{R}^{d \\times C}$，权重矩阵。 z 被称为 compact feature descriptor.   reduction ratio $r$ 是一个超参数， 一般设置 L = 32。  3.3 SK Convolution: Select(3/3) 这一步是 SK Block 的核心操作。\n A soft attention across channels is used to adaptively select different spatial scales of information, which is guided by the compact feature descriptor z.\n （1）Soft attention across channels.\n $A, B \\in \\mathbb{R}^{C \\times d}$，这两个矩阵也是端到端训练出来的。如果只有两个分支，那么矩阵 B 是冗余的，因为 Softmax 的输出 $a_c + b_c = 1$，可以通过 1 减去另一个得到。 $z \\in \\mathbb{R}^{d \\times 1}$，经过 softmax 操作后，得到的 $a \\in \\mathbb{R}^{C \\times 1}$.  z 被称为 compact feature descriptor. a 被称为 soft attention vector.    （2）The final feature map $V$ is obtained through the attention weights on various kernels.\n $\\tilde{U}, \\hat{U} \\in \\mathbb{R}^{H \\times W \\times C}$ $a, b \\in \\mathbb{R}^{C \\times 1}$ 执行的操作是 element-wise product. $a_c \\cdot \\tilde{U}_c$ 表示第 c 个通道的特征图上的每个点，都乘以数 $a_c$。  综上，就是 SK Convolution 的内部原理细节。\n4. Experiments 4.1 ImageNet Classification SKNet 与 state-of-the-art 的比较（1/4） Table2 显示了，在 ImageNet Classification 任务上，SKNet 与各种 state-of-the-art 模型的比较。\n首先将 SKNet-50 和 SKNet-101 与具有相似模型复杂性的模型进行比较。结果表明，在类似的参数量和计算量下，SKNets 始终能够达到最高的性能。\n ResNeXt-101 的参数量比 SKNet-50 大 60％，计算量大 80％，但 SKNet-50 的性能却比 ResNeXt-101 高出 0.32％ 以上。 与 InceptionNets 相比，SKNet 的复杂程度相近或更低，SKNets 的性能提高了1.5％ 以上。 与同行 SENet 相比，SKNet 使用较少的参数，SKNets 可以获得比 SENet 低 0.3-0.4％ 的错误率。  SKNet 与 ResNeXt的比较： Selective Kernel vs. Depth/Width/Cardinality.（2/4） Table 3 显示了，与 ResNeXt（使用 32×4d 的设置）相比，SKNets 不可避免地会引入一些参数和计算上的增加。\n为了公平地比较 ResNeXt 和 SKNet，通过更改 ResNeXt 的深度、宽度和基数来增加其复杂性，以匹配 SKNets 的复杂性。结果显示，ResNeXt 模型复杂度的提升能够带来更低的错误率。\n在参数量和计算量相近的情况下，SKNet 依旧是表示得最好的。\nSKNet 与其他模型的参数量与性能情况（3/4） Figure 2 显示了，SKNet 的 top-1 错误率相对于其参数数量的关系。\nSKNet 与轻量级模型的比较（4/4） Table 4. 显示了，SK 卷积不仅能够在 baseline 的基础之上提升性能，而且比 SENet 要更好。表明了 SK 卷积应用在低端设备上的巨大潜力。\n4.1 CIFAR Table 5 显示了，在相同的 backbone（ResNeXt-29, 16 × 32d）下，\n SKNet 的性能优于 ResNeXt，并且参数量减少了 60% SKNet 始终优于 SENet，并且参数量减少了 22%  4.3 消融研究 Ablation Studies dilation D（1/2） Table 6 显示了，在 ImageNet 数据集上，如下两种具有相同感受野的卷积核：\n kernel size 5×5，空洞 D=1 kernel size 3×3，空洞 D=2  各种 3×3 kernels with various dilations 明显优于与它具有相同感受野的 kernel。3×3, D=3 优于 7×7, D=1。\n各种不同 Kernel 的结合（1/2） 研究分支数 M 的影响。\nTable 7 中，\n K3：标准了 3×3 卷积核 K5：3×3 with dilation 2 K7：3×3 with dilation 3  结果表明：\n（1）随着分支数 M 的增加，top-1 错误率下降了。\n（2）无论 M=2 或者 M=3，SK 的多路聚合始终比简单的聚合方法的 top-1 错误率更低。\n（3）M=2，M=3 的 top-1 错误率差不多（20.79%，20.76%）。为了更好的平衡模型的性能和复杂度，M = 2 是更好的选择。\n4.4 Analysis and Interpretation 上图 Figure3 (a)(b)，以两个随机的样本为例，对于大多数通道，当目标对象增大（1.0×、1.5×、2.0×），大核（5×5）的注意力权重增加，红线大部分位于蓝绿两线之上。\n ❓3×3 的也增加了呢？为什么没有画出来进行对照！\n答：因为只有两个分支，5×5 分支的注意力增加了，3×3 分支的注意力就必须减小，两分支的 soft attention vector 相加和为 1。\n (c) 图显示了所有验证图像的平均值，红线（2.0×）大部分位于蓝绿两线之上。\n论文中的另一个发现是：\n  目标对象越大，则在网络浅层和中层阶段（例如，SK 2_3，SK 3_4），“Selective Kernel” 机制将把更多的注意力分配给更大的内核，条形图中粉色的条值最大（a,b 中的 2_3、3_4 阶段）。但是，在更深的层上（例如，SK 5_3），就没有这种规律性了。\n 但是相减后为负值，5×5 的注意力仍小于 3×3 的注意力，但是 5×5 的注意力随目标对象的增大而增大。\n  (a) 中在 SK 5_3 阶段，5×5 核的注意力减去 3×3，在 1.5× 和 2.0× 的目标对象上，得到的值非常接近（注意力相近），表明网络并不偏向于选择更大的卷积核还是更小的卷积核。 (b) 中在 SK 5_3 阶段就是很乱了，绿条比蓝条小，表明目标对象从 1.0× 放大到 1.5× 后，网络把注意力分配给了更小的 3×3 核；而粉条比绿条大，表明对标对象从 1.5× 放大到 2.0× 后，网络把注意力分给了更大的 5×5 核。 因此，在深层的网络中，对于不同尺度的目标对象，网络对于选择怎么样的核没有什么规律。    论文做了一个实验，对于每个类别，各取 50 张图片，针对 1.0× 和 1.5× 的目标对象绘制平均注意力差异（5×5 的注意力权重减去 3×3 的）。\n如 Figure 4 所示，观察到以前的现象适用于所有 1,000个 类别，其中，随着目标对象规模的增长，5×5 核的重要性持续且同时增加。（浅层和中层橙色线始终位于蓝色线之上）\n 都是负值，是不是代表着 5×5 卷积核的注意力都小于 3×3 的注意力。是的，但是这个图想要说明的是 5×5 随目标对象的增大而增大。\n 这表明，在网络的早期，可以根据对象大小的语义意识来选择适当的内核大小，从而可以有效地调整这些神经元的感受野大小。\n但是，这种现象在像 SK 5_3 这样的非常深的层中不存在了。橙蓝两线交错表示 5×5 的注意力权重减去 3×3 的注意力权重的差时正时负。\n","permalink":"http://landodo.github.io/posts/20210306-selective-kernel-networks/","summary":"论文阅读 Selective Kernel Networks 注意力机制论文阅读，第二次汇报的论文为： 下载地址：（SKNet）Selective Kernel Networks (arXiv: 1903.06586) 发表时间（e-prints posted on arX","title":"Selective Kernel Networks"},{"content":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。\n Hugo 默认是使用 TOML，现在将此更改为更易阅读的 YAML。 Hugo 需要在源目录查找一个 config.toml 的配置文件。如果这个文件不存在，将会查找 config.yaml，然后是 config.json\n 添加站内搜索 Search Page 全局文章内容搜索，从关键字快速定位到文章。\nHogu 文档：https://adityatelange.github.io/hugo-PaperMod/\nPaperMod uses Fuse.js Basic for seach functionality\nAdd the following to site config, config.yml\noutputs:  home:  - HTML  - RSS  - JSON # is necessary Create a page with search.md in content directory with following content\n--- title: \u0026#34;Search\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # is necessary # url: \u0026#34;/archive\u0026#34; # description: \u0026#34;Description for Search\u0026#34; summary: \u0026#34;search\u0026#34; --- To hide a particular page from being searched, add it in post’s fron’t matter\n--- searchHidden: true copy\nex: [search.md]\n添加 Archives 功能 这是自动的，只需要添加一个按钮，其他的就不用管了。\nhttps://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n增加公式支持 此功能会拖慢网站的加载速度，这个以后再说！些许的加载时间能换取良好的阅读体验，目前来看是值得的。我的论文阅读笔记里充斥了大量的 LaTeX 公式。\n公式支持基于 MathJax。\nhttps://mertbakir.gitlab.io/hugo/math-typesetting-in-hugo/\n行内公式：\n给定一组训练样本 $\\mathcal{D}$，TumorCP 有 $(1 - p_{cp})$​​​ 的概率不执行任何操作；有 $p_{cp}$​ 的概率从 $\\mathcal{D}$ 中采样出一个对图像 $(x_{src}, x_{tgt}) \\sim \\mathcal{D}$​​，并执行一次 “Copy-Paste”。\n令 $\\mathcal{O}{src}$ 为图像 $x{src}$ 上肿瘤集合，$\\mathcal{V}{tgt}$ 为 $x{tgt}$ 上的器官的体积坐标集合，$\\mathcal{T}$ 是一组随机数据转换，每个转换都有一个称为 $p_{trans}$​ 的概率参数。\n一次 “Copy-Paste” 流程：\n TumorCP 首先采样一个肿瘤 $o \\sim \\mathcal{O}{src}$​、一组数据转换 $\\tau \\sim \\mathcal{T}$​ 和一个目标位置 $v \\sim \\mathcal{V}{tgt}$； 然后将 $τ(o)$ 以 $v$​ 为中心，取代原始数据和标注。  行间公式：\n$$\\sqrt{x} + \\sqrt{x^{2}+\\sqrt{y}} = \\sqrt[3]{k_{i}} - \\frac{x}{m}$$\n测试 Emoji 🧡💥💢💌💝🕎☪\n测试代码 import sys from PyQt5 import QtCore, QtGui, QtWidgets, uic from PyQt5.QtCore import Qt, QEvent import random  from PyQt5.QtGui import QPixmap from PyQt5.QtWidgets import QAction, QFileDialog   class Canvas(QtWidgets.QLabel):  def __init__(self, parent=None):  super().__init__(parent)  self.background = QPixmap(200, 200)  self.background.fill(Qt.yellow)  # self.clear(Qt.yellow)  self.last_x, self.last_y = None, None  self.pen_color = QtGui.QColor(\u0026#39;#000000\u0026#39;)  self.setPixmap(self.background)   def mouseReleaseEvent(self, e):  self.last_x = None  self.last_y = None 图片的显示 这里需要将图片拷贝的 public 对应的文章目录下，使用原本的相对路径。不需要将文件夹拷贝到 content/posts/ ，这个文件夹下至保存 Markdown 文件。\n使用 hugo 生成文章的 HTML 文件后，每一篇文章都会以 Markdown 文件中的 title 生成一个文件夹。如本篇文章的文件夹名称为 first-post （全小写）。\n在访问时，使用的是如下的链接：\nhttps://landodo.github.io/first-post/0000465981_017.jpg 如果图片实现依旧不正常，使用 F12 进行调试，快速定位问题所在。\n","permalink":"http://landodo.github.io/posts/first-post/","summary":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。 Hugo 默认是使用 TOML，现在将此更改为更易阅","title":"First Post for Hugo"},{"content":"Inside-Outside Net 论文阅读笔记  Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks 时间：2015 年 12 月   SENet——胡杰：https://www.sohu.com/a/161633191_465975\n卷积核作为卷积神经网络的核心，通常被看做是在局部感受野上，将空间上（spatial）的信息和特征维度上（channel-wise）的信息进行聚合的信息聚合体。\n最近很多工作被提出来从空间维度层面来提升网络的性能，如 Inception 结构中嵌入了多尺度信息，聚合多种不同感受野上的特征来获得性能增益；在 Inside-Outside 网络中考虑了空间中的上下文信息；还有将 Attention 机制引入到空间维度上，等等。这些工作都获得了相当不错的成果。\n已经有很多工作在空间维度上来提升网络的性能。那么很自然想到，网络是否可以从其他层面来考虑去提升性能，比如考虑特征通道之间的关系？我们的工作就是基于这一点并提出了 Squeeze-and-Excitation Networks（简称 SENet）。在我们提出的结构中，Squeeze 和 Excitation 是两个非常关键的操作，所以我们以此来命名。我们的动机是希望显式地建模特征通道之间的相互依赖关系。\n采用了一种全新的「特征重标定」策略。具体来说，就是通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。\n Abstract   Inside-Outside Net (ION)：一个利用感兴趣区域的内部和外部的信息的物体检测器（object detector）。\n  利用 skip pooling 从多个尺度和抽象级别提取信息。\n  上下文（context）和多尺度（multi-scale）表示改进了小物体的检测。\n  1. Introduction 目前 SOTA 的检测方法仅使用对象感兴趣区域（region of interest, ROI）附近的信息，这对可能限制了被检测到的物体的类型和准确性。\n本篇论文使用了两种信息：\n（1）multi-scale representation\n通过从 ConvNet 中的多个较低级别的卷积层中进行池化，捕获细粒度的细节（fine-grained details）。\n（2）contextual information\n为了收集上下文信息，使用了空间递归神经网络（spatial Recurrent Neural Networks, RNNs）。\nRNN 在图像上水平和垂直传递空间变化的上下文信息。至少使用两个 RNN 层可确保信息可以在整个图像上传播。\n其他获取上下文信息的方法还有:\n global average pooling：提供了关于整张图片的信息。 additional convolutional layers.  使用 proposal detectors 来识别图片中的 ROIs，将每个 ROIs 归类为包含一个或不包含任何感兴趣的对象。使用 dynamic pooling 可以通过一次前向传播有效的评估成千上万中不同的获选 ROI。对于每个候选的 ROI，将多尺度和上下文信息合并为一个层，并通过几个全连接层进行分类。\n上下文和多尺度这两种来源在本质上都是互补的。直觉上也很符合，上下文特征在整个图像中看起来很宽泛，而多尺度特征捕获了更细粒度的细节。\n总的来说，这种方法比以前的最新技术更擅长检测小物体。对于椅子等被严重遮挡的对象，使用上下文信息可以缓解。\n论文的贡献：\n （1）介绍了利用上下文和多尺度 skip pooling 进行对象检测的 ION 结构。 （2）在 PASCAL VOC 2007（mAP 为 79.2％），VOC 2012（mAP 为 76.4％）和 COCO（mAP 为 24.9％）上取得了 SOTA 的结果。 （3）进行了大量的实验，评估了各种选择。 （4）分析检测器的性能，发现整体上提高的准确性，特别是对于小物体。  2. Prior Work ConvNet object detectors：AlexNet、R-CNN、Fast R-CNN、OverFeat、Fast R-CNN、SPP-Net.\nSpatial RNNs：bidirectional RNNs.\nSkip-layer connections.\n3. Architecture: Inside-Outside Net (ION) ION 是一种在 ROI 内外都具有改进的描述符的检测器。图像由单个深层 ConvNet 处理，并且 ConvNet 各个阶段的卷积特征图都存储在内存中。\n在网络的顶部，一个 2 倍堆叠的 4 方向 IRNN（稍后说明）计算上下文特征，这些特征描述全局和局部图像。\ncontext features 与 “conv5” 具有相同的尺寸。每个图像执行一次。此外，有数千个 proposal regions（ROIs），其中可能包含对象。对于每个 ROI，从几层（“ conv3”，“ conv4”，“ conv5”和“context features”）中提取固定长度的特征描述符。\n对描述符进行 L2 归一化，级联（concatenated），重新缩放（re-scaled）和降维（1x1 卷积），以针对大小为512x7x7 的每个 poposal 生成固定长度的特征描述符。\n两个全连接层处理每个描述符并产生两个输出：K 个对象类别的预测之一（“ softmax”）和对投标区域的边界框（“ bbox”）的调整。\n3.1 Pooling from multiple layers 最近成功的检测器，例如 Fast R-CNN，Faster R-CNN 和 SPPnet，都来自 VGG16 的最后卷积层（“ conv5_3”）。为了将其扩展到多层，我们必须考虑尺寸（dimensionality）和振幅（amplitude）问题。\n使用经过 ImageNet 预训练过的 VGG16，要汇聚更多的层，最终的特征还必须是 512x7x7，除了匹配原始形状外，还必须匹配原始激活幅度，以便输入到 fc6 中。\n  为了匹配所需的 512x7x7：通道轴进行 concatenate，1x1 卷积减小尺度。\n  为了匹配原始幅度，对每个合并的 ROI 进行归一化，然后根据经验确定的比例重新缩放。\n  3.2. Context features with IRNNs 在最后一个卷积层（conv5）的顶部，放置了沿图像横向移动的 RNN。\n传统上，RNN 沿序列从左到右移动，在每一步都消耗输入，更新其隐藏状态并产生输出。通过将 RNN 沿着图像的每一行和每一列放置，将此扩展到了二维。总共有四个 RNN 在基本方向上移动：右，左，下，上。 RNN 位于 conv5 的顶部，并产生与 conv5 相同形状的输出。\n有研究表明，如果将递归权重矩阵初始化为恒等矩阵，则网络将更易于训练并且擅上对远程依赖关系进行建模。\nIRNN：ReLU RNN 以这种方式初始化。优点：\n 在语言建模任务中的性能几乎与 LSTM 相同 非常易于实现和并行化 比 LSTM 或 GRU 的计算快得多  四个独立的 IRNN 在四个方向上移动。输入到隐藏的过渡（input-to-hidden transition）是 1x1 卷积，并且可以在不同方向上共享它。\n这是向右移动的 IRNN 的更新；对于其他方向，存在类似的方程式：\n在第一个 4 方向 IRNN（两个 IRNN 中的一个）之后，获得了一个特征图，该特征图汇总了图像中每个位置的附近物体。如图 4 所示，可以看到第一个 IRNN 在每个单元的左/右/上/下创建了特征的摘要。然后，随后的 1x1 卷积将这些信息混合在一起以减小尺寸。\n在第二个 4 方向 IRNN 之后，输出上的每个单元都取决于输入的每个单元。这样，上下文功能既是全局的又是局部的。这些功能因空间位置而异，并且每个单元格都是相对于该特定空间位置的图像的全局摘要。\n4. Results 评估数据集：PASCAL VOC 2007, VOC 2012, and on MS COCO.\n","permalink":"http://landodo.github.io/posts/20210302-inside-outside-net/","summary":"Inside-Outside Net 论文阅读笔记 Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks 时间：2015 年 12 月 SENet——胡杰：https://www.sohu.com/a/161633","title":"Inside-Outside Net"},{"content":"Attention mechanism: SENet \u0026amp; SKNet 注意力机制论文阅读，第二次汇报的论文为：\n 下载地址：（SKNet）Selective Kernel Networks (arXiv: 1903.06586) 发表时间（e-prints posted on arXiv）：2019 年 03 月.  0. 论文列表（已读一遍）   Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks (arXiv: 1512.04143)\n  Spatial Transformer Networks (arXiv: 1506.02025)\n  Residual Attention Network for Image Classification (arXiv: 1704.06904)\n  ✅ （SENet）Squeeze-and-Excitation Networks (arXiv: 1709.01507)\n   SENet brings an effective, lightweight gating mechanism to self-recalibrate the feature map via channel-wise importances.——SKNet 论文中对 SENet 的描述。\n     Concurrent Spatial and Channel ‘Squeeze \u0026amp; Excitation’ in Fully Convolutional Networks (arXiv: 1803.02579)\n   three variants of SE modules for image segmentation.\n     CBAM: Convolutional Block Attention Module (arXiv: 1807.06521)\n   Beyond channel, BAM [32] and CBAM [45] introduce spatial attention in a similar way.\n——SKNet 论文中的描述。\n     ✅ （SKNet）Selective Kernel Networks (arXiv: 1903.06586)\n   1. 解析 Selective Kernel Convolution  SKNet：用 multiple scale feature 汇总的 information 来 channel-wise 地指导如何分配侧重使用哪个 kernel 的表征。\n adaptively adjust their RF sizes. 自适应的调整其感受野的大小。   SK 模块所做的工作是：输入的特征图为 $X \\in \\mathbb{R}^{H\u0026rsquo; \\times W\u0026rsquo; \\times C\u0026rsquo;}$，经过 SK Convolution 后，得到输出的特征图为 $V \\in \\mathbb{R}^{H \\times W \\times C}$。SK 卷积有 3 个步骤：Split, Fuse and Select.\n1.1 SK Convolution: Split(1/3) 对于输入的特征图 $X \\in \\mathbb{R}^{H\u0026rsquo; \\times W\u0026rsquo; \\times C\u0026rsquo;}$，默认情况下进行两次转换：\n（1）$\\tilde{F}: X \\rightarrow \\tilde{U} \\in \\mathbb{R}^{H \\times W \\times C}$\n卷积核的大小为 $3 \\times 3$。\n（2）$\\hat{F}: X \\rightarrow \\hat{U} \\in \\mathbb{R}^{H \\times W \\times C}$\n卷积核的大小为 $5\\times 5$。为了进一步提高效率，$5 \\times 5$ 的卷积使用空洞大小为 2 的 $3 \\times 3$ 的卷积来替代。\n1.2 SK Convolution: Fuse(2/3) Fuse 操作和 Squeeze and Excitation block 一样。\n（1）integrate information from all branches.\n将分支进行 element-wise 的求和，得到 $U \\in \\mathbb{R}^{H \\times W \\times C}$。\n（2）global average pooling.\n得到 $s \\in \\mathbb{R}^{C \\times 1}$，s 是一个有 C 个元素的列向量。\n（3）compact feature by simple fully connected (fc) layer.\n使用全连接层，即 $s \\in \\mathbb{R}^{C \\times 1} \\rightarrow z \\in \\mathbb{R}^{d \\times 1}$，其中 $d \u0026lt; C$。\n $\\delta$ 使用 ReLU 函数， $B$ 表示 Batch Normalization， $W \\in \\mathbb{R}^{d \\times C}$，权重矩阵。   reduction ratio $r$ 是一个超参数， 一般设置 L = 32。  1.3 SK Convolution: Select(3/3) 这一步是 SK Block 的核心操作。\n A soft attention across channels is used to adaptively select different spatial scales of information, which is guided by the compact feature descriptor z.\n （1）Soft attention across channels.\n $A, B \\in \\mathbb{R}^{C \\times d}$，这两个矩阵也是端到端训练出来的。如果只有两个分支，那么矩阵 B 是冗余的，因为 Softmax 的输出 $a_c + b_c = 1$，可以通过 1 减去另一个得到。 $z \\in \\mathbb{R}^{d \\times 1}$，经过 softmax 操作后，得到的 $a \\in \\mathbb{R}^{C \\times 1}$.  z 被称为 compact feature descriptor. a 被称为 soft attention vector.    （2）The final feature map $V$ is obtained through the attention weights on various kernels.\n $\\tilde{U}, \\hat{U} \\in \\mathbb{R}^{H \\times W \\times C}$ $a, b \\in \\mathbb{R}^{C \\times 1}$ 执行的操作是 element-wise product. $a_c \\cdot \\tilde{U}_c$ 表示第 c 个通道的特征图上的每个点，都乘以数 $a_c$。  综上，就是 SK Convolution 的内部原理细节。\n3. 实验 SK Convolution $\\rightarrow$ SK unit $\\rightarrow$ ==SK Networks==.\n2.1 Selective Kernel (SK) Convolution (1/3) 2.2 Selective Kernel (SK) unit (2/3) 2.3 Selective Kernel (SK) Networks (3/3) 2.4 CIFAR-10 在 CIFAR 数据集上，作者实现的细节：\n 把第二条分支的 5×5 卷积核替换成 1×1，第一条分支保持不变。还是上不去 90%，过拟合严重。   论文中提到，设置 SK[2, 16, 32]，还是上不去 90%。  依旧是过拟合非常严重。进行一定的数据增强（Resize 32×32 为 64×64）后，精度能够上 90%。\n如果获取 EAT？——根绝每个 epoch 的时间，估计总的训练时间。\n3. Notes SK Block 在检测问题中没有显著效果。\n","permalink":"http://landodo.github.io/posts/20210301-attention-mechanism/","summary":"Attention mechanism: SENet \u0026amp; SKNet 注意力机制论文阅读，第二次汇报的论文为： 下载地址：（SKNet）Selective Kernel Networks (arXiv: 1903.06586) 发表时间（e-prints posted on arXiv","title":"Attention mechanism: SENet \u0026 SKNet"},{"content":"AlexNet 中的 LRN（Local Response Normalization） 是什么 对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。\n LRN 操作在哪一步？  答：ReLU 之后。    AlexNet 的 PyTorch 官方实现 （1）PyTorch\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\nPyTorch 把 LRN 给移除了。\n（2）Paper with Code\n下面的一个有 LRN 的版本，来自 Paper with Code。我觉得是写得最清晰的。\nhttps://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40\nclass AlexNet(nn.Module):  \u0026#34;\u0026#34;\u0026#34; Neural network model consisting of layers propsed by AlexNet paper. \u0026#34;\u0026#34;\u0026#34;  def __init__(self, num_classes=1000):  \u0026#34;\u0026#34;\u0026#34; Define and allocate layers for this neural net. Args: num_classes (int): number of classes to predict with this model \u0026#34;\u0026#34;\u0026#34;  super().__init__()  # input size should be : (b x 3 x 227 x 227)  # The image in the original paper states that width and height are 224 pixels, but  # the dimensions after first convolution layer do not lead to 55 x 55.  self.net = nn.Sequential(  nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4), # (b x 96 x 55 x 55)  nn.ReLU(),  nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), # section 3.3  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 96 x 27 x 27)  nn.Conv2d(96, 256, 5, padding=2), # (b x 256 x 27 x 27)  nn.ReLU(),  nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 13 x 13)  nn.Conv2d(256, 384, 3, padding=1), # (b x 384 x 13 x 13)  nn.ReLU(),  nn.Conv2d(384, 384, 3, padding=1), # (b x 384 x 13 x 13)  nn.ReLU(),  nn.Conv2d(384, 256, 3, padding=1), # (b x 256 x 13 x 13)  nn.ReLU(),  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 6 x 6)  )  # classifier is just a name for linear layers  self.classifier = nn.Sequential(  nn.Dropout(p=0.5, inplace=True),  nn.Linear(in_features=(256 * 6 * 6), out_features=4096),  nn.ReLU(),  nn.Dropout(p=0.5, inplace=True),  nn.Linear(in_features=4096, out_features=4096),  nn.ReLU(),  nn.Linear(in_features=4096, out_features=num_classes),  )   def forward(self, x):  \u0026#34;\u0026#34;\u0026#34; Pass the input through the net. Args: x (Tensor): input tensor Returns: output (Tensor): output tensor \u0026#34;\u0026#34;\u0026#34;  x = self.net(x)  x = x.view(-1, 256 * 6 * 6) # reduce the dimensions for linear layer input  return self.classifier(x) 需要注意的是，LRN 发生在 ReLU 激活函数之后。\n接下来看看论文是如何描述 LRN 的。\nLocal Response Normalization ReLU 不需要输入归一化来防止饱和（Saturation），这是 ReLU 的一个理想性质。如果至少有一些训练例子对 ReLU 产生正向输入，学习就会在该神经元中发生。\n图片来源：https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7\n作者发现以下局部归一化方案有助于泛化。响应归一化 $b_{x,y}^{i}$ 由如下表达式得到。\n $a_{x,y}^{i}$ 表示在位置 $(x,y)$ 处应用核 $i$ 卷积计算后，再运用激活函数 ReLU 后的输出。（即 ReLU 后进行 LRN）   如下是 LRN 是整体示意图。\n   响应归一化实现了一种受真实神经元类型启发的横向抑制形式，在使用不同内核计算的神经元输出中创造了大活动的竞争。常量 k，n，α 和 β 是超参数，其值是使用验证集确定的，使用 k = 2，n = 5，α = 10e-4，β = 0.75。在某些层中应用 ReLU 非线性后应用了这种归一化。\n这个方案更正确的说法是”亮度归一化“，因为没有减去平均活性。响应归一化使 top-1 和 top-5 错误率分别降低了 1.4% 和 1.2%。\n在 CIFAR-10 数据集上：一个四层 CNN 在没有归一化的情况下实现了 13% 的测试错误率，而在归一化的情况下实现了 11% 的错误率。\nLRN 细节 接下来深入到 LRN 的细节，看看 LRN 究竟实现了什么样的效果。\n（1）公式的解释\n a 表示卷积层（包括卷积操作和激活操作）后的输出结果。这个输出的结果是一个四维数组 [batch,height,width,channel]。这个输出结构中的一个位置 [a,b,c,d]，可以理解成在某一张特征图中的某一个通道下的某个高度和某个宽度位置的点，即第 a 张特征图的第 d 个通道下的高度为 b 宽度为 c 的点。 $a_{x,y}^{i}$ 表示第 i 片特征图在位置（x,y）运用激活函数 ReLU 后的输出。n 是同一位置上临近的 feature map 的数目，N 是特征图的总数。   参数 $k, n, \\alpha，\\beta$ 都是超参数。k=2，n=5，α=10-4，β=0.75。  举一个例子：\ni = 10, N = 96 时，第 i=10 个卷积核在位置（x,y）处的取值为 $a_{x,y}^{i}$，它的局部响应归一化过程如下：用 $a_{x,y}^{i}$ 除以第 8、9、10、11、12 片特征图位置（x,y）处的取值求和。\n也就是跨通道的一个 Normalization 操作。\ntorch.nn.LocalResponseNorm() $$b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n} ​ \\sum_{c\u0026rsquo;=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c\u0026rsquo;}^2\\right)^{-\\beta}$$\nInit signature: nn.LocalResponseNorm(  size:int,  alpha:float=0.0001,  beta:float=0.75,  k:float=1.0, ) -\u0026gt; None  Docstring: Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.  Args:  size: amount of neighbouring channels used for normalization  alpha: multiplicative factor. Default: 0.0001  beta: exponent. Default: 0.75  k: additive factor. Default: 1  Shape:  - Input: :math:`(N, C, *)`  - Output: :math:`(N, C, *)` (same shape as input)  Examples::   \u0026gt;\u0026gt;\u0026gt; lrn = nn.LocalResponseNorm(2)  \u0026gt;\u0026gt;\u0026gt; signal_2d = torch.randn(32, 5, 24, 24)  \u0026gt;\u0026gt;\u0026gt; signal_4d = torch.randn(16, 5, 7, 7, 7, 7)  \u0026gt;\u0026gt;\u0026gt; output_2d = lrn(signal_2d)  \u0026gt;\u0026gt;\u0026gt; output_4d = lrn(signal_4d) 使用：\nimport torch import torch.nn as nn  torch.__version__ # \u0026#39;1.7.0\u0026#39; lrn = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2) signal_2d = torch.randn(32, 96, 55, 55) # batch_size=32, feature_map=96×55×55 output_2d = lrn(signal_2d) signal_2d.shape # torch.Size([32, 96, 55, 55]) output_2d.shape # torch.Size([32, 96, 55, 55]) 手算检验 import torch torch.__version__ \u0026#39;1.7.0\u0026#39; import torch.nn as nn lrn = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2) torch.manual_seed(666) \u0026lt;torch._C.Generator at 0x7ffd61652d38\u0026gt; signal_2d = torch.randn(2, 3, 2, 2) # batch_size=2, feature_map=3×2×2 output_2d = lrn(signal_2d) output_2d.shape torch.Size([2, 3, 2, 2]) signal_2d.shape torch.Size([2, 3, 2, 2]) signal_2d tensor([[[[-0.7747, 0.7926], [-0.0062, -0.4377]], [[ 0.4657, -0.1880], [-0.8975, 0.4169]], [[-0.3479, -0.4007], [ 0.8059, -0.1021]]], [[[-0.3055, -1.7611], [-0.6461, 0.3470]], [[ 0.9144, 1.6259], [-0.6535, -0.0865]], [[ 0.2100, 0.4811], [ 0.4506, 0.0600]]]]) output_2d tensor([[[[-0.4607, 0.4713], [-0.0037, -0.2603]], [[ 0.2769, -0.1118], [-0.5336, 0.2479]], [[-0.2069, -0.2383], [ 0.4792, -0.0607]]], [[[-0.1816, -1.0471], [-0.3842, 0.2063]], [[ 0.5437, 0.9667], [-0.3886, -0.0514]], [[ 0.1249, 0.2860], [ 0.2680, 0.0357]]]]) 分析：\n这个 batch 里面的第一张特征图、第一个通道、(0,0) 位置的取值为 -0.7747。接下来分析其 LRN 归一化后的值。\nLRN 的超参数：size=5, alpha=0.0001, beta=0.75, k=2。\n$$b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n} ​ \\sum_{c\u0026rsquo;=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c\u0026rsquo;}^2\\right)^{-\\beta}$$\n$$\\frac{-0.7747}{(2 + \\frac{0.0001}{5} (0.7747^2 + 0.4657^2 + 0.3479^2))^{0.75}} = -0.4607$$\n  跨通道求和的下限：max(0, 0 - 5/2) = 0\n  上限：min(2, 0 + 5/2) = 2    LRN 归一化公式内部的细节理解就先这样了，至于它更深层的作用，以及它为什么会被舍弃，留到后面。\n","permalink":"http://landodo.github.io/posts/20210205-what-is-lrn/","summary":"AlexNet 中的 LRN（Local Response Normalization） 是什么 对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。 LRN 操作在哪一步","title":"AlexNet 中的 LRN（Local Response Normalization） 是什么"},{"content":"PyTorch: An Imperative Style, High-Performance Deep Learning Library  ✅ 论文题目：PyTorch: An Imperative Style, High-Performance Deep Learning Library 发表时间：2019 年 12 月  摘要 Abstract 深度学习框架通常无法同时兼顾可用性（usability）和速度（speed），而 PyTorch 实现了两者兼而有之。\n PyTorch 提供了 Pythonic 编程风格，支持代码作为模型，使调试变得简单，并与其他流行的科学计算库保持一致，同时保持高效并支持 GPU 等硬件加速器。  PyTorch 的每个方面都是由用户完全控制的常规 Python 程序。本篇论文详细介绍了 PyTorch 实现的原则，以及这些原则如何体现在其架构中。\n1 介绍 Introduction 随着人们对深度学习兴趣的增加，机器学习工具出现了爆炸式的增长。许多流行的框架如 Caffe、CNTK、TensorFlow 和 Theano，构建了一个静态的数据流图（static dataflow graph）表示计算，然后重复应用于数据批次。这种方法的缺点是缺乏易用性、调试的方便性和可以表示的计算类型的灵活性。\n深度学习的 dynamic eager execution 相比静态具有更高的价值。最近的一些支持 dynamic 的框架：\n Chainer 的表达性较好，但牺牲了性能。 Torch、DyNet 的性能、速度较快，但表达性、易用性低。  PyTorch 是一个兼顾易用性和性能的 Python 库。它可以通过自动微分和 GPU 加速来执行动态张量计算的即时执行，并且在这样做的同时，还能保持与当前最快的深度学习库相当的性能。\n PyTorch, a Python library that performs immediate execution of dynamic tensor computations with automatic differentiation and GPU acceleration, and does so while maintaining performance comparable to the fastest current libraries for deep learning.\n 2 背景 Background 科学计算的四大趋势对深度学习越来越重要。\n （1）array-based programming （2）automatic differentiation （3）open-source Python ecosystem （4）hardware accelerators.  PyTorch 通过提供一个由 GPU 加速的基于数组的编程模型，并通过集成在 Python 生态系统中的自动差异化来建立这些趋势。\n3 设计原则 Design principles PyTorch 的成功源于将之前的想法编织成兼顾速度和易用性的设计。背后有四个主要原则：\n （1）Be Pythonic：PyTorch 是 Python 生态系统的一员，遵循了普遍建立的设计目标，即保持界面的简单和一致。 （2）Put researchers first：使编写模型、数据加载器和优化器的工作尽可能地简单和高效。机器学习所固有的复杂性由 PyTorch 库在内部处理，并隐藏在直观的 API 后面。 （3）Provide pragmatic performance：PyTorch 提供令人信服的性能，不以牺牲简单性和易用性为代价。 （4）Worse is better：PyTorch 内部实现简单所节省的时间可以用来实现更多的功能，适应新的情况，跟上人工智能领域快速发展的步伐。  4 以可用性为中心的设计 Usability centric design 4.1 Deep learning models are just Python programs PyTorch 放弃了 graph-metaprogramming based 方法，以保留 Python 的 imperative programming model。这种设计由 Chainer 和 Dynet 率先用于模型编写，PyTorch 将其扩展到深度学习工作流的各个方面：定义图层、组成模型、加载数据、运行优化器和并行化训练过程，都使用为通用编程开发的熟悉概念来表达。\n这种解决方案确保了任何新的潜在神经网络架构都可以通过 PyTorch 轻松实现。\n层（Layer）（在现代机器学习中，层实际上应该理解为具有隐式参数的有状态函数）通常表示为 Python 类，其构造函数创建和初始化其参数，其前向方法处理一个输入激活。模型（Model）通常被表示为组成各个层的类。Listing 1 演示了通过合成 PyTorch 提供的功能（如 2d 卷积、矩阵乘法、dropout 和 softmax）来创建整个模型，以对灰度图像进行分类。\n“everything is a just a program” 的理念不仅限于模型，也适用于优化器（optimizers）和数据加载器（data loaders）。Listing 2 演示了 PyTorch 中采用的简单设计实现非常流行的生成式对抗网络。\n4.2 互操作性和可扩展性 Interoperability and extensibility PyTorch 允许与外部库进行双向数据交换。例如，它提供了一种机制，使用 torch.from_numpy() 函数和 .numpy() 张量方法在 NumPy 数组和 PyTorch 张量之间进行转换。类似的功能也可以用来交换使用 DLPack 格式存储的数据。\n这种交换都是在没有任何数据复制的情况下发生的，无论转换的数组有多大，都需要恒定的时间。\n此外，许多关键系统都是可以进行可拓展设计。\n例如，自动微分系统（automatic differentiation system）允许用户添加对自定义可微分函数的支持。为此，用户可以定义一个新的 torch.autograd.Function 子类，实现 forward() 和 backward() 方法，指定函数及其导数。\n同样，可以通过子类torch.utils.dataset 并实现两个方法来添加新的数据集。__getitem__ (索引操作符) 和 __len__ (长度操作符)，工作方式完全由实现者决定。\nDataLoader 类处理符合这个接口的对象，并在数据上提供一个迭代器，该迭代器负责处理洗牌、批处理、并行化和管理 pinned CUDA 内存以提高吞吐量。\n用户可以自由更换 PyTorch 中不符合其项目需求或性能要求的任何组件。\n4.3 自动微分 Automatic differentiation 基于梯度的优化对深度学习至关重要。\nPyTorch 能够自动计算用户指定的模型的梯度，而这些梯度可以是任意的 Python 程序。\nPyTorch 使用了运算符重载的方法，每次执行时都会建立计算函数的表示。在目前的实现中，PyTorch 执行逆向模式自动微分（reverse-mode automatic differentiation），计算标量输出相对于多变量输入的梯度。使用正向模式自动微分（forward-mode automatic differentiation）可以更高效地执行输出多于输入的函数微分，但这种用例在机器学习应用中并不常见。PyTorch 可以很容易地扩展到使用数组级对偶数（dual numbers）来执行正向模式微分。\nPyTorch 另一个有趣且不常见的特性是，它可以通过代码对 Tensors 采用对 Tensor 进行变种来进行微分，这是命令式程序（ imperative programs）的基本构件之一。\n PyTorch 的两个特点 Imperative programming 和 Dynamic Computation Graphs.\n ✅ 5 Performance focused implementation 从 Python 解释器中高效地运行深度学习算法是众所周知的挑战：例如，全局解释器锁（global interpreter lock）有效地确保在任何给定时间内，任何数量的并发线程中只有一个在运行。基于构建静态数据流图（static data-flow graph）的深度学习框架通过将计算的评估推迟到自定义解释器来避开这个问题。\nPyTorch 则以不同的方式解决了这个问题，它仔细优化了执行的每一个方面，同时让用户能够轻松利用额外的优化策略。\n5.1 An efficient C++ core 大部分 PyTorch 是用 C++ 编写的，以实现高性能。核心 libtorch 库实现了张量数据结构、GPU 和 CPU 运算符以及基本的并行基元。它还提供了自动微分系统，包括大多数内置函数的梯度公式。这确保了由核心 PyTorch 运算符组成的函数的导数计算完全在多线程评估器（multithreaded evaluator）中执行，避开了 Python 全局解释器锁（ Python global interpreter lock）。\n这种方法的其他作用：\n 允许快速创建绑定到多个其他语言（NimTorch、hasktorch） 创建了一流的 C++ 绑定和建模库，可以在 Python 不方便的地方使用。  5.2 Separate control and data flow PyTorch 在控制流（即程序分支、循环）和数据流（即张量和对其执行的操作）之间保持严格的分离。\n控制流的解析由 Python 和在主机 CPU 上执行的优化的 C++ 代码处理。\nPyTorch 通过利用 CUDA 流机制，将 CUDA 内核调用排队到 GPU 硬件 FIFO，从而在 GPU 上异步执行运算符。这使得系统可以将 CPU 上的 Python 代码与 GPU 上的张量运算符重叠执行。由于张量运算通常需要大量的时间，这让我们可以让 GPU 饱和，即使在像 Python 这样开销相当大的解释语言中也能达到峰值性能。\n5.3 Custom caching tensor allocator 几乎每一个运算器都必须动态分配一个输出张量来保持其执行结果。因此，优化动态内存分配器的速度至关重要。PyTorch 可以依靠优化的库来处理 CPU 上的这项任务。PyTorch 实现了一个自定义分配器，它可以增量地建立CUDA 内存的缓存，并将其重新分配给以后的分配，而无需进一步使用 CUDA API。\n5.4 Multiprocessing 由于全局解释器锁（GIL），Python 的默认实现不允许并发线程并行执行。为了缓解这个问题，Python 社区建立了一个标准的多处理模块，其中包含了一些实用程序，允许用户轻松地生成子进程并实现基本的进程间通信基元。\nPyTorch 将 Python 多处理模块扩展为 torch.multiprocessing，它是对内置包的一个下拉替换，并自动将发送到其他进程的时序器数据移动到共享内存中，而不是通过通信通道发送。\n这种设计极大地提高了性能，并使进程隔离性变弱，从而形成了更接近于常规线程程序的编程模式。\n5.5 引用计数 Reference counting 用户在设计模型时，往往会在训练过程中利用所有可用的内存，而增加 batch size 是加快这一过程的常用技术。因此，为了提供出色的性能，PyTorch 必须将内存视为一种稀缺资源，需要谨慎管理。\nPyTorch 采用了一种不同的方法：它依靠**引用计数方案来跟踪每个张量的使用次数，并在次数达到零后立即释放底层内存。**请注意，PyTorch 通过与 Python 自己的引用计数机制集成，既跟踪 libtorch 库的内部引用，也跟踪用户在其 Python 代码中的外部引用。这确保了当 tensors 不再需要时，内存会准确地被释放。\n6 评估 Evaluation 将 PyTorch 与其他几个常用的深度学习库进行性能比较。\n6.1 异步数据流 Asynchronous dataflow 首先对 PyTorch 在 GPU 上异步执行数据流的能力进行量化。使用内置的剖析器来检测各种基准，并记录单个训练步骤的执行时间线。\n图 1 显示了一个 ResNet-50 模型的前几个操作的代表性执行时间线。排队工作的主机 CPU 很快就超过了 GPU 上运算符的执行速度。这使得 PyTorch 能够实现几乎完美的设备利用率。在这个例子中，GPU 执行时间是 CPU 调度时间的三倍左右。具体比例取决于主机 CPU 和 GPU 的相对性能，以及每个张量中的元素数量和要在 GPU 上执行的浮点计算的平均算术复杂性。\n6.2 Memory management 如图 2 所示，第一次迭代的行为与后续迭代的行为有很大不同。起初，对 CUDA 内存管理函数（cudaMalloc 和cudaFree）的调用会长时间阻塞 CPU 线程，从而降低 GPU 的利用率，使执行速度大大减慢。在随后的迭代中，随着 PyTorch 缓存内存分配器开始重用之前分配的区域，这种影响就会消失。\n6.3 Benchmarks（基准） 将 PyTorch 与三个流行的基于图的深度学习框架（CNTK、MXNet 和 TensorFlow）、一个定式运行框架（Chainer）和面向生产的平台（PaddlePaddle）进行对比，来全面了解 PyTorch 的单机急切模式性能。\n在所有的基准测试中，PyTorch 的性能都在最快框架的 17% 以内。\n6.4 Adoption（采用） 通过统计自 2017 年 1 月 PyTorch 首次发布以来，各种机器学习工具（包括 Caffe、Chainer、CNTK、Keras、MXNet、PyTorch、TensorFlow 和 Theano）在 arXiv 上被提及的频率，来量化机器学习社区对 PyTorch 的接受程度。\n7 Conclusion and future work PyTorch 将对可用性的关注与对性能的谨慎考虑相结合，已经成为深度学习研究界的热门工具。\n除了继续支持深度学习的最新趋势和进展外，未来还计划继续提高 PyTorch 的速度和可扩展性。\n最值得注意的是，我们正在开发 PyTorch JIT：这是一套工具，可以让 PyTorch 程序在 Python 解释器之外执行，并在那里进一步优化。我们还打算改进对分布式计算的支持，为数据并行提供高效的基元，并为基于远程过程调用的模型并行提供 Pythonic 库。\n","permalink":"http://landodo.github.io/posts/20210130-pytorch-paper-reading-1/","summary":"PyTorch: An Imperative Style, High-Performance Deep Learning Library ✅ 论文题目：PyTorch: An Imperative Style, High-Performance Deep Learning Library 发表时间：2019 年 12 月 摘要 Abstract 深度学习框架通常无法同时兼顾可用性（usabilit","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"content":"计算图和反向传播（Computational Graphs and Backpropagation） 学习资料  （0）《深度学习入门——基于 Python 的理论与实践》  我的读书笔记：2021.01.28 和看小说一样流畅的深度学习基础图书\n （1）Hacker\u0026rsquo;s guide to Neural Networks: http://karpathy.github.io/neuralnets/   （2）Computational graphs: Backpropagation: https://kharshit.github.io/blog/2018/03/09/computational-graphs-backpropagation   （3）(CS231n) Backpropagation: Intuitionschain rule interpretation, real-valued circuits, patterns in gradient flow: https://cs231n.github.io/optimization-2/    （4）Computing Gradient Hung-yi Lee 李宏毅: http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/Graph.pdf\n  （5）CS231n: Convolutional Neural Networks for Visual Recognition https://cs231n.github.io/\n  ","permalink":"http://landodo.github.io/posts/20210129-computational-graphs-and-backpropagation/","summary":"计算图和反向传播（Computational Graphs and Backpropagation） 学习资料 （0）《深度学习入门——基于 Python 的理论与实践》 我的读书","title":"计算图和反向传播（Computational Graphs and Backpropagation）"},{"content":"深度学习-卷积神经网络中文综述 《解析卷积神经网络——深度学习实践手册》是南京大学博士魏秀参写的一本小册子，给很多初学者提供了很多帮助。\n 魏秀参主页：http://www.weixiushen.com/ ✅ 手册下载地址：http://www.weixiushen.com/book/CNN_book.pdf  我是在阅读中科院计算所的王晋东《迁移学习简明手册》时，发现里面提到，并给出了不错的评价。因此，我就抽了一天时间看了一遍。\n总体看一遍下来，我认为应该可以算是一份关于卷积神经网络非常好的综述性手册，涵盖了深度学习中卷积神经网络的方方面面。对我而言，既是复习，又获得了新的知识。\n手册 ≈ 168 页，参考文献 101 篇（参考文献对现在的我而言，是一份优秀的论文阅读清单）。\n接下来阅读第二遍，把第一遍标注的笔记进行整理成 Markdown 的形式。\n笔记的形式大多是摘录的一句/段话，因此不会连贯。\n偶尔也会有自己的注解。\n笔记 前言  Must Know Tips/Tricks in Deep Neural Networks by [Xiu-Shen Wei]: http://www.weixiushen.com/project/CNNTricks/CNNTricks.html  绪论   相比传统机器学习算法仅学得模型单一“任务模块”而言，深度学习除了模型学习，还有特征学习、特征抽象等 任务模块的参与，借助多层任务模块完成最终学习任务，故称其为**“深度”学习**。\n  神经网络的第二次高潮，即二十世纪八十至九十年代的连接主义（Connectionism）。但受限于当时数据获取的瓶颈，神经网络只能在中小规模数据上训练，因此过拟合（Overfitting）极大困扰着神经网络型算法。同时，神经网络 算法的不可解释性令它俨然成为一个“黑盒”，训练模型好比撞运气般，有人无奈的讽刺说它根本不是“科学”（Science）而是一种“艺术”（Art）。\n  2006 年，Hinton 等在 Science 上发表文章 Reducing the dimensionality of data with nerual network 提出：一种称为**“深度置信网络”的神经网络模型可通过逐层预训练（greedy layer-wise pretraining）的方式有效完成模型训练过程。很快，更多的实验结果证实了这一发现，更重要的是除了证明神经网络训练的可行性外，实验结果还表明神经网络模型的预测能力**相比其他传统机器学习算法可谓“鹤立鸡群”。\n  卷积神经网络基础知识   1980 年，日本科学家福岛邦彦（Kunihiko Fukushima）提出的一种层级化的多层人工神经网络 Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position，神经认知模型（Neocongnitron）在后来也被认为是现今卷积神经网络的前身。\n  在福岛邦彦的神经认知模型中，两种最重要的组成单元是“S 型细胞”和“C 型细胞”，两类细胞交替堆叠在一起构成了神经认知网络。其中，S 型细胞用于抽取局部特征（local feature），C 型细胞则用于抽象和容错。\n  S-cells 和 C-cells 与现今卷积神经网络中的**卷积层（convolution layer）和池化层（pooling layer）**一一对应。\n   CNN 的起源知识。\n     深度学习普及之后，人工特征已逐渐被表示学习根据任务需求**自动“学到”**的特征表示所取代。\n   作者在 2014 年在北京与深度学习领域巨头 Yoshua Bengio 共进晚餐，Yoshua Bengio 回答：今后深度学习将取代与人工特征方面的研究。\n 2014 年，那时我正在读高一。     卷积神经网络基本部件   卷积网络中的卷积核参数是通过网络训练学习出的。\n  可以学到类似的横向、纵向边缘滤波器，还可以学到任意角度的边缘滤波器。\n  检测颜色、形状、纹理等等众多基本模式（pattern）的滤波器（卷积核）都可以包含在一个足够复杂的深层卷积神经网络中。\n  通过“组合” 这些滤波器（卷积核）以及随着网络后续操作的进行，基本而一般的模式会逐渐被抽象为具有高层语义的“概念”表示，并以此对应到具体的样本类别。颇有“盲人摸象” 后，将各自结果集大成之意。\n   “盲人摸象”，“集之大成”，这个描述非常妙！\n     池化层实际上是一种降采样（down-sampling）操作。三种功效：\n 特征不变性（feature invariant） 特征降维 一定程度上防止过拟合    ✅ 德国 University of Freiburg 的研究者提出用一种特殊的卷积操作（stride convolutional layer）来代替池化层实现降采样，进而构建一个个含卷积操作的网络，其实验结果显示这种改造的网络可以达到、甚至超过传统卷积神经网络（卷积层汇合层交替）的分类精度。（Striving for Simplicity: The All Convolutional Net, arXiv: 1412.6806）\n  激活函数（Activation Function）的引入为的是增加整个网络的表达能力（即非线性）。否则， 若干线性操作层的堆叠仍然只能起到线性映射的作用，无法形成复杂的函数。\n   我看过有一篇论文“Activation Functions: Comparison of Trends in Practice and Research for Deep Learning, arXiv: 1811.03378”，里面列出了 21 种激活函数。\n   为了避免梯度饱和效应的发生，Nair 和 Hinton 于 2010 年将修正线性单元（Rectified Linear Unit, ReLU）引入神经网络。 论文：Rectified Linear Units Improve Restricted Boltzmann Machines\n    全连接层（fully connected layers）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的特征表示映射到样本的标记空间的作用。\n  卷积神经网络经典结构   小卷积核（如 3 × 3）通过多层叠加可取得与大卷积核（如 7 × 7）同等规模的感受野，此外采用小卷积核同时可带来其余两个优势：第一，由于小卷积核需多层叠加，加深了网络深度进而增强了网络容量（model capacity）和复杂度（model complexity）；第二，增强网络容量的同时减少了参数个数。\n  目前已有不少研究工作，通过改造现有卷积操作试图扩大原有卷积核在前层的感受野大小，或使原始感受野不再是矩形区域而是更自由可变的形状，以提升模型预测能力。\n   扩张卷积：Multi-Scale Context Aggregation by Dilated Convolutions, arXiv: 1511.07122\n可变卷积：Deformable Convolutional Networks, arXiv: 1703.06211\n     是同一卷积核在不同原图中响应的区域可谓大相径庭；\n 对于某个模式，如鸟的躯干，会有不同卷积核（其实就是神经元）产生响应； 同时对于某个卷积核（神经元），会在不同模式上产生响应； 神经网络响应的区域多呈现“稀疏“特性，即响应区域集中且占原图比例较小。    深度特征的层次性已成为深度学习领域的一个共识。\n  AlexNet 在整个卷积神经网络甚至连接主义机器学习发展进程中占据里程碑式的地位，一些训练的引入使得“不可为”变“可为”，甚至是“大有可为”。\n ReLU LRN data augmentation dropout 此后的卷积神经网络大体都是遵循这一网络构建的基本思路。    NIN 采用了复杂度更高的多层感知机作为层间映射形式，一方面提供了网络层间映射的一种新可能；另一方面增加了网络卷积层的非线性能力，使得上层特征可有更多复杂性与可能性的映射到下层，这样的想法也被后期出现的残差网络和 Inception 等网络模型所借鉴。\n   Network In Network, arXiv: 1312.4400\nDeep Residual Learning for Image Recognition, arXiv: 1512.03385\nGoing Deeper with convolution, arXiv: 14094842\n   NIN 网络模型的另一个重大突破是摒弃了全连接层作为分类层的传统，转而改用全局平均池化操作（global average pooling）。\n    如果一个浅层神经网络可以被训练优化求解到某一个很好的解，那么它对应的深层网络至少也可以，而不是更差。这一现象（degradation problem）在一段时间内困扰着更深层卷积神经网络的设计、训练和应用。\n ResNet、Highway Network    卷积神经网络的压缩   许多研究表明，深度神经网络面临着严峻的过参数化（over-parameterization）——模型内部参数存在着巨大的冗余。\n  参数冗余是有意义的。深度神经网络面临的是一个极其复杂的非凸优化问题，对于现有的基于梯度下降的优化算法而言，这种参数上的冗余保证了网络能够收敛到一个比较好的最优值 。\n   缓解神经网络遇到了局部最优问题。\n   需要一定冗余度的参数数量来保证模型的可塑性与“容量”（capacity）。\n  Predicting Parameters in Deep Learning, arXiv:1306.0543 发现，只给定很小一部分的参数子集（约全部参数量的 5%），便能完整地重构出剩余的参数，这揭示了模型压缩的可行性。\n  奇异值分解（Singular Value Decomposition）来重构全连接层的权重。\n    数据驱动的剪枝：根据输出中每一个通道（channel）的稀疏度来判断相应滤波器的重要程度。\n   Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures. arXiv: 1607.03250\n     知识蒸馏：其实是迁移学习（Transfer Learning）的一种，其最终目的是将一个庞大而复杂的模型所学到的知识，通过一定的技术手段迁移到精简的小模型上，使得小模型（学生）能够获得与大模型（老师）相近的性能。\n  SqueezeNet 在 ImageNet 上能够达到 AlexNet 的分类精度，而其模型大小仅仅为 4.8MB。\n   SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u0026lt;0.5MB model size\nSqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n     数据扩增  注明的 AlexNet 提出的同时，Krizhevsky 等人还提出了一种名为”Fancy PCA“的数据扩增方法。  网络参数初始化   Xavier 初始化方法仍有不甚完美之处，即该方法并未考虑非线性映射函数对输入 $s$ 的影响。因为使用如 ReLU 函数等非线性映射函数后，输出数据的期望往往不再为 0。\n He 等人对此提出改进，将非线性映射造成的影响考虑进参数初始化中。    激活参数  kaggle 上举办的 2015 年”国家数据科学大赛“（national data science bowl）——浮游动物的图像分类，随机化 ReLU 首次提出并一举夺冠。  网络正则化   正则化是机器学习中通过显式的控制模型复杂度来避免模型过拟合、确保泛化能力的一种有效方式。\n  $ \\ell_1$ 正则化起到使参数更稀疏的作用。稀疏化的结果使优化后的参数一部分为 0 。另一部分非零值实值的参数可起到选择重要参数或特征维度的作用。\n  $\\ell_1, \\ell_2$ 也可以联合使用，这被称为”Elastic 网络正则化“。Regularization and variable selection via the elastic net.\n   Elastic 初始化的论文在 arXiv 上没有。\n     超参数和网络训练   Google 提出了批规范化操作（batch normalization, BN）。\n Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv:1502.03167 Google 的研究人员发现可通过 BN 来规范化某些层或所有层的输入，从而可以固定每层输入信号的均值与方差。这样一来，即使网络模型较深层的响应或梯度很小，也可通过 BN 的规范化作用将其的尺度变大，以此便可解决深层网络训练很可能带来的“梯度弥散” 问题。 在卷积神经网络中，BN 一般应作用在非线性映射函数前。    ✅ 基于动量的随机梯度下降：通过积累前几轮的“动量”信息辅助参数更新。基于动量的随机梯度下降法除了可以抑制振荡，还可在网络训练中后期趋于收 敛、网络参数在局部最小值附近来回震荡时帮助其跳出局部限制，找到更优的网络参数。\n   关于模型优化算法，可以看一篇综述。\nAn overview of gradient descent optimization algorithms\nhttps://arxiv.org/pdf/1609.04747.pdf\n     不平衡样本的处理  不平衡（imbalance）的训练样本会导致训练模型侧重样本数目较多的类别，而“轻视”样本数目较少类别，这样模型在测试数据上的泛化能力就会受到影响。  数据层面处理方法多借助数据采样法()使整体训练集样本趋于平衡。 算法层面的处理方法：注意力机制，通过优化目标函数就可以调整模型在小样本上的“注意力”。算法层面处理不平衡样本问题的方法也多从代价敏感（cost-sensitive）角度出发。    模型集成方法   如 ImageNet、KDD Cup、Kaggle 等竞赛的冠军做法，或简单或复杂其最后一步必然是集成学习。\n   ”众人拾柴火焰高“，”锦上添花“\n     多层特征融合操作时可直接将不同层网络特征级联（concatenate）。而对于特征融合应选取哪些网络层，一个实践经验是：最好使用靠近目标函数的几层卷积特征，因为愈深层特征包含的高层语义性愈强、分辨能力也愈强;相 反，网络较浅层的特征较普适，用于特征融合很可能起不到作用有时甚至会起 到相反作用。\n  ✅ 网络“快照”集成法（snapshot ensemble）利用了网络解空间中的这些局部最优解来对单个网络做模型集成。通过循环调整网络学习率（cyclic learning rate schedul）可使网络依次收敛到不同的局部最优解。\n 利用余弦函数 $cos(\\cdot)$ 的循环特性来循环更新网络学习率，将学习率从 0.1 随 $t$ 的增长逐渐减缓到 0，之后将学习率重新放大从而跳出该局部最优解。     关于模型集成也是可以看一篇综述。\n   深度学习开源工具简介    Torch 和 PyTorch 是同一个东西吗？我一直在 import torch ，所使用的是什么？\n  PyTorch 为 Torch 提供了更便利的接口。    ","permalink":"http://landodo.github.io/posts/202101227-cnn-xiushen-wei/","summary":"深度学习-卷积神经网络中文综述 《解析卷积神经网络——深度学习实践手册》是南京大学博士魏秀参写的一本小册子，给很多初学者提供了很多帮助。 魏秀参","title":"深度学习-卷积神经网络中文综述"},{"content":"SENet 和它的孪生兄弟 SKNet ✅ 论文地址：\n Squeeze-and-Excitation Networks: https://arxiv.org/pdf/1709.01507.pdf Selective Kernel Networks: https://arxiv.org/pdf/1903.06586.pdf  ✅ 论文发表时间（arXiv V1）\n SENet：2017 年 9 月 5 日 SKNet：2019 年 3 月 15 日  相关的论文 （1）Inception 系列（2014 年~2016 年）：Inception 结构中嵌入了多尺度信息，聚合多种不同感受野上的特征来获得性能增益。\n Inception V1 (GoogLeNet): 11 Sep 2014 Inception V2 (Batch Normalization): 11 Feb 2015 Inception V3: 2 Dec 2015 Inception V4: 23 Feb 2016 Xception: 7 Oct 2016  （2）ResNet （10 Dec 2015）\n（3）ResNeXt（16 Nov 2016）\n（4）Inside-Outside Network（14 Dec 2015）：网络中考虑了空间中的上下文信息。\n（5）Spatial Transform Network（5 Jun 2015）：Attention 机制引入到空间维度。\n   Dynamic Capacity Network（24 Nov 2015）    （6）💢 SENet（5 Sep 2017）：通道注意力\n（7）CBAM（17 Jul 2018）：空间注意力+通道注意力相结合\n（8）💢SKNet（15 Mar 2019）\nSENet  一作：胡杰，关于 SENet 中文介绍：\n Momenta 详解 ImageNet 2017 夺冠架构 SENet https://www.sohu.com/a/161633191_465975   🌀通道间的特征都是平等的吗？SENet 给出了这个问题的答案。\n论文的主要工作是：考虑特征通道之间的关系，提出了 Squeeze-and-Excitation Networks（简称 SENet）。显式地建模特征通道之间的相互依赖关系，通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。\nSE block 如下 Fig. 1 所示。\nSQUEEZE-AND-EXCITATION BLOCKS Squeeze 和 Excitation 是两个非常关键的操作。\n给定一个输入 $X$，$X \\in \\mathbb{R}^{C\u0026rsquo; \\times H\u0026rsquo; \\times W\u0026rsquo;}$，通过一系列卷积等一般变换 $F_{tr}$ 后，得到一个 $U \\in \\mathbb{R}^{C \\times H \\times W}$ 的特征图。\n接下来通过一个 Squeeze and Excitation block ，三个操作来重标定前面得到的特征。\n（1）Squeeze: $F_{sq}(\\cdot)$\n首先是 Squeeze 操作，顺着空间维度来进行特征压缩，将每个二维的特征通道变成一个实数，这个实数某种程度上具有全局的感受野，并且输出的维度和输入的特征通道数相匹配。\n即：对 $C \\times H \\times W$ 的特征图进行 global average pooling，得到 $1 \\times 1 \\times C$ 的特征图。\n$$z_c = F_{sq}(u_c) = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} u_c{(i, j)}$$\n（2）Excitation: $F_{ex}(\\cdot , W)$\n通过参数 W 来为每个特征通道生成权重，其中参数 W 被学习用来显式地建模特征通道间的相关性。\n即：使用一个全连接层神经网络，对 Squeeze 之后的结果进行一个非线性变换。\n$$s = F_{ex}(z, W) = \\sigma(g(z, W)) = \\sigma(W_2 \\delta(W_1 z)) $$\n（3）Scale\n最后是一个 Reweight 的操作，将 Excitation 的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。\n$$\\tilde{x} = F_{scale}(u_c, s_c) = s_c u_c$$\nSE Block 实现细节 使用 global average pooling 作为 Squeeze 操作；\n紧接着两个 Fully Connected 层组成一个 Bottleneck 结构去建模通道间的相关性，并输出和输入特征同样数目的权重。\n首先将特征维度降低到输入的 1/16，（降低计算量，16 是实践得到的较好的超参数）\n然后经过 ReLu 激活后再通过一个 Fully Connected 层升回到原来的维度。（增加非线性）\n通过一个 Sigmoid 函数获得 0~1 之间归一化的权重。\n最后通过一个 Scale 的操作来将归一化后的权重加权到每个通道的特征上。\nSE Block 可以嵌入到现在几乎所有的网络结构中。\n实例 Instantiations 通过在原始网络结构的 building block 单元中嵌入 SE 模块，可以获得不同种类的 SENet。如 SE-BN-Inception、SE-ResNet、SE-ReNeXt、SE-Inception-ResNet-v2 等等。\nSENet 的参数量和计算量情况 SENet 额外的模型参数都存在于 Bottleneck 设计的两个 Fully Connected 中。\n以 SE-ResNet-50 和 ResNet-50 为例，从理论上，SE Block 增长的额外计算量仅仅不到 1%。\nSENet 的表现 ResNet-50、ResNet-101、ResNet-152 和嵌入 SE 模型的结果。SE-ResNets 在各种深度上都远远超过了其对应的没有 SE 的结构版本的精度，这说明无论网络的深度如何，SE 模块都能够给网络带来性能上的增益。\nSE 模块嵌入到 ResNeXt、BN-Inception、Inception-ResNet-v2 上均获得了不菲的增益效果，加入了 SE 模块的网络收敛到更低的错误率上。\n其他（CIFAR-10、CIFAR-100、Places365、COCO、ImageNet）：\n最后，在 ILSVRC 2017 竞赛中，SENet 在测试集上获得了 2.251% Top-5 错误率。对比于去年第一名的结果 2.991%，获得了将近 25% 的精度提升。\n 2012~2017： ILSVRC 2017 竞赛冠军🏆：\n 2012，AlexNet：top-5: 15.32% 2013，Clarifai，top-5: 11.20% 2014，GoogleNet v1，top-5: 6.67% 2015，ResNet，top-5: 3.57% 2016，Trimps-Soushen（公安三所），top-5: 2.99% 2017，SENet，top-5: 2.25%   ❌ SKNet  一作：李翔，在知乎谈 SKNet：\n 「SKNet——SENet 孪生兄弟篇」：https://zhuanlan.zhihu.com/p/59690223   SKNet 我留下周进行汇报（1 月 29 日）。\n实验 对 ResNet50、SENet50 和 SKNet 50 进行简单的比较。\n数据集采用 CIFAR-10。\n除了 model 不同，三者其他训练时的参数都是一致的。\n训练时保存 checkpoint，这样调参就不用每次都从 0 开始训练。有一个经过预训练的模型能减少训练需要的时间。\nif args.resume:  # Load checkpoint.  print(\u0026#39;==\u0026gt; Resuming from checkpoint..\u0026#39;)  assert os.path.isdir(\u0026#39;checkpoint\u0026#39;), \u0026#39;Error: no checkpoint directory found!\u0026#39;  checkpoint = torch.load(\u0026#39;./checkpoint/ckpt.pth\u0026#39;)  net.load_state_dict(checkpoint[\u0026#39;net\u0026#39;])  best_acc = checkpoint[\u0026#39;acc\u0026#39;]  start_epoch = checkpoint[\u0026#39;epoch\u0026#39;] 总训练 200 epoch，每两个 epoch 保存一次 checkpoint，使用 matplotlib 绘制 rain_acc 和 test_acc 曲线。\nResNet50 训练 200 个 Epoch。下图为 ResNet50 训练结束，Test Acc 达到 95.41%。\n每 2 个 epoch 保存一次 checkpoint，用于绘图。（忘记修改了，其实不需要保存 net.state_dict 的，非常耗空间。）\n 我把 loss 忘记保存了🌚，loss 曲线也很重要。我只保存了 acc 和 epoch。\n 不保存 state_dict ，只保存 loss、epoch 和 acc。\n1. ResNet50   参考代码链接：https://github.com/kuangliu/pytorch-cifar   \u0026#39;\u0026#39;\u0026#39;ResNet in PyTorch. For Pre-activation ResNet, see \u0026#39;preact_resnet.py\u0026#39;. Reference: [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun Deep Residual Learning for Image Recognition. arXiv:1512.03385 \u0026#39;\u0026#39;\u0026#39; import torch import torch.nn as nn import torch.nn.functional as F   class BasicBlock(nn.Module):  expansion = 1   def __init__(self, in_planes, planes, stride=1):  super(BasicBlock, self).__init__()  self.conv1 = nn.Conv2d(  in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)  self.bn1 = nn.BatchNorm2d(planes)  self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,  stride=1, padding=1, bias=False)  self.bn2 = nn.BatchNorm2d(planes)   self.shortcut = nn.Sequential()  if stride != 1 or in_planes != self.expansion*planes:  self.shortcut = nn.Sequential(  nn.Conv2d(in_planes, self.expansion*planes,  kernel_size=1, stride=stride, bias=False),  nn.BatchNorm2d(self.expansion*planes)  )   def forward(self, x):  out = F.relu(self.bn1(self.conv1(x)))  out = self.bn2(self.conv2(out))  out += self.shortcut(x)  out = F.relu(out)  return out   class Bottleneck(nn.Module):  expansion = 4   def __init__(self, in_planes, planes, stride=1):  super(Bottleneck, self).__init__()  self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)  self.bn1 = nn.BatchNorm2d(planes)  self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,  stride=stride, padding=1, bias=False)  self.bn2 = nn.BatchNorm2d(planes)  self.conv3 = nn.Conv2d(planes, self.expansion *  planes, kernel_size=1, bias=False)  self.bn3 = nn.BatchNorm2d(self.expansion*planes)   self.shortcut = nn.Sequential()  if stride != 1 or in_planes != self.expansion*planes:  self.shortcut = nn.Sequential(  nn.Conv2d(in_planes, self.expansion*planes,  kernel_size=1, stride=stride, bias=False),  nn.BatchNorm2d(self.expansion*planes)  )   def forward(self, x):  out = F.relu(self.bn1(self.conv1(x)))  out = F.relu(self.bn2(self.conv2(out)))  out = self.bn3(self.conv3(out))  out += self.shortcut(x)  out = F.relu(out)  return out   class ResNet(nn.Module):  def __init__(self, block, num_blocks, num_classes=10):  super(ResNet, self).__init__()  self.in_planes = 64   self.conv1 = nn.Conv2d(3, 64, kernel_size=3,  stride=1, padding=1, bias=False)  self.bn1 = nn.BatchNorm2d(64)  self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)  self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)  self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)  self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)  self.linear = nn.Linear(512*block.expansion, num_classes)   def _make_layer(self, block, planes, num_blocks, stride):  strides = [stride] + [1]*(num_blocks-1)  layers = []  for stride in strides:  layers.append(block(self.in_planes, planes, stride))  self.in_planes = planes * block.expansion  return nn.Sequential(*layers)   def forward(self, x):  out = F.relu(self.bn1(self.conv1(x)))  out = self.layer1(out)  out = self.layer2(out)  out = self.layer3(out)  out = self.layer4(out)  out = F.avg_pool2d(out, 4)  out = out.view(out.size(0), -1)  out = self.linear(out)  return out   def ResNet18():  return ResNet(BasicBlock, [2, 2, 2, 2])  def ResNet34():  return ResNet(BasicBlock, [3, 4, 6, 3])  def ResNet50():  return ResNet(Bottleneck, [3, 4, 6, 3])  def ResNet101():  return ResNet(Bottleneck, [3, 4, 23, 3])  def ResNet152():  return ResNet(Bottleneck, [3, 8, 36, 3]) 2. SENet50 基于 ResNet50，加入 SE block 就得到了 SENet50。\nimport torch import torch.nn as nn import torch.nn.functional as F   class BasicBlock(nn.Module):   expansion = 4   def __init__(self, in_planes, planes, stride=1):  super(BasicBlock, self).__init__()  self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)  self.bn1 = nn.BatchNorm2d(planes)  self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,stride=stride, padding=1, bias=False)  self.bn2 = nn.BatchNorm2d(planes)  self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)  self.bn3 = nn.BatchNorm2d(self.expansion*planes)   self.shortcut = nn.Sequential()  if stride != 1 or in_planes != self.expansion*planes:  self.shortcut = nn.Sequential(  nn.Conv2d(in_planes, self.expansion*planes,  kernel_size=1, stride=stride, bias=False),  nn.BatchNorm2d(self.expansion*planes)  )   # SE layers  self.fc1 = nn.Conv2d(self.expansion*planes, self.expansion*planes//16, kernel_size=1) # Use nn.Conv2d instead of nn.Linear  self.fc2 = nn.Conv2d(self.expansion*planes//16, self.expansion*planes, kernel_size=1)   def forward(self, x):  out = F.relu(self.bn1(self.conv1(x)))  out = self.bn2(self.conv2(out))  out = self.bn3(self.conv3(out))   # Squeeze  w = F.avg_pool2d(out, out.size(2))  w = F.relu(self.fc1(w))  w = F.sigmoid(self.fc2(w))  # Excitation  out = out * w # New broadcasting feature from v0.2!   out += self.shortcut(x)  out = F.relu(out)  return out  class SENet(nn.Module):  def __init__(self, block, num_blocks, num_classes=10):  super(SENet, self).__init__()  self.in_planes = 64   self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  self.bn1 = nn.BatchNorm2d(64)  self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)  self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)  self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)  self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)  self.linear = nn.Linear(512*block.expansion, num_classes)   def _make_layer(self, block, planes, num_blocks, stride):  strides = [stride] + [1]*(num_blocks-1)  layers = []  for stride in strides:  layers.append(block(self.in_planes, planes, stride))  self.in_planes = planes * block.expansion  return nn.Sequential(*layers)   def forward(self, x):  out = F.relu(self.bn1(self.conv1(x)))  out = self.layer1(out)  out = self.layer2(out)  out = self.layer3(out)  out = self.layer4(out)  out = F.avg_pool2d(out, 4)  out = out.view(out.size(0), -1)  out = self.linear(out)  return out   def SENet50():  return SENet(BasicBlock, [3,4,6,3]) SENet50 相比于 ResNet50，理论上计算量增加不到 1%，但是我实际训练时，耗时加倍（ ≈ 一天一夜）。\n3. SKNet50  SKNet 的实现参考：https://github.com/developer0hye/SKNet-PyTorch/blob/master/sknet.py\n 绘制 ResNet50、SENet50、SKNet50 的 Acc 曲线 四个网络，各 200 各 Epoch 真的很耗时间。\n# 获取 acc 随 epoch 增加的值 total_epoch = 200  resnet_test_acc = [] resnet_train_acc = [] for i in range(0, total_epoch, 2):  checkpoint = torch.load(resnet_path+\u0026#34;\\\\train_ckpt_epoch_%s.pth\u0026#34; % str(i))  acc = checkpoint[\u0026#39;acc_train\u0026#39;]  resnet_train_acc.append(acc)   checkpoint = torch.load(resnet_path+\u0026#34;\\\\test_ckpt_epoch_%s.pth\u0026#34; % str(i))  acc = checkpoint[\u0026#39;acc_test\u0026#39;]  resnet_test_acc.append(acc) （1）ResNet50：Acc 随 epoch 变换曲线\nResNet 表现得那么好，倒是让我感觉很奇怪。ResNet 论文中，在 CIFAR-10 数据集上，测试得到 ResNet44 的表现为 92.83%，ReNet56 的表现为 93.03%。\n如上实现的 ResNet50 在测试集上的表现竟然达到了 95.45%。\n（2）SENet50\n SENet18  （3）SKNet50\n整合到一张图片上，方便直观的进行比较。\n从理论上分析，各网络的表现情况应该是：\nResNet50 \u0026lt; SENet50 \u0026lt; SKNet50.\n但是我复现的实践结果为： ResNet50(95.45%) \u0026gt; SENet50(95.05%) \u0026gt; SKNet50(89.81)。\n问题出现在哪里？\n参数调优 进行一系列的参数调优，目标是实现 ResNet50 \u0026lt; SENet50 \u0026lt; SKNet50.\n待解决的问题：\n （1）ResNet50 表现得那么好，有问题吗？ （2）SENet18 竟然优于 SENet50，这个很有问题！   （3）SKNet50 竟然没有上 90%，这个很有问题！下周再解决吧。  ","permalink":"http://landodo.github.io/posts/20210122-senet-sknet/","summary":"SENet 和它的孪生兄弟 SKNet ✅ 论文地址： Squeeze-and-Excitation Networks: https://arxiv.org/pdf/1709.01507.pdf Selective Kernel Networks: https://arxiv.org/pdf/1903.06586.pdf ✅ 论文发表时间（arXiv V1） SENet：2017 年 9 月 5 日 SKNet：2019 年 3 月 15 日 相关","title":"SENet 和它的孪生兄弟 SKNet"},{"content":"论文题目 “Squeeze-and-Excitation Networks” ✅ 论文地址：https://arxiv.org/abs/1709.01507\n✅ 发表时间：2017\nABSTRACT 卷积是 CNNs 的核心，它使网络能够通过融合每层局部感受野内的空间（spatial）和通道（channel-wise）信息来构建特征。\n关于空间关系已经有了广泛的研究。\n本篇论文的重点是通道之间的关系，提出了一种称为 “SE block” 的架构单元，它通过明确模拟通道之间的相互依赖关系，自适应地重新校准通道方面的特征响应。\nSqueeze-and-Excitation (SE)：挤压和激励。\nSENet 在 2017 年的 ILSVRC 分类任务上获得了第一名，Top-5 误差降低到 2.251%。\n1 INTRODUCTION Inception 系列架构（Going deeper with convolutions. Batch Normalization: Accelerating deep network training by reducing internal covariate shift），将多尺度过程纳入网络模块，以实现性能的提升。\n这篇论文研究的是：\n 通道之间的关系（the relationship between channels）。 引入了 “SE 块”。 并提出了一种机制，允许网络执行特征重新校准。通过这种机制，网络可以学习使用全局信息，有选择地强调信息性特征，并抑制不太有用的特征。  SE 块如下图 1 所示。\n变化 $F_{tr}$ 将输入 $X$ 映射为 $U$，$U \\in ℝ^{H \\times W \\times C}$。\nU 首先通过一个叫做 squeeze 操作，之后是一个 excitation 操作。\nexcitation（激励）后并产生一个每通道调制权重的集合。这些权重被应用于特征图 U，以产生 SE 块的输出，它可以直接输入到网络的后续层。\n通过 SE 块的堆叠可以构建一个 SENets，SE 块在整个网络中不同深度所执行的角色是不同的。\nSE 块还可以应用于现有的 CNN 结构，通过使用 SE 块进行替换，可以有效地提高性能。\nSENets 在 ILSVRC 2017 分类竞赛中排名第一。最佳模型在测试集上的 top-5 误差率为 2.251%，比上一年的冠军作品（top-5 误差率 = 2.991%）相比，大约有 25% 的相对改善。\n 2016 年的 ILSVRC，Trimps-Soushen（公安部三所） 以 2.99% 的 Top-5 分类误差率和 7.71% 的定位误差率赢得了 ImageNet 分类任务的胜利。该团队使用了分类模型的集成（即 Inception、Inception-ResNet、ResNet 和宽度残差网络模块的平均结果）和基于标注的定位模型 Faster R-CNN 来完成任务。\nImageNet 历年冠军和相关 CNN 模型\n   年 网络 / 队名 val top-1 val top-5 test top-5 备注     2012 AlexNet 38.1% 16.4% 16.42% 5 CNNs   2012 AlexNet 36.7% 15.4% 15.32% 7CNNs。用了 2011 年的数据   2013 OverFeat   14.18% 7 fast models   2013 OverFeat   13.6% 赛后。7 big models   2013 ZFNet   13.51% ZFNet 论文上的结果是 14.8   2013 Clarifai   11.74%    2013 Clarifai   11.20% 用了 2011 年的数据   2014 VGG   7.32% 7 nets, dense eval   2014 VGG（亚军） 23.7% 6.8% 6.8% 赛后。2 nets   2014 GoogleNet v1   6.67% 7 nets, 144 crops    GoogleNet v2 20.1% 4.9% 4.82% 赛后。6 nets, 144 crops    GoogleNet v3 17.2% 3.58%  赛后。4 nets, 144 crops    GoogleNet v4 16.5% 3.1% 3.08% 赛后。v4+Inception-Res-v2   2015 ResNet   3.57% 6 models   2016 Trimps-Soushen   2.99% 公安三所   2016 ResNeXt（亚军）   3.03% 加州大学圣地亚哥分校   2017 SENet   2.25% Momenta 与牛津大学     2 RELATED WORK  更深的网络架构 Deeper architectures.  VGG、Inception Models、Batch Normalization (BN)、ResNets、Highway networks. 分组卷积 Grouped convolutions、多分枝卷积 multi-branch convolutions、跨通道的相关性 cross-channel correlations.   算法架构搜索 Algorithmic Architecture Search.  evolutionary methods：用进化方法进行网络拓扑结构搜索 Lamarckian inheritance、differentiable architecture search：减少了计算负担   注意力和门控机制 Attention and gating mechanisms.  注意力可以被解释为一种将可用计算资源的分配偏向于信号中信息量最大的部分的手段 提出的 SE 块包括一个轻量级的门控机制，该机制专注于通过以一种计算效率高的方式建模通道关系（channel-wise relationships）来增强网络的表示能力。    3 SQUEEZE-AND-EXCITATION BLOCKS Squeeze-and-Excitation 块是建立在输入 $X \\in \\mathbb{R}^{H\u0026rsquo; \\times W\u0026rsquo; \\times C\u0026rsquo;}$ 映射到特征图 $U \\in \\mathbb{R}^{H \\times W \\times C}$ 的变化 $F_{tr}$ 上的计算单元。\n$$u_c = v_c * X = \\sum_{s=1}^{C\u0026rsquo;} v_c^s * X^s \\tag{1}$$\n $F_{tr}$ 为卷积算子 $V = [v_1, v_2, \u0026hellip;, v_C]$ 表示学习到的卷积核的集合 $U = [u_1, u_2, \u0026hellip;, u_C]$ 为输出 $X = [x^1, x^2, \u0026hellip;, x^{C\u0026rsquo;}]$ 为输入 $*$ 表示卷积 $v_c = [v_c^1, v_c^2, \u0026hellip;, v_c^{C\u0026rsquo;}]$，指的是第 c 个卷积核的参数 $u_c \\in \\mathbb{R}^{H \\times W}$ $v_c^s$ 是一个二维空间核，代表 $v_c$ 的单通道，作用于 X 的相应通道。  我们期望卷积特征的学习能够通过明确地模拟通道相互依赖关系来增强，这样网络就能够提高其对信息特征的敏感性，而这些信息特征可以被后续的变换所利用。因此，希望为它提供对全局信息的访问，并在挤压 S 和激励 E 两个步骤中重新校准卷积核响应，然后再将它们输入到下一次变换中。\n3.1 Squeeze: Global Information Embedding 为了解决通道依赖性问题，考虑输出特征中对每个通道的信号。每一个学习的卷积核都是以局部的感受野来操作的，因此，变换输出 U 的每个单元都无法利用感受野之外的上下文信息。\n本论文提供通过使用全局平均池来生成通道方面的统计数据，将全局空间信息挤压到通道描述符中，进而缓解了如上问题。\n通过挤压 U 的空间维度 $H \\times W$ 生成统计量 $z_c$。\n$$z_c = F_{sq}(u_c) = \\frac{1}{H \\times W} \\sum_{i=1}^H \\sum_{j=1}^W u_c(i, j) \\tag{2}$$\n $z \\in \\mathbb{R}^{C}$  3.2 Excitation: Adaptive Recalibration Excitation 操作的目的是完全捕捉通道方面的依赖性。\n这个函数要满足两个要求：\n （1）能够学习通道之间的非线性交互 （2）ensure that multiple channels are allowed to be emphasised  $$s = F_{ex} (z, W) = \\sigma (g(z, W)) = \\sigma(W_2 \\sigma(W_{1}z)) \\tag{3}$$\n $\\sigma$ 是 ReLU 激活函数 $W_1 \\in \\mathbb{R}^{\\frac{C}{r} \\times C}$ $W_2 \\in \\mathbb{R}^{C \\times \\frac{C}{r}}$ $r$ 是一个叫做降低比的参数。后面会详细介绍  3.3 实例化 Instantiations 通过在每次卷积后的非线性之后插入 SE 块，SE 块可以被集成到标准的架构中。\n如下图 2、3，在 Inception 网络和 ResNet 加入 SE 块。\n对于 SENet 架构的具体实例，表 1 给出了 SE-ResNet-50 和 SE-ResNeXt-50 的详细描述。\n4 MODEL AND COMPUTATIONAL COMPLEXITY 对于 SENets 系列网络，需要在提高性能和增加模型复杂度之间进行的权衡。\n以 ResNet-50 和 SE-ResNet-50 为例。\n每个 SE 块在 squeeze 阶段使用一个全局平均池化操作，在 excitation 阶段使用两个 FC 层。\n将参数 $r$ 设置为 16，对于 224 *224 像素的输入图像。\n ResNet-50 一次前向传播约为 3.86 GFLOPs。 SE-ResNet-50 需要约 3.87 GFLOPs，相当于比原来的 ResNet-50 相对增加了0.26%。  SE-ResNet-50 的计算负担略微增加，但其精度却超过了 ResNet-50。实践证明，要达到与 SE-ResNet-50 相同的精度，需要更深的 ResNet-101 网络。\n对 GPU 上计算时间的基准测试：\n 对于 224*224 像素的输入图像，ResNet-50 需要 164 毫秒，而 SE-ResNet-50 则为 167 毫秒  SE 块所引入的额外参数只是两个 FC 层的权重参数：\n SE-ResNet-50 在 ResNet-50 所需的约 2500 万个参数之外，又引入了约 250 万个额外参数，相当于增加了约 10%.  5 EXPERIMENTS 5.1 Image Classification  数据集：ImageNet 2012 数据增强 优化方法：SGD，动量为 0.9，batch size = 1024。初始学习率设置为 0.6，每 30 个EPOCH 降低 10 倍 总共训练 100 EPOCH reduction ratio $r=16$  网络深度 Network depth.\n Table 2 中报告结果可以观察到，SE 块在不同深度的情况下始终如一地提高了性能。  与现代架构集成 Integration with modern architectures.\n 研究将 SE 块与另外两个最先进的架构：Inception-ResNet-v2 和 ResNeXt 整合的效果。即 Inception-ResNet-v2/ResNeXt **V.S. ** SE-Inception-ResNet-v2/SE-ResNeXt 结果如 Table 2 所示。 可以看到在这两种架构中引入 SE 块所引起的显著性能改进。 SE-ResNeXt-50 的 top-5 误差为 5.49%，优于其直接对应的 ResNeXt-50（5.90% top-5误差）以及更深的 ResNeXt-101（5.57% top-5 误差) Table 2 还对 VGG-16 和 BN-Inception 架构进行实验，结果与残差网络进行，SE 块在非残差设置上带来了性能上的改进。  图 4 中描述了 baseline 架构和各自的 SE 对应的运行的训练曲线。可以看到，引入 SE 块的网络在整个优化过程中有了稳定的改进。\nMobile setting.\n 考虑 MobileNet 和 ShuffleNet。  SE 块在计算成本增加最小的情况下，以较大的幅度提高了精度。  Additional datasets.\n CIFAR-10 和 CIFAR-100   5.2 Scene Classification 在 Places365-Challenge 数据集上进行了场景分类的实验，实验结果如 TABLE 6.\n5.3 Object Detection on COCO 5.4 ILSVRC 2017 Classification Competition 通过将 SE 块与修改后的 ResNeXt 集成，构建了一个额外的模型 SENet-154。\nTABLE 8 中使用标准裁剪（$224 \\times 224、$$320 \\times 320$）将该模型与之前在 ImageNet 验证集上的工作进行比较。\nTABLE 9 中列出了目前所知的在 ImageNet 数据集的最好结果。\n","permalink":"http://landodo.github.io/posts/20210112-senet/","summary":"论文题目 “Squeeze-and-Excitation Networks” ✅ 论文地址：https://arxiv.org/abs/1709.0","title":"Squeeze-and-Excitation Networks"},{"content":"十大经典的排序算法 #ifndef SORT_ALGO_H #define SORT_ALGO_H  #include \u0026lt;vector\u0026gt;using std::vector; using std::swap;  // 1. Bubble Sort // Time complexity: O(n^2) // Space complexity: O(1) void bubble_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  bool sorted = false;  for (int i = 0; i \u0026lt; nums.size() \u0026amp;\u0026amp; !sorted; ++i)  {  sorted = true;  for (int j = 1; j \u0026lt; nums.size() - i; ++j)  {  if (nums[j] \u0026lt; nums[j - 1])  {  swap(nums[j], nums[j - 1]);  sorted = false;  }  }  } }  // 2. Insertion Sort // Time complexity: O(n^2) // Space complexity: O(1) void insertion_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  for (int i = 1; i \u0026lt; nums.size(); ++i)  {  for (int j = i; j \u0026gt; 0 \u0026amp;\u0026amp; nums[j] \u0026lt; nums[j - 1]; --j)  swap(nums[j], nums[j - 1]);  } }  // 3. Selection Sort // Time complexity: O(n^2) // Space complexity: O(1) void selection_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  for (int i = 0; i \u0026lt; nums.size(); ++i)  {  int min_index = i;  for (int j = i + 1; j \u0026lt; nums.size(); ++j)  {  if (nums[j] \u0026lt; nums[min_index])  min_index = j;  }   swap(nums[i], nums[min_index]);  } }  // 4. Shell Sort // Time complexity: O(n^2) // Space complexity: O(1) void shell_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  int h = 1;   while (h \u0026lt; nums.size() / 3)  h = (h * 3) + 1; // step: 1 4 13 43 ...   while (h \u0026gt;= 1)  {  for (int i = h; i \u0026lt; nums.size(); ++i)  {  for (int j = i; j \u0026gt;= h; j -= h)  {  if (nums[j] \u0026lt; nums[j - h])  swap(nums[j], nums[j - h]);  }  }  h /= 3;  } }  int partition_1ways(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int r) {  int v = nums[l]; // pivot  int p = l;  int i = l + 1;  while (i \u0026lt;= r)  {  if (nums[i] \u0026lt; v)  swap(nums[i], nums[++p]);   i++;  }  swap(nums[l], nums[p]);  return p; }  void quick_sort_1ways(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int r) {  if (l \u0026gt;= r)  return;   int p = partition_1ways(nums, l, r);  quick_sort_1ways(nums, l, p - 1);  quick_sort_1ways(nums, p + 1, r); }   int partition_2ways(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int r) {  int v = nums[l];  int i = l + 1;  int j = r;  while (true)  {  while (i \u0026lt;= r \u0026amp;\u0026amp; nums[i] \u0026lt;= v)  i++;  while (j \u0026gt;= l \u0026amp;\u0026amp; nums[j] \u0026gt; v)  j--;   if (i \u0026gt; j)  break;   swap(nums[i++], nums[j--]);  }  swap(nums[l], nums[j]);  return j; }   void quick_sort_2ways(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int r) {  if (l \u0026gt;= r)  return;   int p = partition_2ways(nums, l, r);  quick_sort_2ways(nums, l, p - 1);  quick_sort_2ways(nums, p + 1, r); }  void quick_sort_3ways(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int r) {  if (l \u0026gt;= r)  return;   int v = nums[l];  // nums[l ... lt-1] \u0026lt; v  // nums[lt ... gt-1] == v  // nums[gt ... r] \u0026gt; v  int lt = l;  int i = l + 1;  int gt = r + 1;  while (i \u0026lt; gt)  {  if (nums[i] \u0026lt; v)  swap(nums[i++], nums[++lt]);   else if (nums[i] \u0026gt; v)  swap(nums[i], nums[--gt]);   else  i++;  }   swap(nums[l], nums[lt]);   quick_sort_3ways(nums, l, lt - 1);  quick_sort_3ways(nums, gt, r); }  // Quick Sort // \u0008Time complexity: O(nlogn) // Space complexity: O(n) void quick_sort_1ways(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  quick_sort_1ways(nums, 0, nums.size() - 1); }  // 5. Quick Sort Notes: nice! // \u0008Time complexity: O(nlogn) // Space complexity: O(n) void quick_sort_2ways(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  quick_sort_2ways(nums, 0, nums.size() - 1); }  // Quick Sort // \u0008Time complexity: O(nlogn) // Space complexity: O(n) void quick_sort_3ways(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  quick_sort_3ways(nums, 0, nums.size() - 1); }  void merge(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int mid, int r) {  vector\u0026lt;int\u0026gt; aux;  for (int i = l; i \u0026lt;= r; ++i)  aux.push_back(nums[i]);   int i = l, j = mid + 1;  for (int k = l; k \u0026lt;= r; ++k)  {  if (i \u0026gt; mid)  {  nums[k] = aux[j - l];  j++;  }   else if (j \u0026gt; r)  {  nums[k] = aux[i - l];  i++;  }   else if (aux[i - l] \u0026lt; aux[j - l])  {  nums[k] = aux[i - l];  i++;  }  else  {  nums[k] = aux[j - l];  j++;  }  } }  void merge_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int r) {  if (l \u0026gt;= r)  return;   int mid = l + (r - l) / 2;  merge_sort(nums, l, mid);  merge_sort(nums, mid + 1, r);   merge(nums, l, mid, r); }  // 6. Merge Sort // Time complexity: O(nlogn) // Space complexity: O(n) void merge_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  merge_sort(nums, 0, nums.size() - 1); }  void shift_down(vector\u0026lt;int\u0026gt;\u0026amp; nums, int size, int k) {  while (2 * k + 1 \u0026lt; size)  {  int j = 2 * k + 1; // this is left child index.  if (j + 1 \u0026lt; size \u0026amp;\u0026amp; nums[j+1] \u0026gt; nums[j])  j++;   if (nums[k] \u0026lt; nums[j])  swap(nums[k], nums[j]);   k = j; // continue to sink.  } }  // 7. Heap Sort // Time complexity: O(nlogn) // Space complexity: O(1) void heap_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  // The last non-leaf node starts to sink.  for (int i = (nums.size() - 2) / 2; i \u0026gt;= 0; --i)  shift_down(nums, nums.size(), i);   for (int i = nums.size() - 1; i \u0026gt; 0; --i)  {  swap(nums[0], nums[i]);  shift_down(nums, i, 0);  } }  // 8. Counting Sort // NOTES: 0 \u0026lt;= nums[i] \u0026lt; 100 // Time complexity: O(n) // Space complexity: O(n) void counting_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  if (nums.size() \u0026lt; 2)  return;   vector\u0026lt;int\u0026gt; count(100, 0);  // Record the frequency of each element  for (int i = 0; i \u0026lt; nums.size(); ++i)  count[nums[i]]++;   for (int i = 1; i \u0026lt; 100; ++i)  count[i] += count[i - 1];   vector\u0026lt;int\u0026gt; aux(nums.size(), 0);  for (int i = nums.size() - 1; i \u0026gt;= 0; --i)  {  aux[count[nums[i]] - 1] = nums[i];  count[nums[i]]--;  }   nums = aux; }  // 9. Radix Sort  int get_max(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  int max_val = nums[0];  for (int i = 1; i \u0026lt; nums.size(); ++i)  {  if (nums[i] \u0026gt; max_val)  max_val = nums[i];  }   return max_val; }   void counting_sort_ten(vector\u0026lt;int\u0026gt;\u0026amp; nums, int place) {  int max = 10; // [0 ..9]  vector\u0026lt;int\u0026gt; count(10, 0);  for (int i = 0; i \u0026lt; nums.size(); ++i)  count[(nums[i] / place) % 10]++;   for (int i = 1; i \u0026lt; max; ++i)  count[i] += count[i - 1];   vector\u0026lt;int\u0026gt; aux(nums.size(), 0);  for (int i = nums.size() - 1; i \u0026gt;= 0; --i)  {  aux[count[(nums[i] / place) % 10] - 1] = nums[i];  count[(nums[i] / place) % 10]--;  }   nums = aux; }  // 9. Radix Sort // Time complexity: O(n) // Space comnplexity: O(1) void radix_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  if (nums.size() \u0026lt; 2)  return;   int max_val = get_max(nums);  for (int place = 1; max_val / place \u0026gt; 0; place *= 10)  counting_sort_ten(nums, place); }  // 10. Bucket Sort // NOTES: 0 \u0026lt;= nums[i] \u0026lt; 100 void bucket_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) {  vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; bucket(10);   for (int i = 0; i \u0026lt; nums.size(); ++i)  bucket[nums[i] / 10].push_back(nums[i]);   int k = 0;  for (int i = 0; i \u0026lt; 10; ++i)  {  for (int j = 0; j \u0026lt; bucket[i].size(); ++j)  nums[k++] = bucket[i][j];  }  // An almost ordered array, called insertion sort.  insertion_sort(nums); }  #endif // SORT_ALGO_H ","permalink":"http://landodo.github.io/posts/20201230-ten-classic-sorting-algorithms/","summary":"十大经典的排序算法 #ifndef SORT_ALGO_H #define SORT_ALGO_H #include \u0026lt;vector\u0026gt;using std::vector; using std::swap; // 1. Bubble Sort // Time complexity: O(n^2) // Space complexity: O(1) void bubble_sort(vector\u0026lt;int\u0026gt;\u0026amp; nums) { bool sorted = false; for (int i = 0; i \u0026lt; nums.size() \u0026amp;\u0026amp; !sorted; ++i) { sorted = true; for (int j = 1; j \u0026lt; nums.size() - i; ++j) { if (nums[j] \u0026lt; nums[j -","title":"十大经典的排序算法"},{"content":"计算机网络复习 OSI (Open Systems Interconnection) 参考模型和 TCP/IP 模型\nTCP/IP 四层模型的代表协议：\n 网络接口层（Link Layer）：ARP 协议（IP addr \u0026mdash;ARP\u0026mdash;\u0026gt;MAC addr） IP 网络层（Internet Layer）：IP、ICMP、IGMP 等协议。 传输层（Transport Layer）：TCP 和 UDP 协议 应用层（Application Layer）：HTTP、SMTP、DNS、FTP、SSH、DHCP、Telnet  数据传输单位：\n 物理层：比特 数据链路层：帧 网络层：数据报 传输层：报文段（TCP）、用户数据报（UDP）  TCP/IP 四层结构有什么优缺点？\n优点：\n TCP/IP 在设计之初就考虑到了多种异构网的互联问题，并将 IP 作为一个单独的重要层次。  缺点：\n 它在服务、接口与协议的区别上不清楚。 TCP/IP 的主机－网络层本身并不是实际的一层，它定义了网络层与数据链路层的接口。物理层与数据链路层的划分是必要和合理的，一个好的参考模型应该将它们区分开来，而 TCP/IP 参考模型却没有做到这点。  OSI 七层模型的优缺点？\n优点：\n OSI 参考模型的最大贡献就是精确定义了三个主要概念：服务、接口和协议。这与现代的面向对象程序设计思想非常吻合。 OSI 参考模型产生在协议发明之前，通用性较好。  2. TCP 的拥塞控制 2.1 慢启动 慢启动机制\n 初始窗口 initial cwnd，初始值设置为 3 或 10 慢启动门限值 ssthresh，初始值设置为 1 \u0026laquo; 31 每收到 ACK，窗口值加 1  慢启动并不慢\n 在没有丢包情况下，经过 log2(target_cwnd/initial_cwnd) 个 RTT 长到目标窗口大小  Initialization: cwnd \u0026lt;- initial cwnd if cwnd \u0026lt; ssthresh: for each ack: cwnd += 1 else: for each ack: cwnd += 1/cwnd when encountering loss: ssthresh \u0026lt;- cwnd cwnd \u0026lt;- cwnd/2 TCP 锯齿状窗口行为。\n2.2 快速重传  一般情况下，先发送的数据包应该先到达。如果后发送的数据包先被确认，可推测先发送的数据包丢失。 如果一个数据包后面的三个数据包都被确认，而该数据包还未收到确认，则认定该数据包丢失，并重传该数据包。 数据包通常都是连续发送的。快速重传通常可以在 1 个 RTT 内重传数据丢包。  为什么需要等 3 个后续数据包的确认?\n 防止因数据包乱序引起的误重传 在无线网络、多路径传输中会有部分乱序 假设网络中乱序长度不大于 3 个数据包 通过 D-SACK 机制 (Duplicate SACK) 识别一个数据包没有丢失，而是乱序。  一般来说，快速重传可以恢复长流中的大部分丢包。\n3.2 超时重传   重传后丢包，即同一数据包被丢弃两次后，只能等待超时重传。\n  通过超时来判断数据包丢失\n  定时器至少大于一个 RTT (Round Trip Time)\n  定时器必须能够适应 RTT 变化，定时器必须同时反映出 RTT 大小和 RTT 变化\n  Linux 中最小值为 200ms\n  3.3 避免拥塞 流控（Flow Control）：\n 为了防止快发送方给慢接收方发数据造成接收崩溃。注意与拥塞控制的区别。 发送方和接收方各自维护一个窗口大小  两种拥塞控制思路：\n 端到端的拥塞控制：端设备通过丢包、延迟变化 等推测网络拥塞状况。TCP 使用端到端的拥塞控制策略。 网络辅助的拥塞控制：网络设备对端设备提供反馈。  TCP 的拥塞控制算法基础：\n (1) TCP 端设备遇到丢包时，认为网络拥塞，减慢发送速率（发送速率（窗口大小）减半） (2) TCP 端设备定期通过增大发送速率来探测更多可用带宽（每个 RTT，窗口值增加一个数据包大小）  ✅ 3.4 几种拥塞控制的方法总结 RFC 2581 定义了进行拥塞控制的四种算法，即慢开始（slow-start）、拥塞避免（congestion avoidance）、快重传（fast retransmit）和快恢复（fast recovery）。\n（1）慢开始（slow-start）和拥塞避免（congestion avoidance）\n（2）快重传（fast retransmit）和快恢复（fast recovery）\n发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段 M3，而不必继续等待为 M3 设置的重传计时器到期。\n快恢复（fast recovery）门限 ssthresh = cwnd / 2; cwnd = ssthresh; \n4. 简述从输入网页网址到获得相关网页内容的步骤 （1）浏览器根据 DNS（Domain Name System，域名系统）服务器返回的真实地址请求网页，DNS 主要负责把对人类友好的网址转换为对机器友好的 IP 地址。\n（2）浏览器请求计算机建立对这个 IP 地址的标准 Web 端口（80）或标准安全 Web 端口（443）的TCP（Transmission Control Protocol，传输控制协议）连接。\n（3）当浏览器连接到 Web 服务器之后会请求网站，这一步就要用到 HTTP（或 HTTPS） 了。\n（4）服务器会根据请求的 URL 响应相关内容。\n（5）Web 浏览器负责处理返回的响应（HTML/CSS/JS）。\n（6）Web 浏览器请求自己需要的额外资源。\n（7）浏览器在获取了足够的关键资源后，开始在屏幕上渲染页面。\n（8）在页面刚刚显示在屏幕上之后，浏览器会在后台继续下载其他资源，并在处理完它们之后更新页面。\n（9）当页面完全被加载后，浏览器会停止显示加载图标（在多数浏览器上都位于地址栏旁边），然后触发 OnLoad JavaScript 事件。根据这个事件，JavaScript 就知道可以执行某些操作了。\n（10）此时，页面已经完全加载了，但浏览器并不会停止发送请求。网页只包含静态内容的时代早就过去了。\n5. HTTPS 过程   显然易见的是，非对称加密在性能上不如对称加密，两者各有其优缺点。HTTPS 就是将两者的优点结合起来。更具体的说就是，公钥私钥主要用于传输对称密钥，而真正的双方大量数据量的通信都是通过对称密钥进行的。\n  数字证书用来解决公钥的合法性问题。\n  非对称加密需要通过证书（Certificate）和权威机构 CA（Certificate Authority）来验证公钥的合法性。CA 通过层层授信背书的方式，从而确保了非对称加密模式的正常运转。\n  我在 HTTPS 服务器的实验中，在数据传输之前，客户端和服务器会获取对方的证书，检查证书的基本信息（因为证书是自己签名的，没有权威 CA，因此不进行证书层层校验，直接认定是一个合法的证书），之后商量好对称密钥，使用对称密钥进行数据传输。\n6. 简述区块链的技术原理和应用 6.1 技术原理简述 区块链（Blockchain）是一种创新的分布式交易验证和数据共享技术，也被称为分布式共享总账（Distributed Shared Ledger）。\n区块（block）保存：\n （1）业务记录集合（以 hash link 的方式） （2）前序块的哈希值（数字摘要） （3）(1+2) 的哈希值（数字摘要）。  每个记录中有发起者的数字签名，保证操作的不可伪造性和不可抵赖性。\n链（chain），就是逻辑上由 “ (2) 前序块的哈希值” 串联起来的链条，保证不可删除、时序性和不可篡改特性。\n后块为所有的前块背书，起到 “联保” 作用， 这样的数据结构即使是最初发布数据的人也不能改动他自己的数据了。\n区块链解决的核心和本质问题是：无可信中心机构时，如何在信息不对称、不确定的环境下，建立满足活动赖以发生、发展的“信任”生态体系，即 “拜占庭容错”或者“两军问题”。\n6.2 应用简述 区块链技术的适用场景：\n 多方参与 缺乏统一信任主体 存在价值流通或信任传递  工信部《中国区块链技术和应用发展白皮书》\n区块链热点应用案例：\n 供应链金融，现有问题：造假风险、企业信息孤岛、履约风险高。 支付、清算、结算，现有问题：造假资金滞后、成本高昂。 跨境贸易，现有问题：数据缺乏共享，形成数据孤岛；参与方之间信任缺失；流程协同低效；中心平台透明度低，依赖性强。 政务现代化，现有问题：信息资源不共享、不开放、成本高昂；⺴络安全存在隐患；多方介入，效率低下；缺乏法律制度保障。 人力资源，现有问题：学历造假、履历造假、绩效管理不透明。 司法版权 电子发票 医疗健康 智能制造 小额数字资产 国家法定数字货币  7. 数据中心网络，数据级流和流量级流的优劣势  流量模式不固定，分钟级别的变化都可能很大  ECMP, “Equal Cost Multi Path”：流级别的负载均衡\n 使用 HASH （source-addr, dst-addr, source-port, dst-port） 优势：一条流的数据包在相同的 path 上 不足：elephants flows  VL2：Virtual Layer 2（整个网络看做是一个二层交换机）基于实际数据出发，使用已有的技术，实现可 扩展、敏捷的数据中心网络。\n 优点：每条流随机选择中间交换机（Intermediate）转发，随机性解决不稳定性 缺点：（1）目录服务器的引入是否会成为瓶颈? （2）为了实现 VLB，所有链路、所有交换机一直在线，能耗高。  8. NDN 的体系结构 NDN 的体系结构改变的是现行网络体系结构的哪些部分？为什么 NDN 很难部署？\nNDN：Named Data Networking\n目前互联网面临的问题：大量的补丁使得 IP 网络系统越来越复杂，而且可能阻碍业务的创新。\n 流量激增 移动性支持 安全/管理控制问题  新的体系结构，核心：naming，addressing。\nNDN 解决三个关联问题。\n接收方的数据传输：一个 Interest 对应一个 Data\n带状态、hop-by-hop 数据传输：Data 沿着 Interest 的反向路径传输回去\nNDN 安全：\n 完整性、正确性：IP 的管道安全\u0026mdash;\u0026gt;NDN 的内容安全 DoS 攻击：(1) 一个 Interest 对应一个 Data，不会引起 Data 的 flooding。(2) 相同的 Interest 在可以在 Pending Interest Table 中屏蔽。  NDN 缺点导致难以部署：\n 带状态的网络，将使得网络设备的实现和维护复杂 名字是不定长的，路由表查找困难 交互式应用支持问题 路由表规模非常庞大  9. BBR 测量链路瓶颈 BBR：在 TCP 的演进历史中，在不降低吞吐率的前提下减少延迟。是 TCP 拥塞控制算法优化。\nBBR (Bottleneck Bandwidth and RTT) 设计目标：在不降低吞吐率的同时，减少网络延迟。\n (1) 每次收到 ACK 后，更新对 Max Throughput 和 Min RTT 的估计 (2) 通过控制发送数据量来探测 Max Throughput 和 Min RTT  BBR 测量链路瓶颈带宽和 RTT，为什么不能同时测量？如何请求测量这两个值？\nBBR 的基本理念就是尽量估算 $RTT_{prop}$ 和 Bottleneck BW 这两个参数。\n$$cwnd=2\\times BDP=2\\times (RTT_{prop} + btlBW)$$\n$RTT_{prop}$ 和 Bottleneck BW 是两个重新审视拥塞控制的参数:\n $RTT_{prop}$：round-trip propagation time btlBW： bottleneck bandwidth  类比水管的话，$RTT_{prop}$ 就是水管的长度，btlBW 就是水管的宽度。\n我也不知道怎么测？\n10. 内容分发 CDN 访问局部性：\n 80-20 准则、zipf 定律：20% 的内容吸引 80% 的访问（10-20% 的内容产生 80% 的访问）  CDN: Content Delivery Network：把内容拷⻉到不同地域的多台服务器， 减轻服务器负载，提升用户感知的质量。\nCDN 主动发布内容：原始服务器发布内容到 CDN 分发节点，CDN 主动复制内容到其他服务器。\nCDN 核心问题：选择哪个服务器？\n CDN 提供商持续监测服务器的负载、性能 距离客户端近的服务器  静态配置：通过 HTTP Redirect 实现\n 比如：来自北京的请求，都重定向到北京的服务器  DNS 回顾：\nAkamai：全球最大的 CDN。Akamai 工作原理：\n NS-based mapping  $MAP_t = \\sum_{internet} \\times \\sum_{Akamai}\\times Domain \\times LDNS \\rightarrow IPs$\n end-user mapping  $EUMAP_{t} = \\sum_{internet} \\times \\sum_{Akaamai} \\times Domain \\times Client \\rightarrow IPs $\nCDN 面临的挑战：\n 网络层面：(1) CDN服务器所在⺴络与其他⺴络的互联问题；(2) 内容传输效率问题 应用层面：(1) 内容放置问题(地域)：内容复制到哪些服务器上? (2) 存储问题:多大的存储空间?  11. 路由器/交换机 Buffer 队列 数据包队列是网络中间设备中最关键的部分之一，其大小、管理策略等很大程度上影响了网络性能。\n队列应该设为多大？\n $BufferSize=\\bar{RTT} \\times C$: $\\bar{RTT}$ 为端到端平均链路延迟，C 为瓶颈链路带宽。  队列大小与队列管理策略、传输控制策略等关系密切。\n队列大小决定传输速率，传输控制策略影响了队列行为。\n队列过大：$BufferSize \u0026gt; \\bar{RTT} \\times C$，当窗口减半时，队列不能清空数据包，因此数据包的延迟会增加。\n队列过小：$BufferSize \u0026lt; \\bar{RTT} \\times C$，当窗口大小减半时，发送方在等待对方的数据确认，因此瓶颈链路处于空闲状态。\n数据中心网络的交换机队列：队列相对过小 (under-buffered) 造成了 TCP-Incast 问题。\n广域网的路由转发设备队列：队列过大 (over-buffered) 造成了 BufferBloat 问题。\n解决 BufferBloat 问题：\n 减小队列大小 改进传输控制策略 改进队列管理策略 Tail Drop (尾部丢弃) RED (Random Early Detection)：主动 (proactively) 丢包 (Early Detection)，概率性丢包。 CoDel (Controlling Delay)：控制数据包在队列中的时间(延迟)，而不是队列长度。  数据包队列总结：\n 队列是网络设备中的关键部分，其大小、管理策略影响了网络性能。 队列设置过大或过小都容易产生问题。过小 \u0026ndash;\u0026gt; TCP Incast 问题，过大 \u0026ndash;\u0026gt; BufferBloat 问题 队列大小、队列管理策略、传输控制策略之间的关系非常紧密。  12. FIB 计算算法、IPv4 Trie 树、IPv6 的优化方法 FIB (Forwarding Information Base)，路由器转发表。转发表中存储的是网络号与下一跳地址的映射关系\n21.1 IP报文转发规则 路由器收到 IP 报文后，获取目的地址 D，按照如下规则进行转发:\n（1）如果 D 与路由器在同一网络内，直接交付给主机 D，并返回;\n（2）在转发表中进行最长前缀匹配，如果匹配成功，则将 IP 报文转发到该下一跳网关，并返回;\n（3）如果转发表中有默认路由，则将IP报文转发给默认路由器，并返回;\n（4）报告转发分组出错（ICMP，目的网络不可达）。\n12.2 Trie 高速查找、快速更新和可拓展的虚拟路由器 IP 查找方法！\n每增加一个 trie 树，节点大小只增加 1 比特。存储 14 个 IPv4 核心路由器 FIB，trie 树大小仅为 10MB。\n决策树算法的缺点：在决策树算法中，90% 的内存占用来自规则的复制。\n12.3 IPv6  IPv6 地址长度为 128 位。IPv6 地址用十六进制表示，分为 8 段，中间用 : 隔开：  2001:0410:0000:0001:0000:0000:0000:45ff  每段的起始 0 可以省略，连续全零的段可用 :: 表示（只能出现一次）  2001:410:0:1:0:0:0:45ff, 2001:410:0:1::45ff  不同起始码对应不同类型 IPv6 地址，例如 ::ffff/96 表示与 IPv4 兼容的地址  128 为的 IPv6 的优势：更容易进行层次化（Hierarchical）编址。\nMAC 地址转换为 IPv6 地址：\n 先将 MAC 地址转换为 EUI-64 标识（64 位全球唯一标识 (Extended Unique Identifier)） IPv6 网络前缀 (64 位) + EUI-64 地址  IPv4 地址到 IPv6 地址的映射：\n 前缀为 ::ffff/96 是保留一小部分地址与 IPv4 兼容的 数据报在这两类节点之间转发时，需要进行地址的转换 NAT-PT (Protocol Translator）  IPv6 取消了 ARP 协议。通过邻居请求报文（NS）和邻居通告报文（NA）来解析三层地址对应的链路层地址。\n","permalink":"http://landodo.github.io/posts/20201225-computer-network-review/","summary":"计算机网络复习 OSI (Open Systems Interconnection) 参考模型和 TCP/IP 模型 TCP/IP 四层模型的代表协议： 网络接口层（Link Layer）：ARP 协议（IP addr \u0026mdash;ARP\u0026mdash;\u0026gt;MAC addr） IP 网络层（Int","title":"计算机网络基础（3）"},{"content":"计算机网络基础 第一讲主要介绍了计算机网络的体系结构、性能、安全以及挑战和机遇。并且还讲了中科院在网络方面的相关工作。\n互联网不仅仅改变生活与社会，还推动信息技术本身发展。\n物理传输：通信\n 基础对象：bit 调制、编码、信道、同步 香农定理：$C=Blog_2(1 + \\frac{S}{N})$，C：传输速率；B：信道带宽；S/N 为信噪比。 调幅、调频、调相  数据传输：包交换\n 基础对象：数据帧 存储转发  包交换 VS 链路交换。\n数据传输：介质访问控制 MAC（medium access control）\n网络互连：异构网络，不同物理寻址机制。\n路由与路由查找：路由协议（控制平面）、路由查找转发（数据平面）\n可靠性与传输控制：如何在不可靠路径上尽力而为的实现可靠数据传输？无差错、不丢失、不重复、顺序。如何实现拥塞控制？接收窗口（接收端流控）；拥塞窗口（发送端流控）、拥塞判断：丢包、延迟、多指标。\nDNS：递归查询\n层次结构与实现模块化\n功能放置 // 客户端 SocketFD = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP); connect(SocketFD, (struct sockaddr *)\u0026amp;sa, sizeof sa); send() and recv() write() and read() close();  // 服务器 listen(SocketFD, QUEUE); ConnectFD = accept(SocketFD, NULL, NULL); send() and recv() write() and read(); close(); 报文封装：HTTP Header、TCP Header、IP Header、Ethernet Header。\n延迟构成：$T=T_{tras}+T_{proc}+T_{prop}+T_{queue}$\n $T_{tras}=2P/R$：传输延迟 $T_{proc}$：查找处理 $T_{prop}$：信号传播延迟 $T_{queue}$：排队延迟  吞吐量\nBGP 1989，3 张餐巾纸中的边界网关协议（BGP）图片来源: YAKOV REKHTER/WASHINGTON POST。\n网络的安全性和可靠性：路由安全、DDoS、安全防护困难。\n中科院与中国互联网发展  1975 年，计算所开始关注网络研究 1981 年，计算所成立了网络研究室(十室)是国内最早开始从 事互联网研究的实验室 1983 年，中科院与德国弗朗霍夫信息与生物技术研究所合作 研制了 X.25 分组交换网络，十室承担了该项目  国内最早开始与国外合作从事的网络研究项目 开发了 ISO X.25 底层软件，网络通信和管理软件   1989: NCFC 1992 年 6 月开始，由十室研究中国域名体系 1995 年 3 月，成立计算机网络信息中心  谢老师给学生的一些学习建议：\n 忘记分数\n独立思维 We reject kings, presidents and voting. We believe in: rough consensus and running code. –– David D. Clark\n动手实践 Talk is cheap. Show me the code. –– Linus Torvalds\n 第二讲 网络基础：网络模型与直连网络 武老师是计算所的副研究员，研究方向是互联网体系结构和互联网测量与优化。\n第二讲的主要内容是计算机网络体系结构模型和直连网络（Direct Link Networks）。\n分层网络模型（Layered Network Model）：模块化方案、两层结构、三层结构、分层模型。\n细腰结构（narrow-waist）是互联网体系结构模型中最典型的特征。研究表明，分层的体系结构最终会演化成细腰模型，互联网体系结构一直在演进中，现有结构可能会演化成新的细腰模型。\n直连网络性能指标：带宽（Bandwidth）和时延（Latency）= 传播时延+处理时延+排队时延。\n数据帧封装：添加首部和尾部界定帧的范围。\n引入转义字符进行透明传输。\n差错检测：奇偶检验、检验和 checksum、循环冗余校验（Cyclic Redundancy Check, CRC）。CRC 的本质是 Hash 函数。\n可靠传输基本思想：确认（acknowledgment, ACK）ACK 帧、超时（timeout）重传。\n停等（Stop-and-Wait）协议：最简单的可靠传输协议。\n提升传输速率：序列号（Seq）\n滑动窗口 滑动窗口算法（Sliding-window） ：可靠传输、高效传输、按序到达、流控功能。\n 接收端：对于每个新到达的数据帧 Seq：如果 LastFrame \u0026lt; Seq \u0026lt;= ExpMaxFrame，则接受；否则，丢弃。接受数据帧后，将收到的最大连续数据帧 Seq 作为 ACK 回复。 发送端：收到新的 ACK，更新 LastACK，如果窗口允许，发送新的数据帧，更新 LastFrame。  回退 N 机制（Go-Back-N）恢复丢包。只需要保证 MaxSeq/2 \u0026gt;= Wnd，就可以准确区分已接收和等待确认的数据帧。\n多路复用 多路复用技术：频分复用（Frequency Division Multiplexing, FDM）、时分复用（Time Division Multiplexing, TDM）、统计时分复用（Statistic TDM, STDM）、码分复用（Code Division Multiplexing, CDM）。\n载波帧听多路访问（Carrier Sense Multiple Access, CSMA）\n带碰撞检测（Collision Detection）的 CSMA (CSMA/CD)，用于 Ethernet。\n带碰撞避免（Collision Avoidance）的 CSMA (CSMA/CA)，用于 无线局域网络，例如 WiFi。\n以太网 以太网：MAC 地址，以太网基本上统治了有线局域网。\nWiFi 接入 WiFi 热点：\n 扫描：Probe 和 Probe Response 帧 关联：Association Request 和 Association Response 帧 IP 地址分配：DHCP 认证  蜂窝通信 1G 蜂窝通信是为语音通信设计的模拟 FDM 系统，几乎不支持数据传输。\n蜂窝通信网络是覆盖范围最广的通信机制之一。\n5G 毫米波技术、信道编码技术、大规模 MIMO、海量连接、低延迟技术、网络切片。\n第三讲 网络互连 第二讲到第六讲，都由中国科学院计算技术研究所网络技术研究中心的武老师进行讲授。\n第三讲主要讲交换网络、网络互连和数据包队列。\n交换网络的设计目标是数据只朝着目的节点方向传送（转发，Forward）。\n数据帧转发：给定一个包含源目的 MAC 地址的数据帧，如何确定从哪个端口转出?\n 交换机存储目的 MAC 地址到（出）端口的映射关系（Forwarding Database, FDB）。 对于每个数据帧，在 FDB 中查找目的 MAC 地址对应的端口号进行单播或广播。 老化机制（Aging）更新 FDB。 每收一个新的数据帧，记录其源 MAC 地址和入端口，将该映射关系写入 FDB 表。  生成树（Spanning Tree）消除广播风暴。\nIPv4\n无类别域间路由（Classless Inter-Domain Routing, CIDR）：前缀（prefix）长度、网络掩码（network mask）。CIDR 可更加充分的使用 IP 地址。\nIP 数据包头部格式：\n Length：IP 数据包长度，最大为 65535 字节 Protocol：标识所承载协议类型，例如 TCP: 6, UDP: 17 Source \u0026amp; Destination Address：源目的 IP 地址  IP 报文转发：路由器讲转发信息存储在转发表中（Forwarding Information Base），网络号和下一跳。\n地址解析协议（Address Resolution Protocol, ARP）：知道下一跳 IP 地址，查询其 MAC 地址。ARP 只作用于局域网。\nIP 分片（Fragmentation）：最大传输单元（Maximum Transmission Unit, MTU）。\n互联网控制消息协议（Internet Control Message Protocol, ICMP）：通过发送错误代码、控制信息等来诊断和控制网络。\nNAT（Network Address Translation）\nIPv6：128 位。\n以太网地址转换为 IPv6 地址：\nIPv4 地址到 IPv6 地址的映射\n1. TCP/IP 网络面临的问题及其本质原因？ 从根源分析可以简单分为 3 类问题：\n（1）流量激增问题\n 互联网流量爆炸  流量增长速度远远高于芯片处理性能增长速度 增加带宽并不能从根本上解决问题   本质原因  业务的繁荣 数据的重复传输    IP 流量在 5 年（2012-2017）时间里增长了 4 倍，预计未来 5 年增长 3 倍。\n访问频率重尾特效：少量内容被多次重复访问。\n端到端特性：路由器只负责转发，并不知道重复传输。\nCDN 并不能重根本上解决问题。\n（2）移动性支持问题\n\u0026lt;sIP, dIP, sPort, dPort, protocol\u0026gt; ：断开连接，重新建立连接。\n 端到端连接绑定了地址，移动过程切换地址：断开连接，应用层重新建立连接，服务质量下降 本质原因：  IP 地址的二义性：既想表示位置，又想表明身份。 不支持地址和身份的动态绑定。    （3）可管可控问题\n 安全问题  网络攻击和信息安全事件频发 IP 网络以传输为功能，能保证传输通道安全，不能保障数据的安全 IP 设计之初场景是安全的   管理和控制问题  人工网络配置复杂 缺乏一个独立的配置平台    2. TCP/IP 网络如何支持节点的移动？ IP 地址的两个职责：\n 标识符（Identifier） 定位符（Locator）  2.1 Mobile IP 的技术思路：   假设移动主机有一个永久的 IP 地址\n  称为本地地址 (home address)，作为 identifier\n  与移动前的网络拥有相同前缀\n    主机移动到新的网络时\n 获得新的 IP 地址，作为 locator 两个地址可以共存    locator 负责接收数据，identifier 负责解复用数据。即移动前 locator 和 identifier 地址相同，移动后一收一发。\n2.2 Mobile IP 技术方案  移动主机（Mobile Node）：分配一个永久的本地 IP 地址。 本地代理（Home Agent）：位于移动主机的本地网络，维护本地地址到转交地址的映射。 外地代理（Foreign Agent）：在本地代理与移动主机之间转发数据，周期性向外通告提供转交地址。 转交地址（Care-of-Address）：表示移动主机的位置，通常是外地代理的地址 对端主机（Correspondent Node）  数据发生到移动主机过程：\n（1）本地代理截取目标为移动主机的数据\n（2）本地代理将数据发送到外地代理\n（3）外地代理将数据传送到移动主机\n移动主机注册过程：\n2.3 节点移动  NDN：带状态、hop-by-hop 数据传输 MobileFirst：面向移动的体系结构。关键是 GUID（全局唯一 ID） 到 NA（网络地址）的映射。 FOFIA：服务标识（SID）和网络标识（NID）。 XIA：支持多种结构并存。  3. 未来互联网体系结构试图改变思路是什么? 核心：naming, addressing\n 解决主机和位置关联 解决三个关联问题  4. 新旧互联网过渡的方法有哪些? 过渡机制\n 问题分解 SOFIA 穿越到 IP IP 网络通过 SOFIA 网络连接  5. 网络内缓存存在的双体问题指什么? In-Network cacheing\n网络内：路由、交换设备具有缓存内容的功能\n网络外：CDN 服务器\n核心区别：网络内缓存的双体问题（转发和缓存相互影响）\n6. 比较 IPv4、IPv6、NDN、SOFIA、XIA 3.1 NDN  带状态网络，使得实现和维护复杂 路由表规模庞大，查找困难 交互式应用支持  3.2 MobileFirst  复杂！ 全局映射（CUID 到网络地址 NA）是瓶颈 网络不保存连接状态 使用 NA 加快数据转发  3.3 SOFIA   结合了 NDN/MobilityFirst 和 IP 的特性。\n  支持不同种类的应用（如实时通信）\n  网络仍是无状态的\n  路由表面临与 NDN同样的拓展性问题\n  3.4 XIA: Expressive Internet Architecture  支持多种体系结构的体系结构 包头需要压缩 支持体系的演进  7. 安全问题如何通过未来互联网解决？  AIP：Accountability IP，问责制，网络和主机使用自验证（self-certifying）地址，公钥。 网络上的任何一个数据包最终都能追溯到主机（个人）。  ","permalink":"http://landodo.github.io/posts/20201218-computer-network/","summary":"计算机网络基础 第一讲主要介绍了计算机网络的体系结构、性能、安全以及挑战和机遇。并且还讲了中科院在网络方面的相关工作。 互联网不仅仅改变生活与社","title":"计算机网络基础（2）"},{"content":"最短路径问题 Shortest Path 广度优先遍历\n最短路径树 Shorted Path Tree\n单源最短路径 Single Shorted Path Tree。\n无权图的最短路径和带权图的最短路径不同，带权图需要考虑松弛操作（Relaxation）。\n松弛操作是最短路径求解的核心。\nDijkstra 单源最短路径算法 前提：不能有负权边。\n生活中大部分的图，是不存在负权边的。\n复杂度：$O(Elog(V))$\n借助最小索引堆。\n初始，对起点进行标识，对它所有的邻边进行访问。\n接下来就 Dijkstra 算法非常重要的一步。此时找到没有访问的顶点中，能够以最短的方式抵达的那个顶点，此时源点到达此顶点的最短路径确定。如下图 0 \u0026mdash;\u0026gt; 2 的最短路径就是 2。\n因为 0 到其他顶点的花费都高于 2，此时再折回来时，花费只会更大。（图中没有负权边）\n确定新的节点的最短路径后，进行 Relaxation 操作。遍历新节点 2 的所有邻边，经过中转站 2 到达 1 比 0 直接到达 1 花费更少，此时更新花费。\n此时从未访问的所有顶点中找出花费最小的顶点，就是源点到此节点的最短花费。\n对 1 节点进行松弛操作。以 1 为中转站更新。\n此时就 3、4 节点未访问，取最小花费的节点 4，确定最短路径。\n对 4 所有的邻边进行松弛操作。从 4 无法到达任何节点，此时不需要进行任何松弛操作。\n最后只有 3 这个节点没有被访问过，这就找到了从源点到 3 的最短路径。\n实现细节 IndexMinHeap 获取最小边的时间复杂度为 $log(V)$。\n对所有的边进行操作，最后的时间复杂度就是 $Elog(V)$ 。\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;queue\u0026gt;#include \u0026lt;cstdio\u0026gt;using namespace std;  const int n = 12;  #define inf INT32_MAX  int min_distance(int dist[], bool visited[]) {  // Initialize min value  int min = INT32_MAX, min_index;  for (int v = 1; v \u0026lt; n; v++)  if (visited[v] == false \u0026amp;\u0026amp; dist[v] \u0026lt;= min) {  min = dist[v];  min_index = v;  }  return min_index; }  // A utility function to print the constructed distance array void print_solution(int dist[], int s) {  printf(\u0026#34;s -\u0026gt; d : Distance from Source to destination\\n\u0026#34;);  for (int i = 1; i \u0026lt; n; i++) {  if (dist[i] == inf)  printf(\u0026#34;%d -\u0026gt; %d: inf\\n\u0026#34;, s, i);  else  printf(\u0026#34;%d -\u0026gt; %d: %d\\n\u0026#34;, s, i, dist[i]);  } }  void dijkstra(int g[n][n], int s) {  bool visited[n] = {false}; // 初始化所有顶点未访问   int dist[n]; // dist[i] is shortest distance from s to i  for (int i = 0; i \u0026lt; n; ++i)  dist[i] = INT32_MAX;  dist[s] = 0; // Distance of source vertex from itself is always 0   for (int i = 0; i \u0026lt; n; ++i) {  int u = min_distance(dist, visited);  visited[u] = true;   for (int v = 1; v \u0026lt; n; ++v) {  if (!visited[v] \u0026amp;\u0026amp; g[u][v] \u0026amp;\u0026amp; g[u][v] != inf \u0026amp;\u0026amp; (dist[u] != INT32_MAX) \u0026amp;\u0026amp; dist[u] + g[u][v] \u0026lt; dist[v])  dist[v] = dist[u] + g[u][v];  }  }  print_solution(dist, s); }  int main() {  // int c[n][n] = { {0,0,0,0,0,0},  // {0,0,2,3,5000,5000},  // {0,5000,0,1,2,5000},  // {0,5000,5000,0,9,2},  // {0,5000,5000,5000,0,2},  // {0,5000,5000,5000,5000,0}};   // int c[n][n] = { {0,0,0,0,0},  // {0,0,2,3,5000},  // {0,5000,0,1,2},  // {0,5000,5000,0,9},  // {0,5000,5000,5000,0}};   int c[n][n] = { {0,0,0,0,0,0,0,0,0,0,0,0},  {0,0,2,3,4,inf,inf,inf,inf,inf,inf,inf},  {0,inf,0,3,inf,7,2,inf,inf,inf,inf,inf},  {0,inf,inf,0,inf,inf,9,2,inf,inf,inf,inf},  {0,inf,inf,inf,0,inf,inf,2,inf,inf,inf,inf},  {0,inf,inf,inf,inf,0,inf,inf,3,3,inf,inf},  {0,inf,inf,inf,inf,inf,0,1,inf,3,inf,inf},  {0,inf,inf,inf,inf,inf,inf,0,inf,5,1,inf},  {0,inf,inf,inf,inf,inf,inf,inf,0,inf,inf,3},  {0,inf,inf,inf,inf,inf,inf,inf,inf,0,inf,2},  {0,inf,inf,inf,inf,inf,inf,inf,inf,2,inf,2},  {0,inf,inf,inf,inf,inf,inf,inf,inf,inf,inf,0}}; //邻接矩阵  dijkstra(c, 1);   return 0; } 参考资料   慕课网-Play-with-Algorithm\n  GeeksforGeeks\n  ","permalink":"http://landodo.github.io/posts/20201214-shortest-path/","summary":"最短路径问题 Shortest Path 广度优先遍历 最短路径树 Shorted Path Tree 单源最短路径 Single Shorted Path Tree。 无权图的最短路径和带权图的最短路径不同，带权图需要考虑松弛操作（Re","title":"最短路径问题 Shortest Path"},{"content":"论文题目：Xception: Deep Learning with Depthwise Separable Convolutions ✅ 论文地址：https://arxiv.org/pdf/1610.02357.pdf\n✅ 发表时间：2017 年\n对 Inception 模块进行了解释，它是常规卷积和可深度分离卷积（depthwise separable convolution）之间的一个步骤。\nXception 受到已被已被深度可分离卷积取代的 Inception 模块的启发。\nXception 的参数量与 Inception V3 相同，但是在 ImageNet 数据集上的表现要优于后者。\n1. Introduction 卷积神经网络的历史起源于 LeNet，它使用卷积堆叠和用于空间子采样的最大池化操作来进行特征的提取。AlexNet 继续进行优化，网络深度加深，网络可以在每个空间尺度上学习更丰富的特征。然后是 2014 年的 VGG 结构。\nSzegedy 等人在 2014 年提出 Inception 架构，也被称为 GoogLeNet（Inception V1）。 经过完善，出现了 Inception V2、Inception V3，以及 Inception-ResNet。\nInception 结构的灵感来自于 Network-In-Network 架构。\nInception 模块有几个不同的版本，与 VGG 简单的卷积-池化层堆叠不同，Inception 能够用较少的参数学习更丰富的表示。\n1.1. The Inception hypothesis 卷积层有 2 个维度：\n spatial dimensions 空间维度 channel dimension 通道维度  传统网络的卷积核是在通道和空间同时操作的。\nInception 的基本假设是：跨通道的关联性和空间相关性充分解耦。先通过一组 1x1 卷积，获取出通道的相关性，再通过常规的 3x3 或 5x5 卷积，获取空间的相关性。（Figure 1）\nFigure 1 是一个简化版本：\n在此基础上，Figure 3：1x1 卷积（通道维度 ）后，在输出通道的非重叠段上卷积（空间维度）。\n观察 Figure 3，自然而然地提出了一个问题：分区中的段数的影响是什么？假设跨通道相关性和空间相关性可以完全分开映射（即分成的段数等于 1x1 输出通道数），这是否合理？\n1.2. 卷积和可分离卷积 基于上面的假设，首先使用 1x1 卷积来映射跨通道的相关性，然后分别映射每个输出通道的空间相关性。如图 4 所示。这就是 Inception 的一个极端的结构（Extreme Inception）。这与 depthwise separable convolution 几乎相同，深度可分离卷积早在 2014 年就已经在神经网络设计中使用了。\n深度可分离卷积（depthwise separable convolution），在 TensorFlow 和 Keras 等深度学习框架中通常被称为 \u0026ldquo;可分离卷积（depthwise convolution）\u0026quot;。即在输入的每个通道上独立进行空间卷积，然后进行 1x1 卷积。\nextreme Inception module 与深度可分离卷积之间的两个小区别：\n 顺序：深度可分离卷积的顺序是空间\u0026mdash;\u0026gt;通道，而 extreme Inception 是通道\u0026mdash;\u0026gt;空间（即先执行 1x1 卷积）。 ReLU：在 Inception 中，这两个操作后都有 ReLU，而后者通常没有。  作者们认为顺序区别并不重要。将 Inception 模块替换为深度可分离卷积来改进 Inception 系列架构，即通过建立深度可分离卷积的堆栈模型。基于这一思想的卷积神经网络架构（Xception）的参数数量与 Inception V3 相似。\n2. Prior work 本篇论文依赖于前人的很多努力：\n VGG-16 架构 Inception 系列架构 深度可分离卷积 Depthwise separable convolution：2013 年，Laurent Sifre 在 Google Brain 实习期间开发了深度可分离卷积，并将其用于 AlexNet 中，获得了精度的小幅提升和收敛速度的大幅提升，以及模型尺寸的大幅缩小。详细的实验结果在 Sifre 的论文 “Rigid-motion scattering for image classification”。Andrew Howard 介绍了使用深度可分离卷积的 MobileNets。 残差连接 Residual connections  3. The Xception architecture 论文基于“卷积神经网络的特征图中的跨通道相关性和空间相关性的映射平可以完全解耦”这一假设，提出了一种完全基于深度可分离卷积层的卷积神经网络架构。\n命名为 Xception，代表的意思是 \u0026ldquo;Extreme Inception\u0026rdquo;。\nXception 架构有 36 个卷积层，36 个卷积层的结构分为 14 个模块，除了第一个和最后一个模块外，所有模块周围都有线性残差连接。\n简而言之，Xception 架构是一个具有残余连接的可深度分离卷积层的线性堆叠。\n In short, the Xception architecture is a linear stack of depthwise separable convolution layers with residual connections.\n 4. Experimental evaluation Xception 和 Inception V3 的参数数量几乎相同，将 Xception 与 Inception V3 在两个图像分类任务上进行比较：\n ImageNet 数据集上的 1000 类单标签分类任务 JFT 数据集上的 17000 类多标签分类任务  4.1. The JFT dataset JFT 是 Google 内部的大规模图像分类数据集，包括超过 3.5 亿张高分辨率图像，有 17000 个类别。\n论文作者先使用的是一个辅助数据集 FastEval14k，FastEval14k 是一个由 14,000 张图像组成的数据集，其中有约 6,000 个类别。\n使用前 100 个预测的平均平均精度（MAP@100）来评估性能。\n4.2. Optimization configuration ImageNet 和 JFT 使用不同的优化配置。\n  ImageNet\n 优化器：SGD Momentum: 0.9 初始学习率：0.045 学习率衰减：decay of rate 0.94 every 2 epochs.    JFT\n 优化器：RMSprop Momentum: 0.9 初始学习率：0.001 学习率衰减：decay of rate 0.9 every 3,000,000 samples.    对于两个数据集，Xception 和 Inception V3 都使用了完全相同的优化配置。\n4.3. Regularization configuration  权重衰减 Weight decay：L2 正则化，Inception V3 为 $4e-5$，Xception 为 $1e-5$ Dropout：ImageNet 在逻辑回归层之前使用了 0.5 的 dropout 层。JFT 上没有使用 Dropout Auxiliary loss tower：不使用。它可以反向传播网络中早期的分类损失，作为一个额外的正则化机制。  4.4. Training infrastructure 网络使用 TensorFlow 框架实现。在 60 个 NVIDIA K80 GPU 上分别进行训练，ImageNet 实验每次大约需要 3 天时间，而 JFT 实验每次需要一个多月时间（完全收敛需要 3 个多月）。\n4.5. Comparison with Inception V3 4.5.1 Classification performance 在 ImageNet 上，Xception 的结果略优于 Inception V3。\n在 JFT 上，Xception 在 FastEval14k MAP@100 指标上有 4.3% 的提升。\n在 ImageNet 上，Xception 比 ResNet-50、ResNet-101 和 ResNet-50 的表现更好。\n4.5.2 Size and speed 表 3 比较了 Inception V3 和 Xception 的参数数量和迭代速度。\n4.6. Effect of the residual connections 残差连接显然是帮助收敛的。\n对于深度可分离卷积堆栈的模型，残差连接不是必须的。作者们还用非残差的 VGG 模型也获得了很好的结果。\n4.7. point-wise 卷积后的中间激活效果 在 ImageNet，表明没有任何非线性会导致更快的收敛和更好的最终性能。\n对于深层的特征空间（如 Inception 模块中的特征空间)，非线性是有帮助的，但对于浅层的特征空间（如深度可分离卷积的 1 通道深层特征空间），非线性可能会导致信息的损失。\n5. Future directions 常规卷积和深度可分离卷积之间存在着一个离散的频谱（discrete spectrum），Inception 只是其中之一。\n经验评估中表明，Inception 模块的极端（即 Inception）比常规的 Inception 模块有优势。\n但是，作者们还没有证明 Xception 是最优的。可以最优的位于常规 Inception 模块和深度可分离卷积之间，这个问题留待将来研究。\n6. Conclusions 卷积和可深度分离卷积处于两个极端，Inception 模块是介于两者之间的一个中间点。\nXception 用可深度分离的卷积代替 Inception 模块。\nXception 的参数与 Inception V3 相似。\n与 Inception V3 相比，Xception 在 ImageNet 数据集上的分类性能提升较小，而在 JFT 数据集上的分类性能提升较大。\n预计深度可分离卷积将成为未来卷积神经网络架构设计的基石，因为它们提供了与 Inception 模块类似的特性，但又像普通卷积层一样易于使用。\n","permalink":"http://landodo.github.io/posts/20201208-xception/","summary":"论文题目：Xception: Deep Learning with Depthwise Separable Convolutions ✅ 论文地址：https://arxiv.org/pdf/1610.02357.pdf ✅ 发表时间：20","title":"Xception: Deep Learning with Depthwise Separable Convolutions"},{"content":"Hacker’s Delight——高效算法的奥秘 1. 二进制：Google的浪漫 先从一张图片开始。如上图 1 是 Google 在 2018 年 7 月 1 日的首页涂鸦，当天是戈特弗里德·威廉·莱布尼茨诞辰 372 周年。如果用户点击图片，就会跳转跳转到莱布尼茨的维基百科页面。Google 用这种浪漫的方式，纪念了莱布尼茨在二进制方面的贡献。\n这张图片二进制编码，从第一个开始，01000111 所对应的 ASCII 码即为英文字母 G，01101111 对应着小写英文字母 o。以此类推，自上而下，从左到右，这 6 个二进制串编码的信息即为 Google。\n提到莱布尼茨，大部分的第一印象肯定是他与牛顿谁先发明微积分这一数学界著名公案，这一公案直接导致莱布尼茨悲催的晚年。\n莱布尼茨的得意旗手约翰·伯努利在最速降线问题上被牛顿这头狮子的利爪划伤之后，从此退居二线，始终不愿正面与牛顿对抗。在胡克去世后，牛顿成为英国皇家学会会长，他迅速将皇家学会变成了自己的私属领地，依靠在科学方面的成就，受封爵位。此时，达到了一生影响力的巅峰的牛顿，亲自出手料理莱布尼茨，坐实莱布尼茨剽窃其学术成果。牛顿统治着整个皇家学会，而莱布尼茨却只是汉诺威选帝侯乔治治下的小小宫廷官员，两人在话语权上完全就不是一个段位的人。莱布尼茨去世之时仅有一位医生与秘书送行，甚至没有牧师为他祈祷。这位大师的葬礼，潦草得像是埋葬一个强盗。\n回到 Google 首页的涂鸦上，现代二进制计数系统由莱布尼茨于 1679 年设计，之后，通过布尔、香农等人的推动发展，现代意义的电子计算机开始出现，世界开始了翻天翻天覆地的变化。\n每一次微积分概念的应用，微积分符号的每一次书写，在电子设备上的每一次点击，都是莱布尼茨大师精神永存的体现。\n2. 雷神之锤：神一般存在的sqrt函数 我本科也是计算机专业，所以对计算机编程算是略知一二。在我看来，计算机中最神秘、最迷人的算法之一，就是位运算算法。举一个最简单的例子，如果我们想判断一个数 $x$ 是不是偶数，根据偶数的定义，$x%2==0$，则 $x$ 为偶数。但是更为优雅的方式是：$x\u0026amp;1==0$，则 $x$ 为偶数。这两种方式通过编译优化后，性能差距或许不明显，但是它们的思想是完全不同的。\n再来看另一个问题，编程计算 $\\sqrt{n}$，即计算平方根（Square Root）。看到这里，你可以稍稍停一下，如果换做是你进行编程，你会使用什么方式实现？\n(i) 定义法：使用二分法计算 $\\sqrt{n}$（注：暂不考虑溢出问题）：\n​ ① 初始化：$l=0, r = 0$\n② 计算 $l$ 和 $r$ 的中值：$mid=(l +r)/2$\n③ 比较 $mid^2$和 $n$ 的大小，如果 $mid^2 \u0026lt; n$，则 $l = mid$；否则 $r = mid$\n④ 重复②，直到满足精度要求.\n**(ii) 使用牛顿-拉弗森迭代法：**我们想要求解 $x = \\sqrt{n}$，我们令 $f(x) = x^2-n$。让目标满足 $f(x) = 0$，开方问题变成了求方程的根的问题。这里我直接给出迭代式：\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f\u0026rsquo;(xn)}$$\n代入 $f(x)$ 得：$x_{n+1} = \\frac{x_n + n}{2x_n}$\n// 牛顿迭代法 Xn+1 = (Xn + X / Xn) / 2 const double ESP = 10e-5; double sqrt(double n) {  double r = n;  while (fabs((r * r) - n) \u0026gt; ESP)  {  r = (r + n / r) / 2;  }  return r; } 一般来说，如果在算法面试时，能一气呵成编程实现牛顿迭代法，已经可以说是对这道题有一定的理解了。但是接下来介绍的一个算法，绝对能让人打开眼界，瞠目结舌。\n(iii)雷神之锤 3（Quake III Arena）- Fast Inverse Square Root\n这个算法计算的是 $1/\\sqrt{n}$，也就是平方根的倒数。同样，如果是你来实现这个算法，你会怎么做呢？得益于下面的代码实现，这个问题才能广为流传，成为技术人员茶余饭后的美谈。我第一次看到这个代码时，内心久久不能平复，或许那天是个阳光正好的上午，但我的心情肯定不是很好，至少几声发自心底的卧槽是少不了的。\n图 2 神一般存在的 sqrt 函数 [2]\n或许这个项目的其他研发人员在 Code review 时，和我的心情是一样的，否则不会有第 11 行的 “what the fuck?” 这么具有个人感情倾向的注释。\n值得补充的一点是，QUAKE 的开发商 ID SOFTWARE 遵守 GPL 协议，开源了 QUAKE-III，我们才能有幸目睹这个传奇的 3D 引擎的原代码。这个项目的核心研发人员是可以追溯到的，而且肯定是他们其中的一个写的，但是作者就是死不承认。这段代码的作者至今仍然是一个谜，就和区块链的真正创始人一样，人们只能依靠自己的想象和在一次次的感叹中表达自己的崇拜。\n在一次采访中，曾经的项目主管在回忆此事时说出了如下这段话：\nWhich actually is doing a floating point computation in integer - it took a long time to figure out how and why this works, and I can\u0026rsquo;t remember the details anymore.\n如果他没有骗人的话，我们可以推测，这个神秘的数字就是先猜出来的，然后经过测试和优化，最终确定了一个最好的数字。因此在后来的代码 Review 时，研发人员只能抓狂的写下 “what the fuck？” 这样的代码注释。当然，也可能是先算出来的，这群天才就是太谦虚，不想忍受凡人的膜拜而已。\n如果是要我选出一个迄今为止对自己影响最大的算法，Quake 3 Arena 中的 Q_rsqrt() 能毫无疑问的当选。这个程序启发了我对计算机科学更加深入的思考，一个伟大的程序，是一位计算机科学家（11行）和一位数学家（13行）智慧的结晶。\n0x5f3759df，这一串神秘的 16 进制在计算机中表示一个32位的数字。关于它在程序中的作用，网络上已经有大篇文章研究得非常透彻，这里我只做一些简单的解释，更多的细节感兴趣可以在网络上搜索。\n非理工科专业的同学只需要知道，通过使用 0x5f3759df 这个神秘数字，能够得到一个非常接近 $1/\\sqrt{n}$ 的值。之后在此基础上只需要再使用一次牛顿迭代法，就可以求解出一个非常精确的值。普渡大学的数学家 Chris Lomont 看了这个神秘数字后之后，很感兴趣，他在精心研究之后从理论上严格推导出了一个最佳值[3]：0x5f37642f。Lomont 对于他计算出来的结果非常满意，于是拿自己计算出的数字（0x5f37642f）和神秘数字（0x5f3759df）做比赛，看看谁的数字能够更快更精确的求得平方根。结果他输了～。可能他非常生气，然后使用暴力的方式得出另外的数字 0x5f375a86，但是结果也只比神秘数字好那么一丢丢，赢得很不开心。\n图3 计算平方根倒数的神秘数字**[3]**\n关于这段神秘的代码，就暂时先告一段落。\n或许高级语言的编译器已经在我们看不见的地方帮我们做好了所有的工作，即使丑陋的代码运行起来表现得也不会太差。但是，不能因为这样，就不再去追求那些简洁、快速且优雅的实现。这个世界的普通人太多了，如果我们不是天才，那就要努力吸收天才的思想，有些事情明明可以做到最好，就不要对自己的要求太低。就像写代码，代码编译通过、运行正常绝不是我们追求的目标，这只是最最基础的东西而已。\n3. 0x5f3759df: 高效算法的奥秘 这是我最近一段时间阅读的书，书名叫做《Hacker’s Delight》，中文译本为《算法心得：高效算法的奥秘》。正是由于这本书，才有了这一篇读书报告。\n这本书是算法领域最有影响力的著作之一（我最近才知道），与大师高纳德（Donald Knuth）所著的《计算机程序设计艺术》共同被誉为所有程序员都应该阅读的计算机著作。本书基本上涉及了目前计算机编程中的所有能用二进制加减乘除和位运算解决的问题。\n写代码时常常苦于乘法操作频繁溢出、开方算法过于复杂，有些小段代码，仅两三行即能解决平常数十行代码方能实现的功能、只用 0x24924925 这般神奇的数字，就能成倍的提升运算速度。从这本书中，我找到了答案。\n4. 总结 这份读书报告，我本来的是想写一写二进制，然后过渡到计算机编程中的位操作算法的，但是写着写着有点偏了。关于《算法心得：高效算法的奥秘》这本书，我感觉内容不太适合写在读书报告里，这本书的读书笔记我打算发布在我的个人网站上（https://landodo.github.io/）。\n 参考文献 [1] Google. Gottfried Wilhelm Leibniz\u0026rsquo;s 372nd Birthday[EB/OL]. 2018.\nhttps://www.google.com/doodles/gottfried-wilhelm-leibnizs-372nd-birthday.\n[2] Software, R.f., Origin of Quake3\u0026rsquo;s Fast InvSqrt()[EB/OL]. 2006.\nhttps://www.beyond3d.com/content/articles/8/\n[3] Lomont C. Fast inverse square root[J]. Tech-315 nical Report, 2003, 32.\n[4] 算法心得：高效算法的奥秘[M]. 北京：机械工业出版社，2014\n","permalink":"http://landodo.github.io/posts/20201207-hackers-delight/","summary":"Hacker’s Delight——高效算法的奥秘 1. 二进制：Google的浪漫 先从一张图片开始。如上图 1 是 Google 在 2018 年 7 月 1 日的首页涂鸦，当天是戈","title":"Hacker’s Delight——高效算法的奥秘"},{"content":"Deep learning Review 深度学习综述\n  深度学习：一个含多个处理层组成的计算模型学习多层次抽象的数据表示。\n  应用：语音识别、视觉对象识别、对象检测以及药物发现和基因组学等领域。\n  反向传播算法（backpropagation algorithm）来改变模型内部的参数，发现数据集中的复杂结构。\n  卷积神经网络（Deep convolutional nets）在处理图像、视频、语音和音频方面取得了突破性的进展。\n  循环网络（recurrent nets）在文本和语音等序列数据方面大放异彩。\n  网络搜索、内容过滤、推荐系统、识别图像中的对象、将语音转成文本、将新闻项目、帖子或产品与用户的兴趣进行匹配，这些应用广泛的应用一种叫做深度学习的技术（deep learning）。\n  深度学习的关键点是，特征层不是由人类工程师设计的：它们是使用通用的学习程序从数据中学习的。\n The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.\n 深度学习非常擅长发现高维数据中的复杂结构。\n It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government.\n 深度学习只需要很少的手工工程。\n it requires very little engineering by hand.\n Supervised learning 以与梯度向量相反的方向调整权重向量。\n The weight vector is then adjusted in the opposite direction to the gradient vector.\n 负梯度向量表示最陡峭的下降方向，朝这个方向修正，输出误差平均较低。\n The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.\n  使用随机梯度下降 SGD  完成训练后，在测试集上进行评估。\n After training, the prefermance of the system is measured on a different set of example called a test set.\n  泛化能力（generalization ability）  多层神经网络和反向传播\n线性分类器或者其他浅层的网络对微小的细节不敏感，而且需要好的特征提取器才能工作得更好。\n深度学习对细小的细节敏感（区分萨摩耶犬和白狼），对大的不相关的变化如背景、姿势、灯光和周围的物体不敏感。\n system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations such as the background, pose, lighting and surrounding objects.\n Backpropagation to train multilayer architectures   使用可训练的多层网络取代手工设计的特征，通过简单的随机梯度下降来训练。\n  反向传播过程是导数链式规则（chain rule）的实际应用。\n  目前，最流行的非线性函数是整流线性单元（ReLU），$f(z)=max(z，0)$。在过去的几十年里，神经网络使用了更平滑的非线性函数，如 $tanh(z)$ 或 $sigmoid(z)$。ReLU 通常在具有许多层的网络中学习速度更快。\n  20 世纪 90 年代末，神经网络和反向传播在很大程度上被机器学习界所抛弃。那时普遍认为，梯度下降法会陷入局部最小值。\n  对于大型的网络，几乎不会陷入局部最小值，更多遇到的是鞍点（saddle points）。算法并不会卡在鞍点。\n  2006 年，deep feedforward networks 开始恢复生机。\n  预训练方法的第一个主要应用是在语音识别，使得能够以 10～20 倍的速度训练网络。\n  对于较小的数据集，无监督的预训练有助于防止过拟合。\n  卷积神经网络 \u0026ldquo;convolutional neural network (ConvNet)\u0026rdquo; 比全连接网络更容易训练和泛化。\n  卷积神经网络 Convolutional neural networks  ConvNets 被设计用来处理以多个数组形式出现的数据。1-D 的信号和序列、2-D 的图像或音频谱图、3-D 的视频等。 特点：局部连接、参数共享、池化和使用多层。 ConvNets 的结构如图 2，前几个阶段由卷积层和池化层组成。卷积层中的单元以特征图（feature map）的形式组织，一个特征图对应一个卷积核。 一个图案可能出现在图像的任何地方，可以共用一个卷积核。 卷积层的作用是检测前一层特征的局部连接。  池化层的作用是将语义上相似的特征合并为一个特征。\n the role of the pooling layer is to merge semantically similar features into one\n  Conv/ReLU/Pooling 堆叠起来，最后是全连接层。反向传播可以训练所有的卷积核参数。  高层次的特征是通过组成较低层次的特征来获得的。\n higher-level features are obtained by composing lower-level ones\n Image understanding with deep convolutional networks  自 2000 年以来，ConvNets 被成功地应用于图像中物体和区域的检测、分割和识别。 图像可以进行像素级别的标记。 类似 Mobileye 和 NVIDIA 等公司在他们即将推出的汽车视觉系统中使用基于 ConvNet 的方法。 尽管 ConvNets 取得了不少的成绩，但是在当时还是不被看好。直到 2012 年的 ImageNet 挑战赛，ConvNets 取得了压倒性的胜利。 它成功源自：GPU 的高效使用、ReLU 激活函数、Dropout 正则化方法以及数据增强技术。这带来了计算机视觉的革命。 ConvNets 现在几乎是所有识别和检测任务的主流方法。下图将 ConvNets 和 RNN 结合的应用。  Distributed representations and language processing PS：每一篇论文都有让人懵逼的部分。\n深度学习理论表明，与不使用分布式表示的经典学习算法相比，深度网络有两个不同的指数优势。\n 学习分布式表示能够泛化到学习特征值的新组合，超出训练过程中看到的组合。 深度指数（exponential in the depth）  多层神经网络的隐藏层学习以一种易于预测目标输出的方式来表示网络的输入。\n one-of-N 向量 网络的其他层学习将输入的单词向量转换为预测下一个单词的输出单词向量，它可以用来预测词汇中任何单词作为下一个单词出现的概率。 词向量由学习到的特征组成，这些特征不是由专家提前确定的，而是由神经网络自动发现的。Tuesday and Wednesday 的词向量相似、Sweden and Norway 的词向量相似。 神经网络使用 big activity vectors、大的权重矩阵和标量非线性来进行推理。  在引入神经语言模型之前，语言统计建模的标准方法是基于对长度为 N 的短字符序列（称为 N-grams）的出现频率进行计数。\n N-grams 表示的两个词没有什么联系。 神经语言模型则将每个单词与实值特征的向量相关联，语义相关的单词在该向量空间中相互接近。  RNN(Recurrent neural networks) 对于涉及顺序输入的任务，如语音和文本，使用 RNNs 更好。\nRNNs 每次只处理一个元素的输入序列，在它们的隐藏单元中维护着一个状态向量，隐含着序列中所有过去元素的历史信息。\n RNNs 在时间上展开（图5），就可以看作是非常深的前馈网络，其中所有层都共享相同的权重。 训练 RNNs 时，反向传播的梯度在每个时间增大或缩小，即梯度爆炸或梯度消失。 解决梯度问题的方法是使用特殊的隐藏单元的长短期记忆（LSTM）网络。 LSTM 网络或相关形式的门控单元也被用于机器翻译中表现出色的编码和解码网络。  深度学习的未来  无监督学习激起了人们对深度学习的兴趣，预计从长远来看，无监督学习将变得更加重要。 预计未来视觉领域的大部分进展将来自于端到端训练的系统，并将 ConvNets 与使用强化学习来决定看哪里的RNNs 相结合。 深度学习将会在自然语言理解领域产生重大影响。 人工智能的重大进展将通过结合表征学习和复杂推理的系统来实现。  ","permalink":"http://landodo.github.io/posts/20201124-deeplearning-review/","summary":"Deep learning Review 深度学习综述 深度学习：一个含多个处理层组成的计算模型学习多层次抽象的数据表示。 应用：语音识别、视觉对象识别、对象检测以及药物发现和基因","title":"Deep learning Review"},{"content":"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ✅ 论文地址：https://arxiv.org/pdf/1409.1556.pdf\n✅ 发表时间：2015 年（Published as a conference paper at ICLR 2015）\nVGG 名称的来源：Visual Geometry Group, Department of Engineering Science, University of Oxford.\nABSTRACT 论文研究了在大规模图像识别中卷积网络的深度对其精度的影响。\n使用 $3\\times3$ 的卷积核。\n网络深度提升到了 16-19层。\n获得了 2014 ImageNet Challenge 的 localistion 和 classification 的第一名和第二名。（分类任务的第一名是 GoogLeNet）\n1 INTRODUCTION 卷积网络（ConvNets）最近在大规模图像图像和视频识别方面取得了巨大的成功。\nImageNet 大规模视觉识别挑战赛（ILSVRC）在推进深度视觉识别架构方面发挥了重要作用。\nConvNets 目前大多数人已经尝试的改进方式：\n 利用了较小的接收窗口尺寸和较小的第一卷积层步幅（ smaller receptive window size and smaller stride of the first convolutional layer.） 在整个图像和多个尺度上密集地训练和测试网络（training and testing the networks densely over the whole image and over multiple scales.）  本篇论文的工作是固定网络的结构的其他参数，使用 $3\\times3$ 的卷积核，增加更多的卷积层来增加网络的深度。\n we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers.\n 提出了精度更高的 ConvNets 架构，不仅在 ILSVRC 的 Classification 和 Localisation 任务上达到了最高的准确度，而且还适用于其他图像识别的数据集。\n2 CONVNET CONFIGURATIONS 网络结构  网络输入的图片尺寸是 $224\\times224$ 预处理：每个像素中减去在训练集上计算的平均 RGB 值 卷积核大小为 $3\\times3$，还有 $1\\times1$ 可以看作是输入通道的线性变换 卷积的步幅（Stride）固定为 1 像素（pixel） 有 5 个最大池化层，并非每个卷积层后面都接着一个最大池化层。最大池化的窗口为 $2\\times2$，步幅为 2 不同的网络架构，卷积层的深度不相同 卷积层最后接着 3 个全连接层，前两个有 4096 个通道，最后是一个 1000 的 ILSVRC 分类（Softmax）。 所有的隐藏层都使用 ReLU 激活函数 摒弃了 LRN， Section 4 实验证实了 LRN 不会提高 ILSVRC 数据集的性能  6 个网络 A、A-LRN、B、C、D、E 这个 6 网络只在深度上有所不同，其他的都采用通用的设计。\n网络深度越深，参数越多。尽管深度很大，但是网络的参数并没有大很多。\n In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in [Sermanet et al., 2014]).\n 网络更细致讨论 整个网络中使用 $3\\times3$ 的感受野（receptive fields）。\n两个 $3\\times3$ 的 conv. 层的堆叠（中间没有池化）的有效感受野为 $5\\times5$。（画一画就理解了）\n三个这样的层堆叠的有效感受野为 $7\\times7$。\n通过使用三个 $3\\times3$ conv. 层的堆叠而不是只使用一个 $7\\times7$ 层，有什么好处呢？\n more discriminative.（怎么翻译？） 具有更少的参数（怎么算的，C 为什么要平方？）  表 1 的 C 网络，使用了 $1\\times1$ 的卷积。$1\\times1$ 卷积本质上是对相同维度空间的线性投影，但引入了额外的非线性。\nGoodfellow 等人将深度 ConvNets （11 个权重层）应用于街号识别的任务中，得出结论：深度的增加带来了更好的性能。\nGoogLeNet 是 ILSVRC-2014 分类任务的第一名。\n本篇论文的模型在单网分类精度方面优于 GoogLeNet。（Section 4.5，啥是单网分类？）\n our model is outperforming that of [Szegedy et al.(2014)] in terms of the single-network classification accuracy.\n 分类评估细节 训练 TRAINING  Momentum Mini-batch 梯度下降 batch size=256 momentum 参数为 0.9 权重衰减（L2 正则化 $5\\times10^{-4}$）和 Dropout 正则化（丢弃率为 0.5） 学习率初始为 0.01。学习率衰减：精度停止提升时，学习率减少 10 倍，总共减少 3 次。 74 个 epochs  虽然比 AlexNet 更深，但是由于：\n 更小一些的卷积核 conv. 带来的正则化效果 某些层的预初始化  网络将收敛的更快。\n权重的初始化非常重要，初始化不好使得深层网络中梯度的不稳定而导致学习停滞。论文中 A 结构比较浅，因此可以使用均值为 0、方差为 0.01 的高斯分布进行随机初始化，偏置则可以初始化为零。\n数据增强与 AlexNet 相同，随机裁剪 $224\\times224$， random horizontal flipping and random RGB colour shift。\nS 被称为训练规模（we also refer to S as the training scale）。如果 S = 224，那么裁剪到的是整张图像；如果 $S\\ll 224$ ，裁剪到的将是图像的一小部分。\n S = 256 广泛使用，如（AlexNet，GoogLeNet）。首先训练网络时使用 S=256，在训练 S=384 时，用 S = 256 预先训练的权重进行初始化。 从 $[S_{min}, S_{max}]$ 随机抽取（$S_{min} = 256, S_{max}=512$）。识别目标可能这图像的任何位置，因此这是非常有益的。规模抖动的训练集增强。  测试 TESTING 一个训练好的 ConvNet 和输入图片。\n 输入图片缩放到一个 Q（Q 测试尺度 scale，不一定等于 S。对每个 S 使用不同的 Q 可以提升性能） 全连接层转换为卷积层（第一个 FC 层转换为 $7\\times7$ conv. 层，最后两个 FC 层转换为 $1\\times1$ conv. 层） 将得到的网络应用于未裁剪的图像。 结果是一个通道数量等于类数量的类分数映射，空间分辨率可变，取决于输入图像的大小。 可以通过通过水平翻转图像来对测试集进行数据增强。  全卷积网络是应用在整个图像上的，因此不需要在测试时对多个 crops 进行采样。\n使用大量的 crops，可以提升精度，与完全卷积网络相比，它可以对输入图像进行更精细的采样。当将 ConvNet 应用于一个 crop 时，卷积的特征图会被填充为零。\n注：crop 即裁剪的意思。\nPS：这一小节我没看懂。\n实现细节 基于开源的 C++ Caffe toolbox 实现，但是进行的修改，使得能够使用多 GPU 并行 ，以及在多个 scales 的全尺寸（未裁剪）图像上进行训练和评估。\n每 batch 的训练图像分割成多个 GPU batch，在每个 GPU 上并行处理来进行。在计算完 GPU batch 梯度后，对它们进行平均，得到所有 batch 的梯度。梯度计算是在各个 GPU 上同步进行的，所以结果和在单个 GPU 上训练时的结果是完全一样的。\n在配备 4 个 NVIDIA Titan Black GPU 的系统上，根据架构的不同，训练一个网络需要 2-3 周的时间。\n分类实验 上述的 ConvNet 架构在 ILSVRC-2012 数据集上实现的图像分类结果。\n这个数据集包括 1000 个类的图像。\n 训练集（130 万张) 验证集（5 万张) 测试集（10 万张带有保留类标签的图像）。  分类性能的评估采用两个衡量标准：top-1 和 top-5 误差。（这两个指标我在 AlexNet 论文笔记中有说明）\nILSVRC-2014 比赛的 \u0026ldquo;VGG\u0026rdquo; 团队。\n4.1 SINGLE SCALE EVALUATION 对于固定的 S，Q = S 和抖动的 $S ∈ [S_{min}，S_{max}]$，Q = 0.5($S_{min}+S_{max}$)。结果如下表：\n A 和 A-LRN 的结果说明，LRN 没有多大的用处。 分类误差随着层数的加深而降低，A（11 层）到 E（19 层） B 和 C 的结果说明，加入非线性层对精度提升有一定的帮助 C 和 D 的层数相同，C 包含一些 $1\\times1$ 的卷积核，D 全部都是 $3\\times3$。$3\\times3$ 能捕捉更多的上下文信息。$3\\times3$ 论文中称为 filters with non-trivial receptive fields 深度达到 19 层时，错误率会达到饱和，更深的模型可能需要大的数据集  两个 $3\\times3$ 与一个 $5\\times5$ 具有相同的感受野。将 B 网络中的每对 $3\\times3$ 替换为单个 $5\\times5$，替换后的网络的 top-1误差比 B 的 top-1 误差高 7%。\n使用 $S ∈ [S_{min}，S_{max}]$ 相比固定 S 来说更优，证实了通过尺度抖动（scale jittering ）的训练集增强确实有助于捕捉多尺度的图像统计数据。\n4.2 MULTI-SCALE EVALUATION 评估测试时尺度抖动的对模型的影响。\n $Q={S-32, S, S+32}$ 训练时的尺度抖动使得网络在测试时可以应用于更大范围的尺度， 用可变 $S∈[S_{min},S_{max}]$ 训练的模型在更大范围的尺寸上进行评估 $Q={S_{min}, 0.5(S_{min}+S_{max}), S_{max}}$  测试时的规模抖动（scale jittering）会带来更好的性能（和 Table 3 一起看）。测试集上，网络 E 实现了 7.3% 的 top-5 错误了。\n4.3 MULTI-CROP EVALUATION dense 和 multi-crop 两种方法是互补的。单独来看，后者相对较优，两者的组合比单独要更优。\n4.4 多模型结合 结合各网络的输出，对 Softmax 的值进行平均，能提升模型的性能。\nTable 5 中，表现最好的单一模型实现了 7.1% 的 top-5 误差（模型E）。结合 D 和 E 这个两个网络，测试的 top-5 误差降到了 7.0%。\n4.5 与现有的技术进行比较 在 ILSVRC-2014 挑战赛的分类任务中，本篇论文 VGG 团队使用 7 个网络模型的集合，以 7.3% 的测试误差获得了第 2 名。过后，使用 D 和 E 模型的结合将错误率降低到 6.8%。\n与分类任务的第一名（GoogLeNet，错误率为 6.7%）相比也有竞争力，并且大大超过了 ILSVRC-2013 的冠军 Clarifai。\nVGG 的最佳结果只结合了两个模型来实现，比大多数 ILSVRC 提交的作品中使用的模型要少得多。\n在单网性能方面，VGG 做到了最好（7.0% 的测试误差），GoogLeNet 是 7.9%。（In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. ）\n总结 VGG 使用传统的 ConvNet 架构（LeNet, AlexNet），在大幅增加深度的情况下，在 ImageNet 挑战数据集上取得了最好的成绩。\n模型可以很好地应用到数据集中。\n再次证实了深度在视觉表征中的重要性。\n LOCALISATION（定位） VGG 在 2014 年的 ILSVRC 挑战赛的定位任务，误差为 25.3%。\n定位网络 LOCALISATION CONVNET 训练：\n  使用 D 网络结构（Table 1），VGG-16。\n  最后一个全连接层预测边界框的位置。边界框由一个 4-D 向量表示，存储着中心坐标、宽度和高度。\n  损失函数采用 Euclidean loss\n  S = 256 和 S = 384\n  使用上面的分类模型进行初始化，最后的全连接层进行随机初始化\n  学习率初始化为 0.001\n  测试：\n 只考虑对 ground truth class 对边界框预测（Ground truth 是正确标注的数据） 仅应用于图像的中心裁剪，获得边界框  实验 如果 IOU（intersection over union）大于 0.5，则认为预测的边界框是正确的。\nper-class regression（PCR）\nsingle-class regression（SCR）\n PCR 的表现优于 SCR 全部 fine-tuning 比只 fine-tuning 第一第二个全连接层要好  与只进行中心裁剪相比，将网络应用与整张图像能提升精确率。\n25.3% 的测试误差，VGG 团队赢得了 ILSVRC-2014 挑战赛的 localisation 任务的冠军。比 ILSVRC-2013 的冠军 Overfeat 的结果要好得多，而且 VGG 还有没采用分辨率增强等技术，VGG 还有一定的提升空间。\nGENERALISATION OF VERY DEEP FEATURES 在 ILSVRC 数据集上预先训练一个 ConvNets，然后应用到其他数据集上。\n 去掉最后一个全连接层（1000 类的 Softmax）； 使用倒数第二层的 4096 维的激活值作为图像的特征； 经过 L2 归一化，并与线性 SVM 分类器相结合，在目标数据集上进行训练。  VGG 与其他方法在 VOC-2007、VOC-2012、Caltech-101和 Caltech-256 上的图像分类结果如下表。\nVOC-2007 图像数据集包含 10K 张图像，VOC-2012 包含 22.5K 张。每张图像都被标注了一个或几个标签，对应 20 个对象类别。识别性能采用各类平均精度（mAP）来衡量。\n(Wei et al., 2014) 的方法在 VOC-2012 上的 mAP 相比 VGG 高 1%。这是通过在 2000 类 ILSVRC 数据集上进行预训练，其中包括额外的 1000 个类别，语义上与 VOC 数据集中的类别接近，并且采用了对象检测辅助分类流水线（bject detection-assisted classification pipeline.），因此取得了更好的结果。\nCaltech-101 数据集包含 9K 图像，有 102 个类（101 个对象类别和一个背景类）。Caltech-256 有 31K 图像，257 个类别。\nVGG 在 Caltech-101 数据集上相比何恺明等人的方法稍差一些，但是 VGG 在 VOC-2007 上明显优于何恺明等人。\n在 PASCAL VOC-2012 动作分类任务上，VGG 使用 D\u0026amp;E 结合，与的比较如下表。\nVGG 仅仅依靠非常深的卷积特征的表示能力，就取得了第一名的成绩。\nVGG 广泛的应用于其他的图像识别任务，并且始终优于更浅层的表示。\nPS：对于这些数据集训练和测试的细节我没有做笔记，需要了解直接看论文\n","permalink":"http://landodo.github.io/posts/20201123-paper-vgg/","summary":"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ✅ 论文地址：https://arxiv.org/pdf/1409.1556.pdf ✅ 发表时间：2015 年（Published","title":"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION"},{"content":"HTTP 服务器的一些截图记录 HTML 页面 服务器的 HTML 页面有两个分别是 index.html 和 README.html。这个 css 风格是我最喜欢的，因此也就应用在了这个小项目上。\nREADME.html 是我们小组的成员介绍，以及服务器的一些使用说明。\nGET/POST 文件下载和上传功能 第一个功能：实现 GET/POST 方法，支持文件的上传和下载。\n// GET Method if (!strcmp(method, \u0026#34;GET\u0026#34;)) {  handle_request(cfd, uri); }  // POST Method else if (!strcmp(method, \u0026#34;POST\u0026#34;)) {  handle_post(cfd, buffer); } GET 方法用于请求资源和文件下载等，如下是一个简单的 GET 请求。\nGET /css/style.css HTTP/1.1 Host: 127.0.0.1:8888 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:71.0) Gecko/20100101 Firefox/71.0 Accept: text/css,*/*;q=0.1 Accept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2 Accept-Encoding: gzip, deflate Connection: keep-alive Referer: http://127.0.0.1:8888/ Cache-Control: max-age=0 图片传输失败。\n当上传大文件时，服务器段因为代码写得不够好，导致数据接收不完整。上传顾城的一首诗：\n上传完成：\n服务器端查看数据（书籍还未进行处理，还保留包中的一些内容）：\n上传一个较大的文件，《傲慢与偏见》，可能是服务器端缓存区过小等原因，发生数据缺失，文件上传不完整。\n出现报错。服务器端代码还需要再完善。\n查看一下服务器端所保存下来的数据，是不完整的。\n实现分块传输、持久连接和管道 简单来说：\n  分块传输：Chunked Transfer Coding\n   “分块传输编码会将实体主体分成多个部分（块）。每一块都会用十六进制来标记块的大小，而实体主体的最后一块会使用“0(CR+LF)”来标记。\n使用分块传输编码的实体主体会由接收的客户端负责解码，恢复到编码前的实体主体。\nHTTP/1.1 中存在一种称为传输编码（Transfer Coding）的机制，它可以在通信时按某种编码方式传输，但只定义作用于分块传输编码中。”\n摘录来自 图解 HTTP 上野宣、于均良\n   持久连接：keep-alive，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n  管道：客户端不用等待服务器响应，就可以直接发送下一个请求，即并行发送多个请求。\n   “持久连接使得多数请求以管线化（pipelining）方式发送成为可能。从前发送请求后需等待并收到响应，才能发送下一个请求。管线化技术出现后，不用等待响应亦可直接发送下一个请求。\n这样就能够做到同时并行发送多个请求，而不需要一个接一个地等待响应了。”\n摘录来自 图解 HTTP 上野宣、于均良 此材料受版权保护。\n   HTTPS “HTTP + 加密 + 认证 + 完整性保护 == HTTPS.”\n需要安装 OpenSSL。\nbrew install openssl ➜ ~ which openssl /usr/local/opt/openssl@1.1/bin/openssl 编译的命令，注意参数。\ngcc -g server_ssl.c -o server_ssl -I//usr/local/opt/openssl@1.1/include -L//usr/local/opt/openssl@1.1/lib -lssl -lcrypto libevent libevent 在我的电脑上位于：\n/usr/local/Cellar/libevent/2.1.12 # 编译时加上参数 -levent -lpthread #include \u0026lt;event2/event.h\u0026gt;#include \u0026lt;event.h\u0026gt;#include \u0026lt;pthread.h\u0026gt;代码 // DServer // GET/POST 文件上传和文件下载 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; // exit#include \u0026lt;arpa/inet.h\u0026gt; // inet_ntoa#include \u0026lt;netinet/in.h\u0026gt; // sockaddr_in#include \u0026lt;string.h\u0026gt; // strlen#include \u0026lt;unistd.h\u0026gt; // close function#include \u0026lt;sys/stat.h\u0026gt;#include \u0026lt;sys/errno.h\u0026gt;#include \u0026lt;sys/fcntl.h\u0026gt;#include \u0026lt;netinet/tcp.h\u0026gt; #include \u0026#34;config.h\u0026#34; int start_server(); void handle_request(int cfd, char uri[]); void construct_header(char *header, int status, char *type); void get_filetype(char *filename, char *filetype); const char *get_status_by_code(int status); void handle_post(int cfd, char buffer[]); void request_image(int cfd, char uri[]);  int main() {   int sfd = start_server();   while (1) {  // 4. 服务器接收客户端的连接请求  struct sockaddr_in client_addr;  socklen_t len = sizeof(client_addr);  int cfd = accept(sfd, (struct sockaddr *)\u0026amp;client_addr, \u0026amp;len);  if (cfd == -1) {  perror(\u0026#34;accept() error!\\n\u0026#34;);  exit(EXIT_FAILURE);  }  // DEBUG: 连接成功，打印出客户端的 IP 和端口号  // printf(\u0026#34;client ip: %s, port: %d\\n\u0026#34;, inet_ntoa(client_addr.sin_addr), htons(client_addr.sin_port));   char buffer[MAX_SIZE] = {\u0026#39;\\0\u0026#39;};  ssize_t rbytes = recv(cfd, buffer, sizeof(buffer), 0);  if (rbytes == -1) {  perror(\u0026#34;recv() error!\\n\u0026#34;);  exit(EXIT_FAILURE);  }  printf(\u0026#34;%s\\n\u0026#34;, buffer); // 请求内容   char method[MIN_SIZE] = {\u0026#39;\\0\u0026#39;};  char uri[MIDDLE_SIZE] = {\u0026#39;\\0\u0026#39;};  sscanf(buffer, \u0026#34;%s %s\u0026#34;, method, uri);   // GET Method  if (!strcmp(method, \u0026#34;GET\u0026#34;)) {  handle_request(cfd, uri);  }   // POST Method  else if (!strcmp(method, \u0026#34;POST\u0026#34;)) {  handle_post(cfd, buffer);  }   else {  printf(\u0026#34;Method Not implemented!\\n\u0026#34;);  }  close(cfd);  }  close(sfd);  return 0; } 放一张工程的截图。\n","permalink":"http://landodo.github.io/posts/20201113-http/","summary":"HTTP 服务器的一些截图记录 HTML 页面 服务器的 HTML 页面有两个分别是 index.html 和 README.html。这个 css 风格是我最喜欢的，因此也就应用在了这个小项目上。 README.html 是","title":"使用 C 语言实现一个 HTTP 服务器（2）"},{"content":"CIFAR-10、PyTorch 和 AlexNet 先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。\n这些经典的神经网络结构，要熟悉到能白板编程的技能。\n最近看到有关机器学习的一个推特，觉得很有启发。\n What a lot of machine learning courses \u0026amp; books teach:\n Load clean data Train an MNIST-* classifier with 99% accuracy  What they should actually be teaching:\n Process, clean, load your own data Fail many experiments and research/find/understand ways to improve your ML model   很多机器学习课程与书籍所教授的内容：\n 加载干净的数据 训练一个MNIST-* 分类器，准确率达 99%。  他们其实应该教什么。\n 处理、清理、加载自己的数据 多次实验失败，研究/发现/了解改进你的 ML 模型的方法。   环境准备 第一步：启动虚拟环境\n➜ source activate ldl-env 第二步：安装 PyTorch\n(ldl-env) ➜ workspace conda install pytorch torchvision torchaudio -c pytorch  要多看 PyTorch 的官方文档。\n 数据集 torchvision.datasets torchvision.datasets 中包含了较多的数据集\n  MNIST（0-9 手写数字）\n  COCO（用于图像标注和目标检测）\n  LSUN（包含数百万个场景和对象的彩色图像）\n  CIFAR-10\n  CIFAR-10 dataset (Canadian Institute For Advanced Research) ：https://www.cs.toronto.edu/~kriz/cifar.html\nCIFAR-10 和 CIFAR-100 是一个包含 8000 万张微小图像的数据集，由 Alex Krizhevsky、Vinod Nair 和 Geoffrey Hinton 收集。\nCIFAR-10 数据集包含 10 个类别，总共 60000 张 $$32\\times32$$ 彩色图像。\n每个类别 6000 张图像。\n有 50000 张训练图像（train set）和 10000 张测试图像（test set）。\n数据集分为 5 个训练 Batch 和 1 个测试 Batch。每个Train Batch 有 10000 张图像，Test Batch 正好包含 1000 张从每个类中随机选取的图像。 Train Batch 按随机顺序包含剩余的图像，但一些 Train Batch 可能包含来自一个类的图像比另一个类更多。在它们之间，Train batch 正好包含来自每个类的 5000 张图像。\n Fashion-MNIST  GitHub: https://github.com/zalandoresearch/fashion-mnist\n每个训练和测试样本都按照以下类别进行了标注：\n   标注编号 描述     0 T-shirt/top（T恤）   1 Trouser（裤子）   2 Pullover（套衫）   3 Dress（裙子）   4 Coat（外套）   5 Sandal（凉鞋）   6 Shirt（汗衫）   7 Sneaker（运动鞋）   8 Bag（包）   9 Ankle boot（踝靴）    找到了一个可供参考的小项目，数据集是 CIFAR-10。\nAlexNet GitHub：pytorch-cifar10 输入图片的大小为 $$32\\times32\\times3$$\nclass AlexNet(nn.Module):  def __init__(self, num_classes=NUM_CLASSES):  super(AlexNet, self).__init__()  self.features = nn.Sequential(  nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=2),  nn.Conv2d(64, 192, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=2),  nn.Conv2d(192, 384, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.Conv2d(384, 256, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.Conv2d(256, 256, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=2),  )  self.classifier = nn.Sequential(  nn.Dropout(),  nn.Linear(256 * 2 * 2, 4096),  nn.ReLU(inplace=True),  nn.Dropout(),  nn.Linear(4096, 4096),  nn.ReLU(inplace=True),  nn.Linear(4096, num_classes),  )   def forward(self, x):  x = self.features(x)  x = x.view(x.size(0), 256 * 2 * 2)  x = self.classifier(x)  return x 我按照吴恩达老师的视频教程，绘制了一个图，如下。\nPapers with Code 这里的图片输入应该是 $$227\\times227\\times3$$，所以网络的内部 kernel_size 和 stride 会有不同。\nclass AlexNet(nn.Module):  \u0026#34;\u0026#34;\u0026#34; Neural network model consisting of layers propsed by AlexNet paper. \u0026#34;\u0026#34;\u0026#34;  def __init__(self, num_classes=1000):  \u0026#34;\u0026#34;\u0026#34; Define and allocate layers for this neural net. Args: num_classes (int): number of classes to predict with this model \u0026#34;\u0026#34;\u0026#34;  super().__init__()  # input size should be : (b x 3 x 227 x 227)  # The image in the original paper states that width and height are 224 pixels, but  # the dimensions after first convolution layer do not lead to 55 x 55.  self.net = nn.Sequential(  nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4), # (b x 96 x 55 x 55)  nn.ReLU(),  nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), # section 3.3  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 96 x 27 x 27)  nn.Conv2d(96, 256, 5, padding=2), # (b x 256 x 27 x 27)  nn.ReLU(),  nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 13 x 13)  nn.Conv2d(256, 384, 3, padding=1), # (b x 384 x 13 x 13)  nn.ReLU(),  nn.Conv2d(384, 384, 3, padding=1), # (b x 384 x 13 x 13)  nn.ReLU(),  nn.Conv2d(384, 256, 3, padding=1), # (b x 256 x 13 x 13)  nn.ReLU(),  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 6 x 6)  )  # classifier is just a name for linear layers  self.classifier = nn.Sequential(  nn.Dropout(p=0.5, inplace=True),  nn.Linear(in_features=(256 * 6 * 6), out_features=4096),  nn.ReLU(),  nn.Dropout(p=0.5, inplace=True),  nn.Linear(in_features=4096, out_features=4096),  nn.ReLU(),  nn.Linear(in_features=4096, out_features=num_classes),  )  self.init_bias() # initialize bias   def init_bias(self):  for layer in self.net:  if isinstance(layer, nn.Conv2d):  nn.init.normal_(layer.weight, mean=0, std=0.01)  nn.init.constant_(layer.bias, 0)  # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers  nn.init.constant_(self.net[4].bias, 1)  nn.init.constant_(self.net[10].bias, 1)  nn.init.constant_(self.net[12].bias, 1)   def forward(self, x):  \u0026#34;\u0026#34;\u0026#34; Pass the input through the net. Args: x (Tensor): input tensor Returns: output (Tensor): output tensor \u0026#34;\u0026#34;\u0026#34;  x = self.net(x)  x = x.view(-1, 256 * 6 * 6) # reduce the dimensions for linear layer input  return self.classifier(x) Zhihu 知乎 https://zhuanlan.zhihu.com/p/29786939\n给出了局部响应归一化的实现。\nimport torch.nn as nn from torch.nn import functional as F from torch.autograd import Variable  class LRN(nn.Module):  def __init__(self, local_size=1, alpha=1.0, beta=0.75, ACROSS_CHANNELS=False):  super(LRN, self).__init__()  self.ACROSS_CHANNELS = ACROSS_CHANNELS  if self.ACROSS_CHANNELS:  self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1), #0.2.0_4会报错，需要在最新的分支上AvgPool3d才有padding参数  stride=1,  padding=(int((local_size-1.0)/2), 0, 0))  else:  self.average=nn.AvgPool2d(kernel_size=local_size,  stride=1,  padding=int((local_size-1.0)/2))  self.alpha = alpha  self.beta = beta    def forward(self, x):  if self.ACROSS_CHANNELS:  div = x.pow(2).unsqueeze(1)  div = self.average(div).squeeze(1)  div = div.mul(self.alpha).add(1.0).pow(self.beta)#这里的1.0即为bias  else:  div = x.pow(2)  div = self.average(div)  div = div.mul(self.alpha).add(1.0).pow(self.beta)  x = x.div(div)  return x 这个专门分了 layer1， layer2， layer3 ，更加的好理解了。\nfrom torch import nn from torch.nn import functional as F from torch.autograd import Variable  import torch class AlexNet(nn.Module):  def __init__(self, num_classes = 1000):#imagenet数量  super().__init__()  self.layer1 = nn.Sequential(  nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=3, stride=2),  LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)  )   self.layer2 = nn.Sequential(  nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, groups=2, padding=2),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=3, stride=2),  LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)  )   self.layer3 = nn.Sequential(  nn.Conv2d(in_channels=256, out_channels=384, padding=1, kernel_size=3),  nn.ReLU(inplace=True)  )  self.layer4 = nn.Sequential(  nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),  nn.ReLU(inplace=True)  )   self.layer5 = nn.Sequential(  nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=3, stride=2)  )   #需要针对上一层改变view  self.layer6 = nn.Sequential(  nn.Linear(in_features=6*6*256, out_features=4096),  nn.ReLU(inplace=True),  nn.Dropout()  )  self.layer7 = nn.Sequential(  nn.Linear(in_features=4096, out_features=4096),  nn.ReLU(inplace=True),  nn.Dropout()  )   self.layer8 = nn.Linear(in_features=4096, out_features=num_classes)   def forward(self, x):  x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))  x = x.view(-1, 6*6*256)  x = self.layer8(self.layer7(self.layer6(x)))   return x 思考 我有几个疑问？\n  AlexNet 论文里面的数据集使用的是 $$256\\times256\\times3$$，采用数据增强后真正输入到网络的图像 size 为 $$224\\times224\\times3$$，但是编程实现的时候，输入的大多是 $$227\\times227\\times3$$ 这个 227 是怎么算出来的？\n   2021.02.05：224 应该是作者的一个失误，其实没关系的。将 224 代入网络，就会发现 224 存在的问题，227 是一个更好的取值。\n     我感觉有人用 224，有人用 227，有什么区别吗？\n   2021.02.05：用 227，不要用 224。代入网络推一遍就知道为什么了！\n     上文我记录的第一个程序 pytorch-cifar10，每张图像的输入是 $$32\\times32\\times3$$。这种情况下一种处理方式是 resize 成 224；另一种是更改网络的 kernel_size、padding、stride，修改过之后的网络还能叫做是 AlexNet 吗？\n   2021.02.05：我认为修改后的网络就不能叫做 AlexNet 了。\nAlexNet 中的卷积核大小、步长、填充都是作者为 ImageNet 数据集精心设计的。\n     ","permalink":"http://landodo.github.io/posts/20201109-alexnet-code/","summary":"CIFAR-10、PyTorch 和 AlexNet 先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。 这些经典的神经网络结构，要熟悉","title":"CIFAR-10、PyTorch 和 AlexNet"},{"content":"Network In Network 论文阅读 ✅ 论文地址：https://arxiv.org/pdf/1312.4400.pdf\n✅ 论文发表时间：2014 年\n论文提出了一种名为 “Network In Network” (NIN) 的神经网络结构。与传统 CNN 不同的是，NIN 建立了更加复杂的微神经网络来抽象感受野中的数据。\n传统 CNN 是通过滑动 Filter 来获取特征图，而 NIN 则是通过滑动一个微小的神经网络来获取，然后传递到下一层。\n通过微网络增强局部建模，能够在分类层中利用特征图上的全局平均池化，相比于传统的全连接层的表现力更强，也更不容易过拟合。\nNIN 在 CIFAR-10、CIFAR-100、SVHN 和 MNIST 数据集上都取得了不错的性能。\n1 Introduction 卷积神经网络（CNNs）由交替的卷积层和池化层组成，卷积是线性滤波器和输入的感受野的內积，在输入的每一个局部部分进行非线性激活函数。由此产生的输出称为特征图（feature maps）。\nCNN 中的卷积核是一个 generalized linear model (GLM) ，论文作者认为 GLM 的抽象能力很低，而使用更有力的非线性函数逼近器（nonlinear function approximator）代替 GLM 可以提高局部模型的抽象能力。\n GLM can achieve a good extent of abstraction when the samples of the latent concepts are linearly separable, i.e. the variants of the concepts all live on one side of the separation plane defined by the GLM. Thus conventional CNN implicitly makes the assumption that the latent concepts are linearly separable. However, the data for the same concept often live on a nonlinear manifold, therefore the representations that capture these concepts are generally highly nonlinear function of the input.\nXception 也是从 NIN 中获得了启发。\n 在 NIN 中，GLM 被替换为\u0026quot;微网络\u0026quot;结构，选择多层感知器作为微网络的实例，也是一个可通过反向传播进行训练的神经网络。\nNIN 中的结构称之为 mlpconv 层。\nmlpconv 使用一个多层感知器（MLP）将输入的局部 patch 映射到输出特征向量，该多层感知器由多个具有非线性激活函数的全连接层组成。\nNIN 的整体结构是多个 MLPconv 层的叠加，所以被称为 \u0026ldquo;Network In Network\u0026rdquo;（NIN）。\nNIN 没有采用传统 CNN 的全连接层进行分类，而是通过全局平均池层（global average pooling layer）直接输出上一个 mlpconv 层的特征图的空间平均值作为类别的置信度，然后将得到的向量送入 softmax 层。\n相比于全连接层，全局平均池化（GAP）更有意义和可解释性，GAP 强制要求特征图和类别之间的对应关系，这是由使用微网络的更强的局部建模所实现的。GAP 本身就是一个结构正则化器，这防止了整体结构的过拟合。\n2 Convolutional Neural Networks 良好抽象的表示通常是输入数据的高度非线性函数，如果使用传统的 CNN 结构，可能需要一系列的卷积核来提取抽象的特征。使用多个卷积核是一个很多负担。\n最近的 Maxout 网络通过在非线性特征图上的最大池化来减少特征图的数量（非线性特征图是不应用激活函数的线性卷积的直接结果），这一改进使得 Maxout 具有更高的性能。\n ✅ instances of a latent concept 这一名词反复出现，但是我不知道怎么翻译它？\n由于 instances of a latent concept，传统的 CNNs 、Maxout 都表现的不好。\n 当 instances of a latent concept 分布复杂时，Maxout 的表现就不太好。这就需要采用一个更通用的函数逼近器来计算局部 patch 的更抽象的特征。\n结构化多层感知器（Structured Multilayer Perceptron，SMLP）将一个共享的多层感知器应用于输入图像的不同 patch 上。NIN 将微观网络集成到 CNN 结构中，对各级特征进行更好的抽象。\n3 Network In Network Network In Network 结构的关键部分：MLP 卷积层和全局平均池化层（MLP convolutional layer and the global averaging pooling layer）。\n3.1 MLP Convolution Layers  Given no priors about the distributions of the latent concepts. （传统结构和 maxout 的劣势）\n 径向基网络（Radial basis network）和多层感知器（multilayer perceptron）是两个著名的通用函数逼近器。\n论文使用的是多层感知机：\n 多层感知器与卷积神经网络的结构相适应，它同样采用反向传播进行训练。 多层感知器本身可以是一个深度模型，这符合特征重用的思想。  如下图 1 是线性卷积层和 mlpconv 层的区别。\nmlpconv 层进行的计算如下：\n$n$ 是多层感知器的层数，多层感知器中使用 $ReLU$ 激活函数。\n公式 (2) 体现了级联式的跨通道参数池化结构可以实现跨通道信息的复杂和可学习的交互。\n跨通道参数池化层相当于 1x1 卷积核的卷积层（只获取通道的相关性，不考虑空间相关性。理解 NIN 的结构）。\n**与 maxout 层相比：**maxout 网络中的 maxout 层在多个非线性特征图上进行最大池化。maxout 层的特征图计算如下：\n3.2 Global Average Pooling 传统的卷积神经网络在网络的低层进行卷积。对于分类来说，最后一个卷积层的特征图被矢量化，并送入全连接层中，然后是一个 softmax 逻辑回归层（AlexNet/Maxout）。这种结构是卷积结构与传统神经网络分类器的桥梁,它将卷积层视为特征提取器，并以传统方式对产生的特征进行分类。\n全连接层容易出现过拟合，从而影响了整个网络的泛化能力。\n这篇论文提出了一种称为**全局平均池化（global average pooling）**的策略来取代 CNN 中传统的全连接层。\n其思想是在最后一个 mlpconv 层中为分类任务的每个对应类别生成一个特征图，取每个特征图的平均数，所得向量直接送入 softmax 层。\n相比于 FC，GAP 的优势：\n 通过强制执行特征图和类别之间的对应关系，更原生地融入卷积结构。因此特征图可以很容易地解释为类别置信图。 全局平均池化没有参数需要优化，因此在这一层避免了过拟合。 全局平均池化对空间信息进行求和，因此它对输入的空间转换更加稳健。  可以将全局平均池化作为一种结构正则化器，明确地将特征图强制为概念（类别）的置信图。\n3.3 Network In Network Structure NIN 的整体结构是一个 mlpconv 层的堆叠。图 2 显示了一个具有三个 mlpconv 层的 NIN。\n4 Experiments 在 CIFAR-10、CIFAR-100、SVHN 和 MNIST 数据集上进行评估。\n使用的一个正则化是 Krizhevsky 等人（AlexNet）使用的权重衰减。\n4.2 CIFAR-10 8.81% 的测试误差是当下最好的性能。\n在实验中发现，在 NIN 的 mlpconv 层之间使用 dropout 可以提高模型的泛化能力，从而提升网络的性能。\n4.3 CIFAR-100 4.4 Street View House Numbers SVHN 数据集由 630,420 张 32x32 彩色图像组成。\nNIN 在这个数据集上的测试错误率为 2.35%。\n4.5 MNIST NIN 的错误率为 0.47%，这并不是一个最好的结果。\n4.6 Global Average Pooling as a Regularizer 在探讨全局平均池化是否对传统 CNN 具有同样的正则化效果时，实例化了一个 CNN（三个卷积层和一个局部连接层组成）。\n在 CIFAR-10 数据集上进行了测试：\n 全连接层的 CNN 模型只能实现 17.56% 的错误率。 当加入 dropout 时，错误率为 15.99%。 用全局平均池化替换全连接层，得到的错误率为 16.46%。  全局平均池化比 dropout 的结果稍差。\n4.7 Visualization of NIN 从 CIFAR-10 的训练模型的最后一个 mlpconv 层提取并直接可视化特征图。\n可视化再次证明了 NIN 的有效性。它是通过使用 mlpconv 层进行更强的局部感受野建模来实现的。然后，全局平均池化强制学习类别级特征图。\n5 Conclusions Network In Network (NIN) 由输入进行卷积的 mlpconv 层和全局平均池化层组成。mlpconv 层可以更好地对局部 patch 进行建模，全局平均池化可以防止全局的过拟合。\nNIN 在 CIFAR-10、CIFAR-100 和 SVHN 数据集都取得了最好的表现。\n通过对特征图的可视化，证明了 NIN 最后一个 mlpconv 层的特征图是类别的置信度图。\n","permalink":"http://landodo.github.io/posts/20201215-network-in-network/","summary":"Network In Network 论文阅读 ✅ 论文地址：https://arxiv.org/pdf/1312.4400.pdf ✅ 论文发表时间：2014 年 论文提出了一种名为","title":"Network In Network"},{"content":"HTTP Server⟹HTTPS Server: 实验要求 这是网络课的一个小作业，具体要求如下：\n使用 C 语言实现一个 HTTP 服务器\n  支持 HTTP Post/Get 方法，可以上传或下载文件\n  支持 HTTP 分块传输，支持 HTTP 持久连接和管道\n  使用 openssl 库，支持 HTTPS\n  使用 libevent 支持多路并发\n  提交：代码和实验报告，实现越完整越好，测试越充分越好\n  前置知识 HTTP 无所不在，我们日常访问的网站都在 HTTP 服务器上运行。\n现在更常见的是 HTTPS，从技术的角度上看，HTTPS 相比于 HTTP 来说具有更高的安全性。\n如何构建 HTTP 服务器呢？这其实是非常简单的。在开始之前，需要先复习一下基本的网络知识。\n1. OSI 首先，需要对 OSI 有一个基本的认识和了解。\nOSI 全称为 Open Systems Interconnection。\n OSI 模型是一个概念模型，它对电信或计算系统的通信功能进行描述和标准化，而不考虑其基本的内部结构和技术。\n它的目标是用标准协议实现不同通信系统的互操作性。该模型将通信系统划分为若干抽象层。该模型的最初版本定义了七个层。\n 图片链接：OSI 七层模型\n为了实现 HTTP 服务器，需要关心的是第四层——传输层（Transport Layer）。\n2. 传输层（Transport Layer）  传输层主要负责确保数据可靠无误地从一个点传输到另一个点。例如，传输层确保数据以正确的顺序发送和接收。\n传输层提供流控制和错误处理，并参与解决有关数据包的传输和接收问题。传输层协议的常见例子有传输控制协议（TCP）、用户数据报协议（UDP）和顺序包交换（SPX）。\n 在传输层，主要使用 TCP 来实现 HTTP 服务器。虽然也可以使用 UDP 实现，但是并没有很多人这么做。\n HTTP 通信通常通过 TCP/IP 连接进行。默认端口是 TCP 80，但也可以使用其他端口。这并不排除 HTTP 在 Internet 上或其他网络上的任何其他协议之上实现。\nHTTP 只预设了一个可靠的传输，任何提供这种保证的协议都可以使用，HTTP/1.1 请求和响应结构在有关协议的传输数据单元上的映射不在本规范的范围之内。\n——RFC 2616\n 所有著名的 HTTP 服务器如 Apache Tomcat、NginX 等都是在 TCP 之上实现的。所以，使用基于 TCP 的 HTTP 服务器是一个正确的选择。\n3. RFC RFC 是什么？\nRFC 全称为 Request for Comments。\nRequest for Comments 文件是 Steve Crocker 在 1969 年发明的，用于帮助记录 ARPANET 发展的非官方笔记。此后，RFCs 成为互联网规范、通信协议、程序和事件的正式文件。\n   截至 2017 年 8 月，共有 8200 多个 RFC。\n  万维网上 RFCs 的官方来源是 RFC Editor。\n  一些标准化的RFC有。\n HTTP/1.1 → 最初是 RFC 2616，后来被 RFC 7230、RFC 7231、RFC 7232、RFC 7233、RFC 7234、RFC 7235 取代。所以，需要从 RFC 7230 到 RFC 7235 来实现 HTTP 的基本工作原理。 HTTP/2 → RFC 7540 和 RFC 7541 FTP → RFC959    所以，如果要实现 HTTP 服务器，可以先阅读特定的 RFC，即 RFC 7230、RFC 7231、RFC 7232、RFC 7233、RFC 7234、RFC 7235。\n   4. TCP Socket 要实现 TCP，我们必须学习 TCP Socket 编程。\nSocket 也叫做套接字。\n 套接字是大多数流行的操作系统提供的让程序访问网络的机制，它允许不同网络机器上的应用程序（无关的进程）之间收发消息。它允许在不同网络机器上的应用程序（不相关的进程）之间发送和接收消息。\n套接字机制是为了独立于任何特定类型的网络而创建的。\nIP 是迄今为止最主要的网络，也是套接字最流行的用途。\n Socket 编程的几个重要步骤\n 创建 Socket 绑定 Socket 服务器端等待连接 发送和接收消息 关闭 Socket  Socket 编程 实现一个基于 TCP 通信的服务器。\n第 1 步：创建 Socket socket 函数：man socket\nNAME  socket -- create an endpoint for communication  SYNOPSIS  #include \u0026lt;sys/socket.h\u0026gt;   int  socket(int domain, int type, int protocol);  domain：域名或地址族通信域，套接字应在其中创建。一些地址族有 AF_INET(IP)、AF_INET6(IPv6)、AF_UNIX（本地通道，类似管道）、AF_ISO（ISO 协议）和 AF_NS（Xerox 网络系统协议)。 type：服务类型。这要根据应用所需的属性来选择。SOCK_STREAM（TCP 字节流）、SOCK_DGRAM（数据报服务）、SOCK_RAW（直接 IP 服务）。 protocol：对于 TCP/IP 套接字，要指定 IP 地址族为 AF_INET 和虚拟电路服务（SOCK_STREAM）。由于虚拟电路服务只有一种形式，所以没有协议的变化，所以最后一个参数 protocol 为零。  示例：\n// 1. 创建 TCP Socket int sfd = socket(AF_INET, SOCK_STREAM, 0); 第 2 步：绑定 Socket 创建套接字后，需要给套接字绑定（bind）一个网络传输地址。\nint bind(int socket, const struct sockaddr *address, socklen_t address_len);  第一个参数 socket 是第 1 步所创建的 sfd。 第二个参数，sockaddr  是一个结构体数据，对于 IP 网络，使用的是 sockaddr_in，它在头netinet/in.h 中定义 第三个参数一半传入sizeof(svr_addr).  // #include \u0026lt;netinet/in.h\u0026gt; // sockaddr_in /* * Socket address, internet style. */ struct sockaddr_in { \t__uint8_t sin_len; \tsa_family_t sin_family; \tin_port_t sin_port; \tstruct in_addr sin_addr; \tchar sin_zero[8]; };  sin_family：socket 使用的地址族。本次实现的 TCP ，它是 AF_INET。 sin_port：端口号 sin_addr：IP 地址，是一个 32 位的无符号整型  struct in_addr { \tin_addr_t s_addr; };  typedef __uint32_t in_addr_t; /* base type for internet address */ 使用示例：（这里还涉及大端序和小端序的知识）\n// 2. bind() 绑定 IP 和端口号 struct sockaddr_in svr_addr; //用于填写服务器的 IP 地址与端口号 svr_addr.sin_family = AF_INET; //地址族 svr_addr.sin_port = htons(8888); //端口号 svr_addr.sin_addr.s_addr = inet_addr(\u0026#34;10.203.11.153\u0026#34;); // IP 地址 if (bind(sfd, (const struct sockaddr *)\u0026amp;svr_addr, sizeof(svr_addr)) \u0026lt; 0) {  perror(\u0026#34;bind() error\u0026#34;);  exit(EXIT_FAILURE); } 第 3/4 步：服务器监听端口等待连接 建立监听队列，等待连接。\n#include \u0026lt;sys/socket.h\u0026gt; int listen(int socket, int backlog); 第二个参数 backlog 定义了在拒绝连接之前可以排队等待的最大连接数。\n使用示例：\n// 3. 建立监听队列 if (listen(sfd, 10) \u0026lt; 0) {  perror(\u0026#34;listen()\u0026#34;); //输出出错原因  exit(EXIT_FAILURE); //退出应用程序 } accept 系统调用会抓取待处理连接队列中的第一个连接请求，并为该连接创建一个新的套接字。\n原来为监听而设置的套接字只用于接受连接，不用于交换数据。默认情况下，套接字操作是同步的，或者说是阻塞的，accept 会阻塞，直到队列上有连接存在。\n#include \u0026lt;sys/socket.h\u0026gt; int accept(int socket, struct sockaddr *restrict address, socklen_t *restrict address_len);  第一个参数 socket 是设置用于接受监听连接的 socket。 第二个参数 address 是与进行连接的客户端的地址一起归档的地址结构。这样我们就可以在需要的时候检查连接 socket 的地址和端口号。 第三个参数是填入地址结构的长度。  使用示例：\n// 4. 服务器接收客户端的连接请求 // 记录客户端的地址信息 struct sockaddr_in client_addr; socklen_t len = sizeof(client_addr); int cfd = accept(sfd, (struct sockaddr *)\u0026amp;client_addr, \u0026amp;len); if (cfd == -1) {  perror(\u0026#34;accept() error!\\n\u0026#34;);  exit(EXIT_FAILURE); } 第 5/6 步：接收/发送数据 现在终于把客户端（当从网页浏览器访问服务器的 IP 地址时）和服务器之间的套接字连接起来了。\n// 第4步连接成功，打印出客户端的 IP 和端口号 printf(\u0026#34;client ip: %s, port: %d\\n\u0026#34;, inet_ntoa(client_addr.sin_addr), htons(client_addr.sin_port));  // 5. 接收客户端的数据 server \u0026lt;------ client char recv_buffer[1024] = {\u0026#39;\\0\u0026#39;}; ssize_t rbytes = recv(cfd, recv_buffer, sizeof(recv_buffer), 0); if (rbytes == -1) {  perror(\u0026#34;recv() error!\\n\u0026#34;);  exit(EXIT_FAILURE); } printf(\u0026#34;recv: %s\\n\u0026#34;, recv_buffer);  // 6. 向客户端发送数据： server ------\u0026gt; client char send_buffer[1024] = \u0026#34;Hello, Client. Your IP is \u0026#34;; strcat(send_buffer, inet_ntoa(client_addr.sin_addr)); size_t sbytes = send(cfd, send_buffer, strlen(send_buffer)+1, 0); if (sbytes == -1) {  perror(\u0026#34;send() error!\\n\u0026#34;);  exit(EXIT_FAILURE); } 第 7 步：关闭 Socket 最后，别忘了关闭 Socket。\n// 关闭连接 close(cfd); TCP Socket 服务器端完整代码 // HTTP_server.c #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; // exit#include \u0026lt;arpa/inet.h\u0026gt; // inet_ntoa#include \u0026lt;netinet/in.h\u0026gt; // sockaddr_in#include \u0026lt;string.h\u0026gt; // strlen#include \u0026lt;unistd.h\u0026gt; // close function int main() {  // 1. 创建 Socket 套接字  int sfd = socket(AF_INET, SOCK_STREAM, 0);  if (sfd == -1) {  perror(\u0026#34;socket() error!\\n\u0026#34;);  exit(EXIT_FAILURE);  }   // 2. bind() 绑定 IP 和端口号  struct sockaddr_in svr_addr; //用于填写服务器的ip地址与端口号  svr_addr.sin_family = AF_INET; //地址族  svr_addr.sin_port = htons(8888); //端口号  svr_addr.sin_addr.s_addr = inet_addr(\u0026#34;10.203.11.153\u0026#34;); //ip地址  if (bind(sfd, (const struct sockaddr *)\u0026amp;svr_addr, sizeof(svr_addr)) \u0026lt; 0) {  perror(\u0026#34;bind()\u0026#34;);  exit(EXIT_FAILURE);  }   // 3. 建立监听队列  if (listen(sfd, 10) \u0026lt; 0) {  perror(\u0026#34;listen()\u0026#34;);  exit(EXIT_FAILURE);  }   // 4. 服务器接收客户端的连接请求  // 记录客户端的地址信息  struct sockaddr_in client_addr;  socklen_t len = sizeof(client_addr);  int cfd = accept(sfd, (struct sockaddr *)\u0026amp;client_addr, \u0026amp;len);  if (cfd == -1) {  perror(\u0026#34;accept() error!\\n\u0026#34;);  exit(EXIT_FAILURE);  }   // 连接成功，打印出客户端的 IP 和端口号  printf(\u0026#34;client ip: %s, port: %d\\n\u0026#34;, inet_ntoa(client_addr.sin_addr), htons(client_addr.sin_port));   // 5. 接收客户端的数据 server \u0026lt;------ client  char recv_buffer[1024] = {\u0026#39;\\0\u0026#39;};  ssize_t rbytes = recv(cfd, recv_buffer, sizeof(recv_buffer), 0);  if (rbytes == -1) {  perror(\u0026#34;recv() error!\\n\u0026#34;);  exit(EXIT_FAILURE);  }  printf(\u0026#34;recv: %s\\n\u0026#34;, recv_buffer);   // 6. 向客户端发送数据： server ------\u0026gt; client  char send_buffer[1024] = \u0026#34;Hello, Client. Your IP is \u0026#34;;  strcat(send_buffer, inet_ntoa(client_addr.sin_addr));  size_t sbytes = send(cfd, send_buffer, strlen(send_buffer)+1, 0);  if (sbytes == -1) {  perror(\u0026#34;send() error!\\n\u0026#34;);  exit(EXIT_FAILURE);  }   // 7. 关闭连接  close(cfd);  close(sfd);  return 0; } 客户端的写法是：socket() - connect() - send() - recv() - close()。\n这里我偷懒了，直接使用一个简单的「网络调试助手」来替代客户端。\n测试结果 1、服务器 IP 和监听的端口号\nsvr_addr.sin_port = htons(8888); //端口号 svr_addr.sin_addr.s_addr = inet_addr(\u0026#34;10.203.11.153\u0026#34;); //IP 地址 2、启动服务器（配置 IP 地址为自己电脑的）\n➜ Build-a-simple-HTTP-server gcc HTTP_server.c ➜ Build-a-simple-HTTP-server ./a.out 3、客户端连接服务器，发送 “Hello, Server! This is Client.”\n4、观察结果：服务器端接收到了客户端发送的消息；客户端接收区接收到了服务器的消息。\n总结 程序运行能够在应用程序之间进行通信。这意味着以上实现的 TCP 实现工作正常。\n至此已经完成了最基本的编码部分。\n现在继续进行 HTTP 服务器的实现。\nHTTP 服务器 首先看看服务器和 Web 浏览器之间的交互。\n再把 HTTP 部分放大，进行更加细致的了解。\n HTTP Client（即 Web 浏览器）向 HTTP Server 发送 HTTP 请求。 服务器处理收到的请求，并向 HTTP Client 发送 HTTP 响应。  现在，细致的客户端-服务器，以及它们发送和接收的内容。\nHTTP Client HTTP 客户端（Web浏览器）是请求的发起者。\n向浏览器中输入一个连接：\nhttp://www.example.com 为了显示页面，浏览器从 Web 服务器 80 端口获取文件 index.html（默认网页）。发出的请求如下：\n现在，HTTP_server.c  先不做任何修改，运行程序，启动服务器。直接在浏览器中输入 l0.203.11.153:8888 ，看看会发生什么。\n该网页无法正常运行。\n在看看服务器端输出了什么：\n非常有意思，这值得好好看一看。第一行：\nclient ip: 10.203.11.153, port: 54813 这是我们的服务器端自己 printf 的，第二行的 \u0026ldquo;recv:\u0026rdquo; 也是，不算。所有服务器端接收到的内容应该是：\nGET / HTTP/1.1 Host: 10.203.11.153:8888 Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3 Accept-Encoding: gzip, deflate Accept-Language: en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7,und;q=0.6,es;q=0.5,fr;q=0.4,zh-TW;q=0.3,ja;q=0.2 按理说服务器接收到客户端的连接后，应该会向客户端发送 \u0026ldquo;Hello, Clinet, \u0026hellip;.\u0026quot;，但是网页却无法正常运行。\n// 6. 向客户端发送数据： server ------\u0026gt; client char send_buffer[1024] = \u0026#34;Hello, Client. Your IP is \u0026#34;; strcat(send_buffer, inet_ntoa(client_addr.sin_addr)); size_t sbytes = send(cfd, send_buffer, strlen(send_buffer)+1, 0); 要解决这个问题，就要了解一下 HTTP 的知识了。\nHTTP Methods GET 是 HTTP 默认使用的方法，如上面终端中，Server 接收到的信息，第一行就是一个 GET。\n常用的 HTTP Methods：\n GET：获取一个 URL。 HEAD：获取一个 URL 的信息。 PUT：存储到一个 URL。 POST：将表单数据发送至一个 URL 并获得响应。 DELETE：删除一个 URL GET 和 POST（表单）是常用的。  HTTP Server 客户端发送了一些头信息（GET \u0026hellip;），希望服务器做出响应的响应。\n但我们自己实现的 HTTP_server 只发送了一个问候信息，这缺少一些必要的头信息，这就是问题的所在。\n// 6. 向客户端发送数据： server ------\u0026gt; client char send_buffer[1024] = \u0026#34;Hello, Client. Your IP is \u0026#34;; strcat(send_buffer, inet_ntoa(client_addr.sin_addr)); size_t sbytes = send(cfd, send_buffer, strlen(send_buffer)+1, 0); 浏览器希望得到的是一个与请求相同格式的数据。\nHTTP 不过是遵循 RFC 文件中指定的一些规则，它的实现与语言无关。\n如下是网络浏览器所期待的 HTTP 响应格式的例子。显示的头信息只是一个例子。HTTP 中还有很多头信息存在，具体可以看 HTTP RFCs → RFC 7230, RFC 7231, RFC 7232, RFC 7233, RFC 7234, RFC 7235。\n如果想从服务器发送 Hello：\n  首先需要构造 Header，\n  然后插入一个空行\n  之后我们就可以发送消息/数据。\n现在，开始构造一个最小的 HTTP Header 来使 HTTP 服务器正常工作。\n  char send_buffer[1024] = \u0026#34;HTTP/1.1 200 OK\\nContent-Type: text/plain\\nContent-Length: 39\\n\\nHello, Client. Your IP is \u0026#34;; strcat(send_buffer, inet_ntoa(client_addr.sin_addr)); 如上的 3 个头信息是最低要求。\n HTTP/1.1 200 OK ：使用的 HTTP 版本号、状态码和状态信息。 Content-Type: text/plain ：服务器发送的是纯文本。Content-Type 还有其他的格式。 Content-Length: 39 ：服务器向客户端发送 39 字节。网络浏览器只读取我们在这里提到的数量。 最后一部分是 Body。在这里发送自定义的数据。  首先，需要在 Body 中计算出发送的字节数，与 Content-Length 相同。    \u0026quot;Hello, Client. Your IP is 10.203.11.153\u0026quot; 的长度刚好等于 39 字节。\n客户端将会接收到：\nHTTP/1.1 200 OK Content-Type: text/plain Content-Length: 39 Hello, Client. Your IP is 10.203.11.153 浏览器访问 IP:8888，可以看到工作正常。\n 关于状态码和状态信息：\n  状态码是服务器根据客户向服务器提出的请求而发出的。它包括 IETF Request for Comments (RFCs)、其他规范中的代码，以及超文本传输协议（HTTP）的一些常见应用中使用的附加代码。\n  状态码的第一个数字指定了五种标准响应类别中的一种。所显示的消息短语是典型的，但可以提供任何人类可读的替代方案。除非另有说明，否则状态码是 HTTP/1.1 标准（RFC 7231）的一部分。\n  所以，如果服务器找不到客户端要求的文件，那么就发送适当的状态码。\n  如果客户端没有权限查看文件，那么就发送相应的状态码。\n  常见的状态码：\n  200 OK\n  403 Forbidden\n  404 Not Found\n  502 Bad Gateway\n    Informational responses (100–199),\n  Successful responses (200–299),\n  Redirects (300–399),\n  Client errors (400–499),\n  and Server errors (500–599).\n   现在，HTTP 已经正常工作了。虽然它非常非常的简陋。\n客户端请求网页（理论） 到现在为止，已经实现一个能向浏览器客户端发送一个字符串的 HTTP 服务器。\n现在，来看看如何发送一个文件、图片等信息。假设在地址栏中输入：\nIP:8888/info.html 在服务器终端，将会得到如下请求。\nGET /info.html HTTP/1.1 Host: 10.203.11.153:8888 Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3 Accept-Encoding: gzip, deflate Accept-Language: en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7,und;q=0.6,es;q=0.5,fr;q=0.4,zh-TW;q=0.3,ja;q=0.2 看一看请求头中的第一行。\nGET /info.html HTTP/1.1 所以，服务器只需要在当前目录下搜索 info.html 文件。\n当然还有很多情况需要服务器考虑，例如：\n 文件（网页）是存在的 文件（网页）不存在 客户端没有访问文件（网页）的权限。 \u0026hellip;..  服务器端需要做的是：\n 选择合适的状态码； 如果文件是存在的，并且客户端有权限访问它，选择合适的 Content-Type； 然后打开文件，将数据读到一个变量中。统计从文件中读取的字节数； 设置 Content-Length； 构造 Response Header； 在 Response Header 的末尾添加一个新行，并将从文件中读取的数据追加到其中； 发送给客户端。  参考链接  HTTP Server: Everything you need to know to Build a simple HTTP server from scratch HTTP Status Codes  ","permalink":"http://landodo.github.io/posts/20201107-http-server-1/","summary":"HTTP Server⟹HTTPS Server: 实验要求 这是网络课的一个小作业，具体要求如下： 使用 C 语言实现一个 HTTP 服务器 支持 HTTP Post/Get 方法，可以上传或下载文件 支持 HTTP 分","title":"使用 C 语言实现一个 HTTP 服务器（1）"},{"content":"Prim 算法\u0026amp;Kruskal 算法求最小生成树 最小生成树 概述：在一给定的无向图 G=(V,E) 中，(u,v) 代表连接顶点 u 与顶点 v 的边，而 w(u,v) 代表此边的权值，若存在 T 为 E 的子集，且为无循环图，使得 w(T) 最小，\n则此 T 为 G 的最小生成树。\n当边的权重相同时，最小生成树不唯一，但是最小生成树的权值之和最小是确定的。\n这个无向图中有 6 个定点，10 条边。\n依次使用 Prim 算法和 Kruskal 算法求解最小生成树。\n图的表示 图的表示可以使用邻接表和邻接矩阵。\n图的数据存储在 data.txt，第一行表示顶点数 6 和边数 10。然后依次是边的两个顶点和权重。\n6 10 0 1 6 0 2 1 0 3 5 1 4 3 1 2 5 2 3 5 2 4 6 2 5 4 3 5 2 4 5 6 一条边由两个顶点和边上的权重表示，edge.h\n#ifndef EDGE_H #define EDGE_H  #include \u0026lt;ostream\u0026gt;#include \u0026lt;cassert\u0026gt; using std::ostream;  class Edge {  public:  Edge();  Edge(int a, int b, double weight);  ~Edge();   int V(); // 返回第一个顶点  int W(); // 返回第二个顶点  double Wt(); // 返回边的权值  int Other(int x); // 返回边上与顶点x相连另一个顶点   void UpdateWt(double weight);   friend ostream\u0026amp; operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Edge \u0026amp;e);   bool operator\u0026lt;(Edge \u0026amp;e);  bool operator\u0026lt;=(Edge \u0026amp;e);  bool operator\u0026gt;(Edge \u0026amp;e);  bool operator\u0026gt;=(Edge \u0026amp;e);  bool operator==(Edge \u0026amp;e);   private:  int a_;  int b_;  double weight_; };  Edge::Edge() {  }  Edge::~Edge() {  }   Edge::Edge(int a, int b, double weight) {  a_ = a;  b_ = b;  weight_ = weight; }   // 返回第一个顶点 int Edge::V(){  return a_; }  // 返回第二个顶点 int Edge::W() {  return b_; }   int Edge::Other(int x) {  assert(x == a_ || x == b_);  if (x == a_) {  return b_;  } else {  return a_;  } }  double Edge::Wt() {  return weight_; }  void Edge::UpdateWt(double weight) {  this-\u0026gt;weight_ = weight; }  ostream\u0026amp; operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const Edge \u0026amp;e) {  os \u0026lt;\u0026lt; e.a_ \u0026lt;\u0026lt; \u0026#34;-\u0026#34; \u0026lt;\u0026lt; e.b_ \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; e.weight_;  return os; }  bool Edge::operator\u0026lt;(Edge \u0026amp;e) {  return weight_ \u0026lt; e.Wt(); }  bool Edge::operator\u0026lt;=(Edge \u0026amp;e) {  return weight_ \u0026lt;= e.Wt(); }  bool Edge::operator\u0026gt;(Edge \u0026amp;e) {  return weight_ \u0026gt; e.Wt(); }  bool Edge::operator\u0026gt;=(Edge \u0026amp;e) {  return weight_ \u0026gt;= e.Wt(); }  bool Edge::operator==(Edge \u0026amp;e) {  return weight_ == e.Wt(); }  #endif 使用邻接矩阵存储图，dense_graph.h 稠密图。\n#ifndef DENSE_GRAPH_H #define DENSE_GRAPH_H  // 稠密图：邻接矩阵表示 #include \u0026#34;edge.h\u0026#34;#include \u0026lt;vector\u0026gt;#include \u0026lt;iostream\u0026gt;#include \u0026lt;cassert\u0026gt; using std::cout; using std::endl;  using std::vector;  // 邻接矩阵表示稠密图 class DenseGraph {  public:  DenseGraph(int n, bool directed);  ~DenseGraph();  int V(); // 图的总节点数  int W(); // 图的总边数  bool HasEdge(int v, int w);  void AddEdge(int v, int w, double weight);   void Show();   class AdjIterator {  public:  AdjIterator(DenseGraph \u0026amp;graph, int v) : G_(graph) {  v_ = v;  index_ = -1;  }  ~AdjIterator() {   }   Edge* begin() {  index_ = -1;  return next();  }   Edge* next() {  // 从当前index开始向后搜索, 直到找到一个g[v][index]为true  for (index_ += 1; index_ \u0026lt; G_.V(); ++index_) {  if (G_.g_[v_][index_]) {  return G_.g_[v_][index_];  }  }  // 若没有顶点和v相连接, 则返回nullptr  return nullptr;  }  // 查看是否已经迭代完了图G中与顶点v相连接的所有边  bool end() {  return index_ \u0026gt;= G_.V();  }   private:  DenseGraph \u0026amp;G_;  int v_;  int index_;  };   private:  int n_;  int m_;  bool directed_;  vector\u0026lt;vector\u0026lt;Edge *\u0026gt; \u0026gt; g_; // 图的数据 };  DenseGraph::DenseGraph(int n, bool directed) {  n_ = n;  m_ = 0;  directed_ = directed;  // g初始化为n*n的矩阵, 每一个g[i][j]指向一个边的信息, 初始化为null  g_ = vector\u0026lt;vector\u0026lt;Edge *\u0026gt; \u0026gt;(n, vector\u0026lt;Edge *\u0026gt;(n, nullptr));  }  DenseGraph::~DenseGraph() {  for (int i = 0; i \u0026lt; n_; ++i) {  for (int j = 0; j \u0026lt; n_; ++j) {  if (g_[i][j] != nullptr) {  delete g_[i][j];  }  }  } }  int DenseGraph::V() {  return n_; }  int DenseGraph::W() {  return m_; }  bool DenseGraph::HasEdge(int v, int w) {  assert(v \u0026gt;= 0 \u0026amp;\u0026amp; v \u0026lt; n_);  assert(w \u0026gt;= 0 \u0026amp;\u0026amp; w \u0026lt; n_);  return g_[v][w]; }  void DenseGraph::AddEdge(int v, int w, double weight) {  assert(v \u0026gt;= 0 \u0026amp;\u0026amp; v \u0026lt; n_);  assert(w \u0026gt;= 0 \u0026amp;\u0026amp; w \u0026lt; n_);  if (HasEdge(v, w)) {  delete g_[v][w];  if (v != w \u0026amp;\u0026amp; !directed_) {  delete g_[w][v];  }  m_--;  }  g_[v][w] = new Edge(v, w, weight);  if (!directed_) {  g_[w][v] = new Edge(w, v, weight);  }  m_++; }  void DenseGraph::Show() {  for (int i = 0; i \u0026lt; n_; ++i) {  for (int j = 0; j \u0026lt; n_; ++j) {  if (g_[i][j]) {  printf(\u0026#34;%.2f \\t\u0026#34;, g_[i][j]-\u0026gt;Wt());  } else {  printf(\u0026#34;null\\t\u0026#34;);  }  }  cout \u0026lt;\u0026lt; endl;  } }  #endif 将 data.txt 的数据存储到图中，read_file.h\n#ifndef READ_FILE_H #define READ_FILE_H  #include \u0026lt;string\u0026gt;#include \u0026lt;fstream\u0026gt;#include \u0026lt;cassert\u0026gt;#include \u0026lt;sstream\u0026gt;#include \u0026lt;iostream\u0026gt; using std::string; using std::ifstream; using std::stringstream; using std::cout; using std::endl;  template \u0026lt;typename Graph\u0026gt; class ReadFile {  public:  ReadFile(Graph \u0026amp;graph, const string filename) {  ifstream file(filename);  string line;  int V;  int E;   assert(file.is_open());  assert(getline(file, line));  stringstream ss(line);  ss \u0026gt;\u0026gt; V \u0026gt;\u0026gt; E;  cout \u0026lt;\u0026lt; \u0026#34;Read Graph V = \u0026#34; \u0026lt;\u0026lt; V \u0026lt;\u0026lt; \u0026#34;, E = \u0026#34; \u0026lt;\u0026lt; E \u0026lt;\u0026lt; endl;  assert(V == graph.V());   for (int i = 0; i \u0026lt; E; ++i) {  assert(getline(file, line));  stringstream ss(line);  int a;  int b;  double w;  ss \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b \u0026gt;\u0026gt; w;  assert(a \u0026gt;= 0 \u0026amp;\u0026amp; a \u0026lt; V);  assert(b \u0026gt;= 0 \u0026amp;\u0026amp; b \u0026lt; V);  graph.AddEdge(a, b, w);  }  } };  #endif Prim 算法 Prim 算法基于贪心的策略，每次取权重最小的边。因此需要实现一个最小堆。\n#ifndef MIN_HEAP_H #define MIN_HEAP_H  #include \u0026lt;cassert\u0026gt;#include \u0026lt;algorithm\u0026gt; // swap template \u0026lt;typename Item\u0026gt; class MinHeap {  public:  MinHeap(int capacity);  ~MinHeap();  int GetSize();  int GetCapacity();  bool IsEmpty();  void Insert(Item item);  void ExtractMin();  Item GetMin();   private:  void ShiftUp(int k);  void ShiftDown(int k);   private:  Item *data_;  int size_;  int capacity_; };  template \u0026lt;typename Item\u0026gt; MinHeap\u0026lt;Item\u0026gt;::MinHeap(int capacity) {  data_ = new Item[capacity+1];  size_ = 0;  capacity_ = capacity; }  template \u0026lt;typename Item\u0026gt; MinHeap\u0026lt;Item\u0026gt;::~MinHeap() {  delete [] data_; }  template \u0026lt;typename Item\u0026gt; int MinHeap\u0026lt;Item\u0026gt;::GetSize() {  return size_; }  template \u0026lt;typename Item\u0026gt; int MinHeap\u0026lt;Item\u0026gt;::GetCapacity() {  return capacity_; }  template \u0026lt;typename Item\u0026gt; bool MinHeap\u0026lt;Item\u0026gt;::IsEmpty() {  return size_ == 0; }  template \u0026lt;typename Item\u0026gt; void MinHeap\u0026lt;Item\u0026gt;::Insert(Item item) {  assert(size_ + 1 \u0026lt;= capacity_);  data_[size_+1] = item;  ShiftUp(size_+1);  size_++; }  template \u0026lt;typename Item\u0026gt; void MinHeap\u0026lt;Item\u0026gt;::ExtractMin() {  assert(size_ \u0026gt; 0);  swap(data_[1], data_[size_]);  size_--;  ShiftDown(1); }  template \u0026lt;typename Item\u0026gt; Item MinHeap\u0026lt;Item\u0026gt;::GetMin() {  assert(size_ \u0026gt; 0);  return data_[1]; }  template \u0026lt;typename Item\u0026gt; void MinHeap\u0026lt;Item\u0026gt;::ShiftUp(int k) {  while (k \u0026gt; 1 \u0026amp;\u0026amp; data_[k/2] \u0026gt; data_[k]) {  std::swap(data_[k], data_[k/2]);  k /= 2;  } }  template \u0026lt;typename Item\u0026gt; void MinHeap\u0026lt;Item\u0026gt;::ShiftDown(int k) {  while (2*k \u0026lt;= size_) {  int j = 2*k;  if (j + 1 \u0026lt;= size_ \u0026amp;\u0026amp; data_[j+1] \u0026lt; data_[j]) {  j++;  }  if (data_[k] \u0026lt; data_[j]) {  break;  }  std::swap(data_[k], data_[j]);  k = j;  } } #endif 接下来开始 Prim 算法\n// Prim算法求最小生成树 // 最小堆数据结构辅助  #include \u0026#34;min_heap.h\u0026#34;#include \u0026#34;edge.h\u0026#34;#include \u0026lt;vector\u0026gt; using std::vector;  template \u0026lt;typename Graph\u0026gt; class PrimMST {  public:  PrimMST(Graph \u0026amp;graph):G_(graph), pq_(MinHeap\u0026lt;Edge\u0026gt;(G_.W())) {  // 算法初始化  marked_ = new bool[G_.V()]; // 节点数  for (int i = 0; i \u0026lt; G_.V(); ++i) {  marked_[i] = false;  }  mst_.clear();   // Prim  Visit(0); // 首先访问0号节点  while (!pq_.IsEmpty()) {  Edge e = pq_.GetMin();  pq_.ExtractMin(); // 删除最小元素  // 如果这条边上的两个点都已经被访问，则已经不为横切边了，直接剔除  if (marked_[e.V()] == marked_[e.W()]) {  continue;  }  // 否则，此边为横切边，加入到最小生成树中  mst_.push_back(e);  if (!marked_[e.V()]) {  Visit(e.V());  } else {  Visit(e.W());  }  }  weight_ = mst_[0].Wt();  for (int i = 1; i \u0026lt; mst_.size(); ++i) {  weight_ += mst_[i].Wt();  }  }   ~PrimMST() {  delete [] marked_;  }   // 返回最小生成树的所有边  vector\u0026lt;Edge\u0026gt; MSTEdges() {  return mst_;  }   // 返回最小生成树的权值  double MSTWeight() {  return weight_;  }   private:  void Visit(int v) {  assert(!marked_[v]);  marked_[v] = true;   // 将和v相连的所有未被访问的边加入最小堆中:图的邻边迭代器  typename Graph::AdjIterator adj(G_, v);  for (Edge* e = adj.begin(); !adj.end(); e = adj.next()) {  // 如果与v相连接点未被标记，则此边为横切边，  // 将此边加入到最小堆中，作为备选  if (!marked_[e-\u0026gt;Other(v)]) {  pq_.Insert(*e);  }  }  }   private:  Graph \u0026amp;G_;  MinHeap\u0026lt;Edge\u0026gt; pq_; // 最小堆优先队列  bool *marked_; // 节点是否被标记  vector\u0026lt;Edge\u0026gt; mst_; // 保存最小生成树的边  double weight_; // 最小生成树的权值 }; Kruskal 算法 Kruskal 也需要取最小的边，但是它还需要判断是否会产生环，不会产生环才能将边加入最小生成树。\n通过并查集（Union Find）可是实现环的检测。union_find.h\n#ifndef UNION_FIND_H #define UNION_FIND_H #include \u0026lt;cassert\u0026gt; class UnionFind {  public:  UnionFind(int n);  ~UnionFind();  int Find(int p);  bool IsConnected(int p, int q);  void UnionElements(int p, int q);  private:  int *parent_; // parent[i] 表示i元素的根节点  int *rank_; // rank[i]表示以i为根的集合所表示的树的层数  int count_; };  UnionFind::UnionFind(int n) {  parent_ = new int[n];  rank_ = new int[n];  count_ = n;  for (int i = 0; i \u0026lt; n; ++i) {  parent_[i] = i;  rank_[i] = 1;  } }  UnionFind::~UnionFind() {  delete [] parent_;  delete [] rank_; }  int UnionFind::Find(int p) {  assert(p \u0026gt;= 0 \u0026amp;\u0026amp; p \u0026lt; count_);  while (parent_[p] != p) {  p = parent_[p];  }  return p; }  bool UnionFind::IsConnected(int p, int q) {  return Find(p) == Find(q); }  void UnionFind::UnionElements(int p, int q) {  int p_root = Find(p);  int q_root = Find(q);  if (p_root == q_root) {  return;  }  if (rank_[p_root] \u0026lt; rank_[q_root]) {  parent_[p_root] = q_root;  } else if (rank_[p_root] \u0026gt; rank_[q_root]){  parent_[q_root] = p_root;  } else {  parent_[p_root] = q_root;  rank_[q_root] += 1;  } }  #endif 接下来实现 Kruskal 算法。\n#ifndef KRUSKAL_H #define KRUSKAL_H  #include \u0026#34;min_heap.h\u0026#34;#include \u0026#34;edge.h\u0026#34;#include \u0026#34;union_find.h\u0026#34; template \u0026lt;typename Graph\u0026gt; class KruskalMST {  public:  KruskalMST(Graph \u0026amp;graph) {  // 算法初始化  marked_ = new bool[graph.V()]; // 节点数  for (int i = 0; i \u0026lt; graph.V(); ++i) {  marked_[i] = false;  }  mst_.clear();  // 将所有的边存放到一个最小堆中  // 创建一个容量为边的总数的堆  MinHeap\u0026lt;Edge\u0026gt; pq(graph.W());  for (int i = 0; i \u0026lt; graph.V(); ++i) {  typename Graph::AdjIterator adj(graph, i);  marked_[i] = true;  for (Edge* e = adj.begin(); !adj.end(); e = adj.next()) {  if (marked_[e-\u0026gt;V()] \u0026amp;\u0026amp; marked_[e-\u0026gt;W()]) {  continue;  }  pq.Insert(*e);  }  }  UnionFind uf = UnionFind(graph.V()); // 节点数  while (!pq.IsEmpty() \u0026amp;\u0026amp; mst_.size() \u0026lt; graph.V() - 1) {  // 从最小堆中依次从小到大取出所有的边  Edge e = pq.GetMin();  pq.ExtractMin(); // 从堆中删除  // 如果该边的两个端点是联通的, 说明加入这条边将产生环, 扔掉这条边  if(uf.IsConnected(e.V(), e.W())) {  continue;  }  // 否则, 将这条边添加进最小生成树, 同时标记边的两个端点联通  mst_.push_back(e);  uf.UnionElements(e.V(), e.W());  }  weight_ = mst_[0].Wt();  for (int i = 1; i \u0026lt; mst_.size(); ++i) {  weight_ += mst_[i].Wt();  }  }   ~KruskalMST() {}   // 返回最小生成树的所有边  vector\u0026lt;Edge\u0026gt; MSTEdges() {  return mst_;  }   // 返回最小生成树的权值  double MSTWeight() {  return weight_;  }   private:  vector\u0026lt;Edge\u0026gt; mst_; // 保存最小生成树的边  double weight_; // 最小生成树的权值  bool* marked_; };  #endif ","permalink":"http://landodo.github.io/posts/20201106-prim-kruskal/","summary":"Prim 算法\u0026amp;Kruskal 算法求最小生成树 最小生成树 概述：在一给定的无向图 G=(V,E) 中，(u,v) 代表连接顶点 u 与顶点 v 的边，而 w(u,v) 代表此边的权值","title":"Prim 算法\u0026Kruskal 算法求最小生成树"},{"content":"#论文阅读-NumPy# 论文名称：Array Programming with NumPy\n这是一篇关于 NumPy 的论文，最近也在学习 Python，NumPy 也是常用的一个包，可以花点时间读一读这篇论文。\n摘要 NumPy 是 Python 的一个主要的数组编程库（array programming library）。\n数组编程（Array Programming）为访问和操作向量、矩阵或高维数组中的数据提供了一种强大、紧凑的语法，在不同的领域的研究分析中发挥着重要的作用。\n本篇论文会介绍几个基本的数组概念，从而探索如何形成一个简单而强大的编程范式来组织、探索和分析科学数据。\nNumPy 是构建 Python 科学计算的基础。有一些特殊的项目，会针对性的开发出类似 NumPy 的接口和数组对象。\n历史 NumPy 之前 Python 有两个数组包，分别是 Numeric 和 Numarray。\nNumeric 始于 1995 年左右，它使用 C 语言结合线性代数的标准编写实现。最早是为了处理来自哈勃空间遥感仪的大型天文图像。\nNumarray 是对 Mumeric 的重写，增加了对结构化数组的支持、灵活的索引、内存映射、字节顺序变体（byte-order variants）、更有效的内存使用、灵活的 IEEE 错误处理能力以及更好的类型定义规则。\n这两个包相互兼容，但是在很多方面存在差异和分歧。\n2005 年，NumPy 作为二者的完美统一体诞生，将 Numarray 的丰富功能与 Numeric 在数组上的高性能及其丰富的 C 应用编程接口结合起来。\n15 年后的今天，NumPy 已经成为几乎所有科学计算的 Python 包的基础，包括 SciPy、Matplotlib、pandas、scikit-learn 和 scikit-image。\nNumPy 是一个社区开发的开源库，它提供了一个多维的 Python 数组对象以及对其进行操作的函数（array-aware functions）。\nNumPy 使用了 CPU 对内存中的数组进行操作。目前计算机的存储和硬件不断的升级，NumPy 也在不断的更新。\nNumPy 数组 NumPy 数组是一个高效存储和访问多维数组的数据结构，这也被称为张量（Tensors），可以进行各种科学计算。\n它由一个指向内存的指针，以及用于解释存储在那里的数据的元数据组成，特别是数据类型（data type）、形状（shape）和步长（strides）。\n如下图，显示了 NumPy 数组包含的几个基本的数组概念。\n a：NumPy 数组数据结构及其相关的元数据字段（metadata fields）。 b：用 slices 和 steps 对数组进行索引，这些操作返回原始数据的视图。 c：用掩码（）、标量坐标或其他数组对数组进行索引，使其返回原始数据的副本。 d：向量化高效地操作数组元素。 e：在二维数组的乘法中广播（broadcasts）。在这个例子中，一个数组沿选择 axes 求和产生一个向量，或者沿两个 axes 连续求和产生一个标量。 f：还原操作沿一个或多个 axes 进行。在这个例子中，一个数组沿着选定的 axes 求和产生一个向量，或者沿着两个 axes 连续求和产生一个标量。 g：NumPy 代码示例。  数组的形状决定了每个轴上元素的数量，轴的数量就是数组的维数。例如，一个矢量数字可以存储为一个 (N, 1) 的向量，彩色视频存储为 (T, M, N, 3)。\nStrides（步幅）是将线性存储元素的计算机内存解释为多维数组的必要条件。它描述了在内存中从一行跳到另一行，从一列跳到另一列等要向前移动的字节数。例如，考虑一个形状为 (4, 3) 的二维浮点数组，每个元素在内存中占据 8 个字节。要在相邻的列之间移动，则需要在内存中向前或向后移动 8 个字节，访问下一行需要移动 3×8=24 个字节（a）。\nNumPy 可以按照 C 一样的内存顺序进行存储数组，先迭代行或先迭代列，这使得用 C 语言编写的外部库可以直接访问内存中的 NumPy 数组数据。\n用户使用索引（indexing）访问子数组或单个元素、运算符（向量化）以及 array-aware functions 与 NumPy 数组进行交互。\n对数组进行索引，可以返回单个元素、子数组或满足特定条件的元素（b）。\n数组甚至可以使用其他数组进行索引（c）。\n只要有可能，检索子数组的索引就会返回原始数组的视图，这样两个数组之间就可以共享数据。这提供了一种强大的方法来操作数组数据的子集，同时限制了内存的使用。\n \u0026ldquo;Wherever possible, indexing that retrieves a subarray returns a view on the original array, such that data is shared between the two arrays. This provides a powerful way to operate on subsets of array data while limiting memory usage.\u0026rdquo;\n NumPy 包含了对数组进行矢量化（对整个数组进行操作）计算的函数，包括算术、统计和三角函数（d）。\n在 C 语言中需要几十行代码才能完成的操作，Python 只需要简洁清晰的一行代码。在内部的细节中，NumPy 以近乎最佳的方式处理数组元素的循环，考虑到例如步长（strides），以最好地利用计算机的快速缓存内存。\n当两个形状相同的数组执行向量化操作时，其结果显而易懂。当形状不同时，广播（broadcasting）机制可以实现相同的效果。\n举一个例子，将一个标量值加到一个数组上，广播机制可以将这个标量分别加到数组的每一个元素上。\n其他的 array-aware 函数，如：sum/mean/max ，创建（creating）、重塑（reshaping）、连接（concatenating）和填充（padding）数组，搜索、排序和计数数据，读写文件，随机数、各种概率分布等。\n Altogether, the combination of a simple in-memory array representation, a syntax that closely mimics mathematics, and a variety of array-aware utility functions forms a productive and powerfully expressive array programming language.\n Python 生态 Python 是一门开源的、通用的、解释型的编程语言，非常适合于标准的编程任务，如清理数据、与网络资源交互和解析文本。\nNumPy 为 Python 增加了快速数组运算和线性代数，使得科学家们可以在 Python 中完成所有的工作，而且 Python 具有的易学易教的优势，许多大学都将其作为主要学习语言。\nNumPy 不是 Python 标准库的一部分，因此 NumPy 能够决定自己的发布策略和开发模式。\nSciPy、Matplotlib 与 NumPy 有紧密的联系。SciPy 提供了科学计算的基本算法，包括数学、科学和工程例程。Matplotlib 用于数据的可视化。\nNumPy、SciPy 和 Matplotlib 的结合，再加上 IPython 或 Jupyter 这样的高级交互环境，为Python 的数组编程提供了坚实的基础。\n eht-imaging：用于射电干涉测量成像、分析和模拟。这个库依赖于 Python 生态系统的许多低级组件。如 NumPy 数组用于存储和处理每一步的数值数据：从原始数据到校准和图像重建。 scikit-image：SciPy 的拓展图像处理库，提供了更高层次的功能，如边缘过滤器和Hough变换。 NetworkX：一个用于复杂网络分析的软件包，用于验证图像对比的一致性。 Astropy：处理标准的天文文件格式，并计算时间/坐标变换  交互式编程环境非常适合探索性数据分析，用户可以流畅地检查、操作和可视化他们的数据，并快速迭代以完善编程语句。\n探索性之外的科学计算工作通常是在集成开发环境（IDE）中完成的。\n这一段我不太理解：（Keywords：分布式、自动化测试）\n To complement this facility for exploratory work and rapid prototyping, NumPy has developed a culture of employing time-tested software engineering practices to improve collaboration and reduce error [31]. This culture is not only adopted by leaders in the project but also enthusiastically taught to new- comers. The NumPy team was early in adopting distributed revision control and code review to improve collaboration on code, and continuous testing that runs an extensive battery of automated tests for every proposed change to NumPy. The project also has comprehensive, high-quality documentation, integrated with the source code [32, 33, 34].\n 大概的意思就是这一种新的机制非常的好，非常 nice。已经被建立在 NumPy 生态基础上的库所采用。例如，在英国皇家天文学会对 Astropy 库就给出了如下夸奖。\n “The Astropy Project has provided hundreds of junior scientists with experience in professional-standard software development practices including use of version control, unit testing, code review and issue tracking procedures. This is a vital skill set for modern researchers that is often missing from formal university education in physics or astronomy.”\n 目前数据科学、机器学习和人工智能发展迅猛，Python 被大规模的推广使用。现在在自然科学和社会科学领域中，几乎每一个学科都有一些库，这些库已经成为许多领域的主要软件环境。\nNumPy 及其生态是全球社区会议和研讨会的焦点，NumPy 和它的 API 已经变得真正的无处不在。\nArray proliferation and interoperability NumPy 既可以在嵌入式设备上运行，也可以在世界上最大的超级计算机上运行，其性能接近于编译语言。NumPy 从其诞生到现在，负责了绝大多数的数组科学计算。\n目前大部分的科学数据集通常会超过单台计算机的内存容量，这些数据集存储在多台机器上或云端。\n此外，最近深度学习和人工智能应用的需求导致了专门的加速器硬件的出现，例如，图形处理单元（GPU）、张量处理单元（TPU）和现场可编程门阵列（FPGA）。\nNumPy 目前还不能直接利用这种专用的硬件。\n However, both distributed data and the parallel execution of GPUs, TPUs, and FPGAs map well to the paradigm of array programming: a gap, therefore, existed between available modern hardware architectures and the tools necessary to leverage their computational power.\n 为了解决这个问题，每个深度学习框架都有针对性的创建了自己的数组（array）：PyTorch Tensorflow、Apache MXNet、JAX 等，这些数组实现都有能力以分布式方式在 CPU 和 GPU 上运行。\n此外，还有一些项目是建立在 NumPy 数组作为数据容器的基础上，并扩展其功能。这样的库通常会模仿 NumPy API，提供了稳定的数组编程接口，这能大大的吸引新人，或者降低学习的成本。\n这防止了 NumPy 重蹈 Numeric 和 Numarray 的破坏性分裂的覆辙。\n探索新的数组工作方式本质上是实验性的，目前几个比较有前途的库 Theano、Caffe 已经停止开发了。每次用户决定尝试一种新的库（框架）时，必须改变 import 语句，并确保新的库实现了他们目前使用的 NumPy API 的所有部分。\n在理想情况下，用户使用 NumPy function 或 semantics 编写一次代码，然后根据实际情况在NumPy 数组、GPU 数组、分布式数组等之间进行切换。NumPy 提供了一个规范完善的 API。\n为了促进这种互操作性，NumPy 提供了协议（protocols），允许将专门的数组传递给NumPy 函数（Fig. 3）。而 NumPy 则根据需要将操作分配给如 Dask、CuPy、xarray 和 PyData/Sparse。\n例如，用户现在可以使用 Dask 将他们的计算从单机扩展到分布式系统。\n关于大规模部署，我不太明白：\n The protocols also compose well, allowing users to redeploy NumPy code at scale on distributed, multi- GPU systems via, for instance, CuPy arrays embedded in Dask arrays. Using NumPy’s high-level API, users can leverage highly parallel code execution on multiple systems with millions of cores, all with minimal code changes [42].\n 这些数组协议现在是 NumPy 的一个关键功能，具有很高的重要性。与 NumPy 的其他部分一样，协议是在不断完善和增加的，以提高实用性和简化采用。\n总结 NumPy 将 array programming 的表达能力、C 语言的性能、Python 的易读性、可用性和通用性等优点结合在一起，形成了一个成熟的、经过良好测试的、有良好文档的、由社区开发的库。\nPython 生态中的库提供了大多数重要算法的快速实现。\n在需要极端的要求高性能的情况下，可以使用如 Cython、Numba 和 Pythran 等编译型语言，这些语言扩展了 Python 并透明地加速了瓶颈。\n由于 NumPy 的简单内存模型，可以使用低级编程语言如 C 来操作 NumPy 数组，然后将其传回给 Python。此外，使用数组协议，可以在对现有代码进行最小改动的情况下，利用全部的专用硬件加速代码运行。\nNumPy 最初是由学生、教师和研究人员开发的，目的是为 Python 提供一个先进的、开源的数组编程库，它可以免费使用。\n可以想象这样的场景：一群志同道合的人，为了共同的利益，一起建立了一些有意义的东西。\n这些最初的开发者使用 C 等低级的编程语言，参考了其他强大的科学计算交互式编程语言如 MATLIB 来编写代码。\n最初的版本是在 Python 中添加一个数组对象，到后来通过不断升级迭代，成为一个充满活力的工具生态的基础。现在，大量的科学工作都依赖于 NumPy。NumPy 已经成为了一个核心的科学基础设施。\n现在的 NumPy 项目开发流程已经成熟，这个项目有正式的管理结构，并由 NumFOCUS 提供财政赞助，NumFOCUS 是一个非营利性组织，旨在促进研究、数据和科学计算方面的开放实践。\n在赞助资金的支持下，该项目能够（也是）在多个月内持续专注于实现实质性的新功能和改进。尽管如此，它仍然在很大程度上依赖于研究生和研究人员在业余时间做出的贡献。\nNumPy 不再只是科学 Python 生态的基础数组库，而且已经成为张量（tensor）计算的标准 API，也是 Python 中数组类型和技术之间的协调机制。目前仍然在继续努力扩展和改进相关互操作性功能。\n在未来的十年，NumPy 将面临着不少的挑战。\n 设备仪器的升级，导致科学数据收集的规模继续扩大 专有硬件的提升不协调 新一代编程语言、解释器和编译器可能出现  NumPy 已经准备好迎接挑战，并继续在交互式科学计算中发挥领导作用。要做到这一点，需要政府、学术界和工业界的持续资助。但更重要的是，它还需要新一代的研究生和其他开发者的参与贡献，以建立一个满足下一个十年数据科学需求的 NumPy。\n方法 使用 Git 进行版本控制，项目托管在 GitHub，利用 GitHub 的 pull request（PR）机制，在合并代码之前进行审核，还使用 GitHub 的 issue 来收集社区用户的改进建议。\n库组织 NumPy 库由以下几个部分组成：\n 数据结构 ndarray 通用函数（universal functions） 一组用于操作数组和进行科学计算的库函数 单元测试和 Python 包构建的基础库 用于在 Python 中包装 Fortran 代码的程序 f2py  核心（Core）：ndarray 和通用函数一般被认为是 NumPy 的核心。\nndarray 是 NumPy 的核心数据结构。该数据结构在一个连续的块状内存中存储有规律的同质数据类型，允许有效地表示 n 维数据。\n通用函数（universal functions）： ufuncs 是使用 C 语言编写的函数，它实现了对 NumPy 数组的高效循环。ufuncs 的一个重要特点是内置了广播的实现。例如，函数 arctan2(x, y) 是一个接受两个值并计算 $$tan^{-1}(y/x)$$ 的 ufunc。\n计算库（Computing libraries）：\n  数组操作和科学计算的函数库。如：创建、重塑、连接和填充数组；搜索、排序和计算数组中的数据；计算基本的统计数据，如平均值、中位数、方差和标准差；文件 I/O 等。\n  计算快速傅立叶变换（FFT）及其逆。\n  求解线性代数问题，如：计算行列式、矩阵的逆、特征值、特征向量等。\n  可以生成服从特点概率分布（高斯分布、β、gamma 等）的随机数。\n  基础设施库（Infrastructure libraries）：\n  编写测试和构建 Python 包的工具（如 assert_allclose(actual, desired) 等函数）。\n  distutils：配置、安装和打包依赖 NumPy 的库。例如，当发布到 PyPI 网站时，会使用到。\n  F2PY：F2py 是一个用于构建 NumPy-aware Python 封装 Fortran 函数的工具。\n治理 NumPy 于 2015 年 10 月 5 日通过了一份正式的治理文件，用于监督和审查 NumPy 日常的发展。 NumPy 的官方行为准则于 2018 年 9 月 1 日获得批准。\n这份行为准则主要体现：开放、富有同情心、欢迎、友好和耐心、具有协作精神、具有探究精神等。\n资金 2017 年，NumPy 从 Gordon \u0026amp; Betty Moore 和 Alfred P. Sloan 基金会获得了130 万美元的资助。\n2019 年，Chan Zuckerberg 资助了 19.5 万美元。\n自 2019 年 5 月起，每年都会从 Tidelift 获得少量资金，用于文档和网站改进等工作。\n开发者 NumPy 目前由 23 名拥有 NumPy 代码库提交权的贡献者维护。\n在 NumPy 的历史上，已经有 823 个贡献者的 PR。但是它的发展主要还是依赖于少数活跃的维护者，这些人贡献了大部分的代码。\nNumPy 大约每半年一次更新，历年的 PR 情况如 Fig.4 所示。\n社区 有大量的 Python 包以 NumPy 作为基础，因此需要 NumPy 具备很高的稳定性。\nNumPy 的新功能开发流程，会吸取社区的贡献和反馈。\n每周一次的社区电话会议。\n加强 NumPy 的建议 NumPy 代码库是非常复杂的，还有大量依赖于它的项目。因此，在进行修改时，需要吸取社区的反馈和意见。\nNumPy 增强提案（NEP）是在 Python 增强提案（PEP）的基础上建模的，用于“提出重大新功能，收集社区对某个问题的意见以及记录已纳入 Python 的设计决策”。\n核心角色 NumPy在构建和标准化科学Python社区的大部分基础设施方面发挥了核心作用。\nPython 社区的大部分基础设施对于 NumPy 的构建和标准化起到了核心作用。\nNumPy 的 docstring 标准现在被广泛采用。\n科学 Python 生态的所有项目都采用一个共同的 \u0026ldquo;time window-based\u0026rdquo; 策略支持 Python 和 NumPy 版本。\nWheels build system Python wheel 是一种标准的文件格式，用于分配 Python 库。\nwheel 还可以包括编译后的 C 扩展和其他二进制数据。包括 NumPy 在内的许多库都需要 C 编译器和其他构建工具来从源代码进行安装，这使得许多用户很难自行安装软件。\n在 Python 打包系统中引入了 wheel 后，使得用户安装预编译的库变得更加容易。\n这个系统使得用户可以很容易地在这些平台上安装预编译版本的 NumPy。\nWheel（造轮子？）还在不断发展中。\n近期的技术改进 测试基础架构的改变，以支持大规模计算中使用的硬件平台。\n数组函数协议 目前 NumPy 已经无法满足某些方面的需求，因此，出现了众多的专业项目。\n如：PyTorch 和 Tensorflow 提供了具有 NumPy 启发语义的张量 API。\nDask 是一个在 Python 中用于并行计算的库。Dask 采用了 NumPy API，因此为现有的 NumPy 用户提供了一个熟悉的界面，同时增加了强大的并行化和分配任务的能力。\n使用 NumPy 类 API 的用户要针对一个项目编写特定的代码，这对于依赖某个特定框架的用户来说是一个负担，因为他们需要的工具可能不适合他们。\n这也给那些需要从 NumPy 过渡到更专业的数组的用户带来了挑战。越来越多的具有 NumPy-like API 的专业项目（框架）有可能再次使科学 Python 社区出现分裂。\n为了解决这些问题，NumPy 的目标是为各种 NumPy-like 的 API 之间的互操作性提供基本的 API。\n较早的 __array_ufunc__ 协议，它使大多数数学函数的互操作性得以实现。\n2019 年， __array_function__ 协议被纳入 NumPy 1.17 中。\n下面是通过 NumPy 对 CuPy GPU 数组进行处理，所有的操作都被派发回 CuPy。\nimport numpy as np import cupy as cp x_gpu = cp.array([1, 2, 3]) y = np.sum(x_gpu) # Returns a GPU array 同样，使用 NumPy 组成的用户定义函数现在也可以应用于多节点分布式 Dask 数组。\nimport numpy as np import dask.array as da  def f(x):  \u0026#34;\u0026#34;\u0026#34; Function using NumPy API calls \u0026#34;\u0026#34;\u0026#34;  y = np.tensordot(x, x.T) return np.mean(np.log(y + 1))  x_local = np.random.random([10000, 10000]) # random local array x_distr = da.random.random([10000, 10000]) # random distributed   array f(x_local) # returns a NumPy array f(x_distr) # works, returns a Dask array 随机数生成 NumPy 的 random 模块提供了各种分布的伪随机数。在 NumPy 的遗留版本中，模拟随机值是由一个 RandomState 对象产生的。\nRandomState 对象做出了兼容性保证，固定的种子和函数调用序列就会产生相同的数据集合。\nNumPy 1.17 引入了一个新的生成随机数的 API，它使用了一个更灵活的结构，可以由库或用户扩展。新的 API 使用组件构建，这些组件将生成随机变量所需的步骤分开。伪随机位由位发生器（a bit generator）生成。然后，这些比特由一个生成器转化为来自复杂分布的变量。最后，种子由一个对象处理，该对象产生高质量的初始值序列。\n比特生成器（Bit generator）是简单的类，管理底层伪随机数的状态。\nNumPy 有四种比特生成器：\n 默认的生成器：Permuted Congruential Generator Philox generator Chris Doty-Humphrey 的 Small Fast Chaotic generator 32-bit Mersenne Twister  比特生成器在 Python 和 C 中提供了公开的函数，用于生成随机整数和浮点数。\n生成器（Generator）消耗其中一个比特位生成器，生成复杂分布的数。\n还有很多分布：正态、指数，以及用于有界随机整数生成的 Lemire 方法。Generator 与传统的 RandomState 较为相似，其API也基本相同。\n最后，SeedSequence 用于初始化位发生器。种子序列可以在没有参数的情况下初始化，在这种情况下，它从一个依赖于系统的提供者那里读取熵，或者用用户提供的种子来初始化。\n以上三个组件组合起来，构建了一个完整的随机数发生器。\nfrom numpy.random import (  Generator,  PCG64, # 默认生成器  SeedSequence, ) seq = SeedSequence (1030424547444117993331016959) pcg = PCG64(seq) gen = Generator(pcg) 这种方法保留了对种子序列的访问权，然后可以用来产生更多的生成器。\nchildren = seq.spawn(2) gen_0 = Generator(PCG64(children[0])) gen_1 = Generator(PCG64(children[1])) 虽然这种方法保留了完整的灵活性，但当不需要重现性的时候，可以使用np.random.default_rng 方法来实例化一个 Generator。\n新 API 的最终目标是提高可扩展性。\nRandomState 掩盖了所有的底层状态和功能。\n在多种架构上测试 Summit 和 Sierra 这两台超级计算机都采用了IBM POWER9 架构。\n截至 2017 年，ARM 处理器的产量已经超过1000 亿个，成为世界上应用最广泛的指令集架构。\nNumPy 这个科学计算软件库在 POWER 和 ARM 进行了测试。\n ppc64le（Travis CI上 的 POWER8） ARMv8（Shippable 服务） s390x 架构（Travis CI上的 IBM Z CPU）  PEP 599 提出了一个新的 Python 二进制 wheel 发行支持计划。\nmanylinux2014 增加了对 ARMv8、pc64le 以及 s390x 的支持。\nNumPy 已经做好了准备，未来在这些架构上提供的二进制文件，并成为一个基础库。\n","permalink":"http://landodo.github.io/posts/20201104-paper-numpy/","summary":"#论文阅读-NumPy# 论文名称：Array Programming with NumPy 这是一篇关于 NumPy 的论文，最近也在学习 Python，NumPy 也是常用的一个包，可以花点时间读","title":"Array Programming with NumPy"},{"content":"SVM 笔记 1: $\\min_{w,b}\\frac{1}{2}\\left\\Vert w \\right\\Vert^2_2$ 的推导 学习率应该怎样去选择？\n 鲁莽司机（Bold Driver） 每轮之后在一个 dataset 上进行评价 $$\\alpha_t=\\frac{\\alpha_0}{1+t/T}$$，学习率随着 t 不断下降  分类任务\n  Linear SVM：最简单的 SVM 的形式\n  Kernelized SVM：核化的一种 SVM 方法\n  回归任务\n SVR（Support Vector Regression）  SVM 这个方法用得并没有那么广泛，这个算法最大的一个好处就是它比较稳定。从 ML 上来看，它是一个很漂亮的方法，它背后有很好的思想。\n泛化能力的分析（Generalization Analysis），有两本书，一本厚的、一本薄的，其中 SVM 就是其中一个核心的算法。现有的理论，后来才有的算法。\n Generalization Analysis: Vapnik, the nature of Statistical Learning Theory, Spinger, 1995. Generalization Analysis: Vapnik, Statistical Learning Theory, Wiley\u0026amp;Sons, 1998.  SMO：下一节课会讲\n工具包：\n SVM Light LIBSVM Multi-Class SVM and StructSVM：多分类、结构化数据  什么是 SVM？它究竟在做些什么事情？ 从最简单的情况出发，假设数据线性可分。\n如下的直线都可以完美的将数据分开，分类面不唯一，所有分类面的训练误差都等于 0，那么该如何选择分类面呢。\n应该选择泛化误差最小的那一个。那么怎么去考量泛化误差最小的呢？正确和错误之间有一个较大的容忍度，才能容忍更多的测试数据。\n从肉眼来看，这条黑色的线是最好的，因为它之间的间隔充分的大，容忍区域宽。这是从几何的角度看的。\n怎么样将这种思想用一种可计算的方式表达出来呢？从而有一个算法能够自动的找到那条黑色的线。这是 SVM 算法的一个出发点。\n从以前的算法，看看能得到什么启示。\n逻辑回归（Logistic Regression） 逻辑回归是一个概率模型，概率模型的好处是：每次算给定的 x 的标签的时候，事实上算的是一个概率，有了概率之后，就有了一个叫做置信度的概念。\n有了概率就可以做极大似然估计。置信度告诉我们分类结果有多大的把握。\n$$P(y = 1|x)=\\frac{1}{1+e^{(-w^Tx)}}$$\n概率模型都会有一个置信度。\n置信度和前面的那条黑色的分割线有什么关系？可以理解为：黑色的线有较大的置信度。\n在没有概率模型的情况下，该怎么定义置信度呢？这就是 SVM 最初要解决的东西：如何定义置信度。\nSVM 通过一个叫做 Margin 的东西来定义置信度。SVM 方法也叫做 Max Margin。\n注意，最大化Margin是一种思想，SVM 只是其中一个典型的代表。\nMargin 刻画置信度这一概念。\nMargin 在数学上的表达  假设在做一个二分类问题，特征是 x，标签是 y。 用一个线性的模型来做分类  $$f_{w,b}(x) = g(w^Tx+b).$$ 当 $$w^Tx+b\u0026gt;=0$$ ，则为类 1；否则为类 -1.   分类面：$$w^Tx+b = 0$$  接下来来定义Margin Margin，衡量一个点到一个分类面的一个远近程度。\n 函数Margin（Functional Margin）  对于每一个训练数据$$(x^{(i)},y^{(i)})$$，得到一个值，如下就是函数 Margin 的。\n$$\\check{\\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)}+b)$$\n$$\\check{\\gamma}^{(i)}$$是一个大于 0 的值，它不是距离，但是和距离有一定关系。\n 因为如果标签 $$y^{(i)}=1$$，则需要 $$w^Tx+b$$ 是一个正的值 因为如果标签 $$y^{(i)}=-1$$，则需要 $$w^Tx+b$$ 是一个负的值  函数 Margin 只是想象中有一个量去尽可能的刻画置信度的作用，什么时候可以把它真正定义成一个距离呢？这就是几何 Margin 所定义的内容。\n 几何 Margin（Geometric Margin）  假设 A 点的坐标是 $$x^{(i)}$$，它与 $$\\vec{OA}$$ 等价，一个样本可以理解为空间中的一点，不同维度对应它的不同特征。\n则 B 点的坐标可以通过 A 点推导出来。\n$$B 点的坐标 = x^{(i)}-\\gamma^{(i)}w/\\left\\Vert w \\right\\Vert_{2}$$\n 这个式子的推导如下：\n$$\\frac{w}{\\left\\vert w \\right\\vert}$$ 为 $w$ 方向上的单位向量。\n$$\\gamma^{(i)}w/\\left\\vert w \\right\\vert$$ 得到的是 $\\vec{BA}$\n我们知道 $$\\vec{OB} = \\vec{OA} + \\vec{AB}$$，$$\\vec{OB}$$ 等价于 B 点的坐标。\n因此 $$\\vec{OB} = \\vec{OA} - \\vec{BA}$$\n得到式子：$$\\vec{OB} = x^{(i)}-\\gamma^{(i)}w/\\left\\Vert w \\right\\Vert_2$$. 推导完成。\n注：$$\\left\\Vert \\right\\Vert_2$$ 相当于 $$\\left\\vert \\right\\vert$$.\n B 在判别超平面上，将 B 点的坐标代入得：\n$$w^T(x^{(i)}-\\gamma^{(i)}\\frac{w}{\\left\\Vert w \\right\\Vert_2}) + b = 0$$\n求得 $$\\gamma^{(i)}$$\n$$\\gamma^{(i)} = \\frac{w^Tx^{(i)}+b} {\\left\\Vert w\\right\\Vert_2}=(\\frac{w}{\\left\\Vert w \\right\\Vert_2})^T x^{(i)}+\\frac{b}{\\left\\Vert w \\right\\Vert_2}$$\n观察这个式子可以看到就是在 $$wx+b$$ 上系数除以一个范数。\n$$\\gamma$$ 和 $$\\check{\\gamma}$$ 成一个比例的关系。\n$$\\gamma^{(i)}= \\frac{\\check{\\gamma}^{(i)}}{\\left\\Vert w \\right\\Vert_2}$$\n$$\\gamma= \\frac{\\check{\\gamma}}{\\left\\Vert w \\right\\Vert_2}$$\n这样的话，就可以将训练数据的几何 Margin 给计算出来。\n如果 $$\\left\\Vert w\\right\\Vert_2 = 1$$，则函数 Margin 和几何 Margin 是相同的。\n几何 Margin 会用得更多，一方面，几何的概念反映了一个距离；另一方面，几何 Margin 有一种不变性，它除了范数，所以随便 w 怎么变，它不变。\n函数 Margin 是一个相对距离，几何 Margin 是一个绝对距离。\n置信度就是由几何 Margin 来决定的。\n算法就可以表示出来了。\n$$max\\quad\\gamma$$\n$$s.t. \\quad y^{(i)}\\frac{w^T}{\\left\\Vert w \\right\\Vert_2} x^{(i)}+\\frac{b}{\\left\\Vert w \\right\\Vert_2}\u0026gt;=\\gamma,\\quad i=1,\u0026hellip;,n$$\n这就是在最简单的情况下（线性可分）的一个 SVM 原始问题。\n将 SVM 写成一个更容易求解的形式 有两个步骤。\n 利用几何 Margin 和函数 Margin 之间的关系得：  $$\\max_{\\check{\\gamma}, w,b} \\quad \\frac{\\check{\\gamma}}{\\left\\Vert w \\right\\Vert_2}$$\n$$s.t. y^{(i)}(w^Tx^{(i)}+b)\\geqslant\\check{\\gamma}\\quad i=1,\u0026hellip;,n$$\n取一个合适的 w，使得 $$\\check{\\gamma}=1$$， $$\\left\\Vert \\right\\Vert^2$$ 是为了更好的优化（平方），1/2 是为了计算的简便。得到简化的式子：  $$\\min_{w,b}\\frac{1}{2}\\left\\Vert w \\right\\Vert^2_2$$\n$$s.t. y^{(i)}(w^Tx^{(i)}+b)\\geqslant 1 \\quad i=1,\u0026hellip;,n$$\n这就是平常看到的最熟悉的 SVM 算法。要优化的参数是 $w,b$，目标是 Margin 最大。条件是每一个训练数据都分对了，$$\\geqslant 1$$ 的意思是支持向量内没有任何样本，也就是说这是一个 Hard Margin。\n关于 SVM 的推导（Hard Margin/Soft Margin）我还会再写一篇通俗易懂的文章。\n老师的 PPT：\n","permalink":"http://landodo.github.io/posts/20201103-svm-notes/","summary":"SVM 笔记 1: $\\min_{w,b}\\frac{1}{2}\\left\\Vert w \\right\\Vert^2_2$ 的推导 学习率应该怎样去选择？ 鲁莽司机（Bold Driver） 每轮之后在一个 dataset 上进行评价 $$\\alpha_t=\\frac{\\al","title":"SVM 支持向量机推导"},{"content":"[2021.02.04] 第三层用 384 个大小为 $3\\times3\\times256$ 的核进行卷积。\n但是每个 GPU 的前一层的特征图都是 $27\\times 27 \\times 128$，通道数是不同的。因此是要将两块 GPU 上的特征图进行 concat，得到通道数为 256，之后正常进行卷积，每块 GPU 使用 384/2 = 192 个卷积核。\n2021.01.19 AlexNet 把 feature map 分给 2 块 GPU 分别进行训练，分组卷积（Group Convolution）的最早出现。我的疑问是：输入的图片（特征图）为 $224 \\times 224 \\times 3$，3 个通道，怎么分呢？是一块 GPU 处理两片特征图，另一块处理 1 片吗？\n The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU.\n  本次阅读的论文是《基于深度卷积神经网络的图像网络分类》（ImageNet Classification with Deep Convolutional Neural Networks [1]）。\n这篇论文的第一作者名字叫做 Alex，所以论文中提出的模型也经常被称为 AlexNet。Alex Krizhevsky 发表这篇论文的时候，他正在多伦多大学计算机系攻读博士学位。\n ImageNet Classification with Deep Convolutional Neural Networks AlexNet\n✅ 论文地址：http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\n✅ 发表时间：2012 年\n论文的结构  Abstract\n  Introduction\n  The Dataset\n  The Architecture\n  Reducing Overfitting\n  Details of learning\n  Results\n  Discussion\n  数据集 ImageNet 是一个包含超过 1500 万张已标记的高分辨率图像数据集，大约有 22000 个类别。\nImageNet 大规模视觉识别挑战赛（ILSVRC）使用的数据集是 ImageNet 的一个子集，共有 1000 个类别，每个类别中大约包含 1000 张图像。 总共大约有 120 万张训练图像，5 万张验证图像和 15 万张测试图像。\n本论文的网络模型参加了 ILSVRC-2012 的比赛，论文的最后列出了模型在比赛的数据集上的表现情况。\n这里有两个衡量指标：top-1 和 top-5。\n其中 top-5 错误率指的是正确标签不在模型认为的最有可能的五个标签中的测试集比例。\n the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\n 模型要求输入的维度是固定的，因此需要对 ImageNet 中的可变分辨率图像进行一个采样，使其分辨率固定为 $256\\times256$。\n网络结构 网络包含 5 个卷积层和 3 个全连接层。\nReLU 激活函数：\n$$tanh(x) = \\frac{e^{x} - e^{-x}}{e^x + e^{-x}}$$\n$$sigmoid(x) = (1 + e^{-x})^{-1}$$\n$$ReLU(x) = max(0, x)$$\nReLU（ Rectified Linear Units）：修正线性单元。使用 ReLU 激活函数的深度卷积神经网络的训练速度比使用 tanh 要更快。\n下图显示了网络在 CIFAR-10 数据集上达到 25% 的训练误差所需要的迭代次数。使用了 ReLU 的四层卷积神经网络（实线）在 CIFAR-10 上达到 25% 的训练错误率，比使用 tanh 的等效网络（虚线）快 6 倍。\n在多块 GPU 上进行训练 通过实践发现，网络规模过大的情况，只用一块 GPU 可能装不下。这篇论文将网络分散在两个 GPU 上。目前的 GPU 能够直接从对方的内存中读取和写入，因此特别适合跨 GPU 并行化工作。另外，这两块 GPU 只在某些层中进行通信。\n与在一个 GPU 上训练的每个卷积层有一半数量的内核的网络相比，两块 GPU 的 top-1 和 top-5 错误率分别降低了 1.7% 和 1.2%。\n This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU.\n 双 GPU 网络的训练时间比单 GPU 网络略少。\n局部响应归一化（LRN） 网络使用了一种叫做 Local Response Normalization 的策略来提升模型的泛化能力。\n响应归一化使得网络的 top-1 和 top-5 错误率分别降低了 1.4% 和 1.2%。在 CIFAR-10 数据集上：一个四层的 CNN 在不进行归一化的情况下，测试错误率为 13%，而在进行归一化后，测试错误率为 11% 。\n池化 非重叠池化：对于卷积核 $z\\times z$，则步幅 $s == z$；重叠池化：卷积核 $z\\times z$，步幅 $s \u0026lt; z$。\n本篇论文使用重叠池化，最后得出：采用重叠池的模型能一定程度上避免过拟合。\n相比与非重叠池化，重叠池化能将 top-1 和 top-5 的错误率分别降低 0.4% 和 0.3%。\n网络结构 如图所示，网络有 8 层，前 5 个卷积层，其余的是全连接层。一块 GPU 负责图像的上部分图层，另一块 GPU 负责下部分图层。网络的输入为 $150528=(224\\times224\\times3)$ 维的向量。\n最后一个全连接层的输出被输入到一个有 1000 类的 Softmax。\n第 2、4 和 5 层卷积层的卷积核只与前一层中驻留在同一个 GPU 上的卷积核映射相连接。\n The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU.\n 第 3 层卷积层两块 GPU 发生数据通信。\n全连接层的神经元与前一层的所有神经元相连。\n响应归一化层（LRN）在第 1 和第 2 卷积层之后。\n最大池化层只在响应归一化层以及第 5 卷积层之后。\nReLU 非线性激活函数应用于每个卷积层和全连接层的输出。\n第一层用 96 个大小为 $11\\times11\\times3$ 的卷积核对 $224\\times224\\times3$ 的输入图像进行卷积，步幅为 4 像素。\n第二层用 256 个大小为 $5\\times5\\times48$ 的核进行卷积。\n第三层用 384 个大小为 $3\\times3\\times256$ 的核进行卷积。\n第四层用 384 个大小为 $3\\times3\\times192$ 的核进行卷积。\n第五层用 256 个大小为 $3\\times3\\times192$ 的核进行卷积。\n全连接层每层有 4096 个神经元。\n2020.11.09：论文好像没有提到 Padding，给一个吴恩达老师的版本吧。我看很多不同的代码，它们第一层有的 Padding 了，并且使用的是 64 个 $$3\\times3$$ 的卷积核，不知道结果是否是相同的。\n减少过拟合 神经网络模型有 6000 万个参数，ImageNet 的 1000 个类别数据集学习这么多的参数仍会产生过拟合。\n数据增强 减少图像数据过拟合的最简单也是最常见的方法是人为的放大数据集。通过原始图像产生转换图像，转换过程只涉及很少的计算量，转换后的图像是在 CPU 上用代码生成的，不需要存储在磁盘。\n数据增强的第一种方式是：生成图像转换（image translations）和水平反射（horizontal reflections）。\n通过从 $256\\times256$ 的图像上随机的提取 $224\\times224$ 的区域块，再水平翻转，这使得训练集的大小增加了 $$2048 = (256-224)^2\\times2$$ 倍。\n第二种数据增强的方式是：改变训练图像中 RGB 通道的强度。\n通过对整个 ImageNet 训练集的 RGB 像素集进行 PCA 后，对于每张图片，添加一个均值为 0 ，标准差为 0.1 的高斯。这种方案近似地抓住了自然图像的一个重要特性，即物体标识对照明强度和颜色的变化是不变的。这种方案可以将 top-1 的错误率降低 1% 以上。\nDropout 将每个隐藏神经元的输出乘以 0.5 的概率设置为 0（随机失活），丢弃一些神经元，这种方法丢弃的神经元不再参与前向传播和反向传播。\n因此，对于每次输入，神经网络都会是不同的网络结构，但仍然共享参数。\n这种策略减少了神经元之间复杂的共同适用性（complex co-adaptations ），一个神经元不能过度依赖于特定的其他神经元，因为每个神经元可能被丢弃。每个神经元会学到更加健壮的特征。\n以 0.5 的概率随机失活是一个合理的近似值。\nDropout 在训练过程中只需要花费大约 2 倍的成本，会使收敛所需的迭代次数增加约一倍。\n学习的细节 本篇论文使用随机梯度下降法训练模型，其中的一些超参数设置如下：Batch 的大小为 128，momentum 设置为 0.9，权重衰减为 0.0005。\n权重衰减不仅仅是一个正则化：它降低了模型的训练误差。权重 w 的更新规则是：\n$i$ 是迭代次数，$v$ 是动量变量，$\\epsilon$ 是学习率，\n$$\\langle \\frac{\\partial L}{\\partial w}|{w_i}\\rangle{D_i}$$\n是目标对于 $w$ 的导数在第 $i$ 个批次 $D_i$ 上的平均值，评价为 $w_i$ （is the average over the $i$ th batch $D_i$ of the derivative of the objective with respect to $w$, evaluated at $w_i$. ）\n权重初始化 以标准差为 0.01、均值为 0 的高斯分布初始化每层的权重。第 2、4、5 层以及全连接层的偏置初始化为 1，其余的神经元偏置初始化为 0。这种初始化通过为 ReLU 提供正输入来加速学习的早期阶段。\n学习率 所有层使用了相同的学习率，在整个训练过程中进行手动调整。遵循的启发式方法是：当验证误差率在当前学习率下停止改善时，将学习率除以 10。\n学习率初始化为 0.01，并在终止前降低三次。通过 120 万张图像的训练集对网络进行了大约 90 个周期的训练，在两颗 NVIDIA GTX 580 3GB GPU 上花了 5 到 6 天时间。\n结果 在 ILSVRC-2010 上的结果如表 1 所示。\nILSVRC-2012 比赛结果如表 2。\n最后，在 2009 年秋季版 ImageNet 上的错误率，共有 10184 个类别和 890 万张图片。\n使用一半的图像进行训练，一半的图像进行测试。由于没有既定的测试集，这种的拆分必然与前人使用的拆分不同，但这并不会明显影响结果。\n在这一数据集上的 top-1 和 top-5 错误率分别为 67.4% 和 40.9%。\n如果在最后一个池化层上增加第六个卷积层，则在这个数据集上的最佳结果是 78.1% 和 60.9%。\n定性评估 Figure 3 显示了网络的两个数据连接层学习到的卷积核。这个网络已经学习了各种频率和方向选择的核。\n这两块 GPU 表现出了不同分工。\nFigure 4：（左）八张 ILSVRC-2010 测试图像和模型认为最有可能的五个标签。正确的标签写在每张图像下面，分配给正确标签的概率也用红条显示（如果它恰好在前5名）。\n右）第一列是五张 ILSVRC-2010 测试图像。其余的几列显示了在最后一个隐藏层中产生特征向量的六张训练图像，这些图像与测试图像的特征向量的欧氏距离最小。\n在图 4 的左图中，通过计算其在 8 张测试图像上的前 5 名预测结果，定性评估网络已经学到了什么。\n右上角的螨虫（mite）虽然偏离图像的中心，但是仍可以被网络正确识别分类。\n大多数前 5 名的标签都显得很合理。少数情况，例如：马达加斯加的猫被认为是猴子、在某些情况下（格栅、樱桃），照片的预期焦点确实会影响分类结果。\n探究网络视觉知识的另一种方法是考虑图像在最后的一个 4096 维向量。如果两幅图像产生的特征激活向量具有较小的欧式距离，则可以认为它们是相似的。\nFigure 4 (left) 显示了来自测试集的五幅图像和来自训练集的六幅图像，根据欧式距离的衡量标准，它们各自的相似度最高。\n Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the results for many more test images in the supplementary material.\n 使用两个 4096 维实值向量来计算欧式距离是低效的。可以通过训练自动编码器将这些向量压缩成短二进制代码来提高效率。\n总结 结果表明，一个大型的深度卷积神经网络能够在一个高度挑战的数据集上使用纯监督学习实现破纪录的结果。\n如果去掉一个卷积层，此网络性能就会下降。例如，去掉任何一个中间层会导致网络的 top-1 性能损失约 2%。所以深度对于实现的结果确实很重要。\n为了简化，本实验没有使用任何无监督的预训练。\n","permalink":"http://landodo.github.io/posts/20201024-paper-alexnet/","summary":"[2021.02.04] 第三层用 384 个大小为 $3\\times3\\times256$ 的核进行卷积。 但是每个 GPU 的前一层的特征图都是 $27\\times 27 \\times 128$，通道数是不同的。因此是要将两块 GPU 上的特征图进行 concat","title":"ImageNet Classification with Deep Convolutional Neural Networks"},{"content":"深度学习基础图书 今天又花了点时间看了一本书（第二遍）——《深度学习入门——基于 Python 的理论与实践》。\n  全书下载地址：https://github.com/LeoLiu8023AmyLu/Machine_Learning\n  推荐阅读：Hacker\u0026rsquo;s guide to Neural Networks (Andrej Karpathy blog): http://karpathy.github.io/neuralnets/\n  全书 ≈300 页，通篇看下来涉及的数学基础非常少，读起来十分通畅，一气呵成。\n这本书给我最大的收获是「误差反向传播基于计算图的推导」，把反向传播的推导细节讲得清清楚楚，让我对反向传播的推导有了更加深刻的认识。\n ✅ 计算图理解反向传播我在其他的书也有看到，但是我认为《深度学习入门——基于 Python 的理论与实践》这本书讲得最好。\n 图书行文的组织也是非常值得学习，排版、图片实属上乘。\n如果有一定的基础的话，2~3 个小时，可以全部看完看懂。\n照例，如下是我的读书笔记。记录下我第二遍阅读时的一些标注的地方，整理为 Markdown 形式的笔记。\n我的笔记内容 1. Python   这本书从零开始实现深度学习。因此，除了 NumPy 库和 Matplotlib 库之外，避免使用外部库。\n   关于从零开始编程实现深度学习，我认为最好的开始是使用吴恩达老师深度学习课程配套的实验。\n   NumPy：数值计算。（另一本书：《利用 Python 进行数据分析》）\n  Matplotlib：画图\n     Python 的 list，切片（slicing）是一个重点。\n  Python 广播（broadcast）    2. 感知机（Perceptron）   机器学习的课题就是将决定参数值（$w, b$）的工作交由计算机自动进行。学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机。\n  $b$ 称为偏置：是调整神经元被激活的容易程度的参数。$w$ 是权重：是控制输入信号的重要性的参数。\n  异或门：感知机通过叠加层能够进行非线性的表示。\n   单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。  3. 神经网络   神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。\n  参数 $b$ 被称为偏置，用于控制神经元被激活的容易程度； $w1, w2$ 是表示各个信号的权重的参数，用于控制各个信号的重要性。\n  激活函数（Activation Function）$h(\\cdot)$：作用在于决定如何来激活输入信号的总和。\n $a = b + w_1 x_1 + w_2 x_2$ $y = h(a)$    激活函数是连接感知机和神经网络的桥梁。\n  感知机中使用了阶跃函数作为激活函数。如果感知机中将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了。\n  解决 Softmax 的溢出问题：\n  4. 神经网络的学习   “学习”是指从训练数据中自动获取最优权重参数的过程。学习的目的就是损失函数为基准，找出能使它的值达到最小的权重参数。\n  传统的手工特征工程：SHFT、SURF 和 HOG。\n  损失函数：均方误差（mean squared error）交叉熵误差（cross entropy error）。\n  导数：如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。\n  偏导数：需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。全部变量的偏导数汇总而成的向量称为梯度（gradient）。梯度指示的方向是各点处的函数值减小最多的方向。\n  ✅ 如果 $W \\in \\mathbb{R}^{2 \\times 3}$，则 $dW \\in \\mathbb{R}^{2 \\times 3}$。如果 $\\frac{\\partial{L}}{\\partial{W}}$ 中的 $\\frac{\\partial{L}}{\\partial{w_{11}}}$ 的值大约为 0.2，这表示如果将 $w_{11}$ 增长 h，那么损失函数的值会增加 0.2h。如果 $\\frac{\\partial{L}}{\\partial{W}}$ 中的 $\\frac{\\partial{L}}{\\partial{w_{23}}}$ 的值大约为 -0.5，这表示如果将 $w_{23}$ 增长 h，那么损失函数的值会减少 0.5h。因此，从减小损失函数值的观点来看，$w_{23}$ 应向正方向更新，$w_{11}$ 应向负方向更新。至于更新的程度，$w_{23}$ 比 $w_{11}$ 的贡献要大。\n  5. 误差反向传播法（计算图）  要正确理解误差反向传播法，作者认为有两种方法：\n(1) 一种是基于数学式；\n(2) 另一种是基于计算图（computational graph）。\n前者是比较常见的方法，机器学习相关的图书中多数都是以数学式为中心展开论述的。因为这种方法严密且简洁，所以确实非常合理，但如果一上来就围绕数学式进行探讨，会忽略一些根本的东西，止步于式子的罗列。因此，作者希望通过计算图，帮助读者直观地理解误差反向传播法。\n 通过计算图来理解误差反向传播法这个想法，参考了 Andrej Karpathy 的博客和他与 Fei-Fei Li 教授负责的斯坦福大学的深度学习课程 CS231n。\n计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。\n计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。\n一个最简单的计算图，如下图所示：\n反向传播的计算顺序是，将信号 E 乘以节点的局部导数 $( \\frac{\\partial(y)}{\\partial(x)} )$，然后将结果传递给下一个节点。\n在来看一个计算图的例子（图 5-7、5-8）：\n反向传播计算图 （1）加法节点的反向传播\n以 $z = x + y$ 为对象，$\\frac{\\partial{z}}{\\partial{x}} = 1,\\frac{\\partial{z}}{\\partial{y}} = 1 $。\n加法节点的反向传播只乘以 1，所以输入的值会原封不动地流向下一个节点。\n（2）乘法节点的反向传播\n这里我们考虑 $z = xy$。这个 式子的导数：$\\frac{\\partial{z}}{\\partial{x}} = y,\\frac{\\partial{z}}{\\partial{y}} = x $。\n乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值” 后传递给下游。翻转值表示一种翻转关系，如下图 5-12 所示，正向传播时信号是 x 的话，反向传播时则是 y；正向传播时信号是 y 的话，反向传播时则是 x。\n（3）ReLU 层\n如果正向传播时的输 入 x 大 于 0 ，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。用计算图表示的话，如下图 5-18 所示。\n（4）Sigmoid 层\n（5）全连接层（Affine 层）\n将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。\nnp.dot(X, W) + B （6）Softmax-with-Loss 层\nSoftmax 反向传播的推导。图 5-29 为前向传播和反向传播计算图。\n书中给出了详细的 Softmax 推导步骤。\n（7）Batch Normalization\nFrederik Kratzert 的􏶸博客 “Understanding the backward pass through Batch Normalization Layer” 里有详细说明。\n6. 与学习相关的技巧   SGD 低效的根本原因是：梯度的方向并没有指向最小值的方向。如果函数的形状非均向（anisotropic），比如呈延伸状，SGD 可能就会呈”之“字型移动，搜索的路径就会非常低效。\n  将权重初始值设为 0 的话，将无法正确进行学习。\n  不能将权重初始值设为 0 呢？严格地说，为什么不能将权重初始值设成一样的值呢?\n这是因为在误差反向传播法中，0 初始化会使得所有的权重值都会进行相同的更新。比如，在 2 层神经网络中，假设第 1 层和第 2 层的权重为 0。这样一来，正向传播时，因为输入层的权重为 0，所以第 2 层的神经元全部会被传递相同的值。第 2 层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新（回忆一下“乘法节点的反向传播”内容）。\n权重被更新为相同的值，并拥有了对称的值（重复的值）。这使得神经网络拥有许多不同的权重的意义丧失了。\n随机初始化（np.random.randn()）就是为了瓦解权重的对称结构。\n（1）Sigmoid 函数\n从图 6-10 可知，各层的激活值呈偏向 0 和 1 的分布。\nSigmoid 函数是 S 型函数，随着输出不断地􏶒近 0（或者􏶒近 1），它的导数的值逐渐接近 0。这个问题就是梯度消失􏴾（gradient vanishing）。\n图 6-11 呈集中在 0.5 附近的分布。\n因为不像刚才的例子那样偏向 0 和 1，所以不会发生梯度消失的问题。但是，激活值的分布有所偏向，说明在表现力上会有很大问题。\n使用 Xavier 初始值后的结果如图 6-13 所示。从这个结果可知，越是后面的层，图像变得越歪斜，但是呈现了比之前更有广度的分布。因为各层间传递的数据有适当的广度，所以 sigmoid 函数的表现力不受限制。\n图 6-14 为激活函数使用 ReLU 时激活值的分布。\n7. 卷积神经网络   全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。\n  如何编程实现卷积，前向传播和反向传播，是需要重点留意的地方。\n  8. 深度学习 “What is the class of this image ?”的网站上，以排行榜的形式刊登了目前为止通过论文等渠道发表的针对各种数据集的方法的识别精度。\n https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html   叠加小型滤波器来加深网络的好处是可以减少参数的数量，扩大感受野（receptive field，给神经元施加变化的某个局部空间区域）。并且，通过叠加层，将 ReLU 等激活函数夹在卷积层的中间，进一步提高了网络的表现力。  附录 附录给出了：\n Softmax-with-Loss 层的计算图反向传播详细推导   全书完。\n","permalink":"http://landodo.github.io/posts/20210128-deep-learning-from-scrach/","summary":"深度学习基础图书 今天又花了点时间看了一本书（第二遍）——《深度学习入门——基于 Python 的理论与实践》。 全书下载地址：https://github.","title":"深度学习基础图书"},{"content":"十大排序算法 #面经 #算法与数据结构\n 冒泡排序（Bubble Sort） 选择排序（Selection Sort） 插入排序（Insertion Sort） 希尔排序（Shell Sort） 快速排序（Quick Sort） 归并排序（Merge Sort） 堆排序（Heap Sort） 计数排序（Counting Sort） 基数排序（Radix Sort） 桶子排序（Bucket Sort）  冒泡排序 // 冒泡排序 void bubble_sort(int data[], int n) {  for (int i = 0; i \u0026lt; n; ++i)  {  for (int j = 1; j \u0026lt; n-i; ++j)  {  if (data[j] \u0026lt; data[j-1])  swap(data[j], data[j-1]);  }  } } 冒泡排序算法的优化，当数据已经成升序排放，则算法的时间复杂度为 O(n)。\n// 冒泡排序优化 void bubble_sort2(int data[], int n) {  bool is_sorted = false;  for (int i = 0; i \u0026lt; n \u0026amp;\u0026amp; !is_sorted; ++i)  {  is_sorted = true; // 如果遍历一遍后，没有元素交换，则说明已经有序  for (int j = 1; j \u0026lt; n-i; ++j)  {  if (data[j] \u0026lt; data[j-1])  {  swap(data[j], data[j-1]);  is_sorted = false;  }  }  } } 选择排序 // 选择排序 void selection_sort(int data[], int n) {  for (int i = 0; i \u0026lt; n; ++i)  {  int min_index = i;  for (int j = i+1; j \u0026lt; n; ++j)  {  if (data[j] \u0026lt; data[min_index])  min_index = j;  }  swap(data[i], data[min_index]);  } } 插入排序 插入排序算法有一个很明显的优势，当数据几乎有序时，算法的时间复杂度可以将为线性的。插入排序可作为其他排序算法的子过程，进行优化。\n// 插入排序 void insertion_sort(int data[], int n) {  for (int i = 0; i \u0026lt; n-1; ++i)  {  for (int j = i+1; j \u0026gt; 0 \u0026amp;\u0026amp; data[j] \u0026lt; data[j-1]; --j)  swap(data[j], data[j-1]);  } } 希尔排序 布长的选择：\nwhile (h \u0026lt; n/3) h = h*3 + 1; // 1 4 13 43 … // 希尔排序 void shell_sort(int data[], int n) {  int h = 1;  while (h \u0026lt; n/3)  h = h*3 + 1; // 1 4 13 43 ...   while (h \u0026gt;= 1)  {  for (int i = h; i \u0026lt; n; ++i)  {  for (int j = i; j \u0026gt;= h \u0026amp;\u0026amp; data[j] \u0026lt; data[j-h]; j -= h)  swap(data[j], data[j-h]);  }  h /= 3;  } } 快速排序 快速排序的核心在于切分（partition ）。\n单路快排 // 单路快排 int partition(int data[], int l, int r) {  int v = data[l];  int p = l, i = l + 1;  for (; i \u0026lt;= r; ++i)  {  if (data[i] \u0026gt; v)  continue;  else  {  swap(data[i], data[++p]);  }  }  swap(data[l], data[p]);  return p; } 双路快排 最常用的快速排序算法的 partition 是双路快排。\n// 双路切分 int partition_2ways(int data[], int l, int r) { int v = data[l]; int i = l+1, j = r; while (true) { while (i \u0026lt;= r \u0026amp;\u0026amp; data[i] \u0026lt;= v) i++; while (j \u0026gt;= l \u0026amp;\u0026amp; data[j] \u0026gt; v) j--; if (i \u0026gt; j) break; swap(data[i], data[j]); i++; j--; } swap(data[l], data[j]); return j; } 三路快排 拥有大量重复元素时，性能相比双路快排得到优化。不常用\nvoid quick_sort_3ways(int data[], int l, int r) {  if (l \u0026gt;= r)  return;  // 切分  int v = data[l]; // 基准值  int lt = l, i = l + 1, gt = r + 1;  while (i \u0026lt; gt)  {  if (data[i] \u0026lt; v)  {  swap(data[i], data[lt+1]);  i++;  lt++;  }  else if (data[i] \u0026gt; v)  {  swap(data[i], data[gt-1]);  gt--;  }  else // ==  i++;  }  swap(data[l], data[lt]);  // data[l ...lt-1] \u0026lt; v  // data[lt … gt-1] == v  // data[gt …r] \u0026gt; v  quick_sort_3ways(data, l, lt-1);  quick_sort_3ways(data, gt, r); }  void quick_sort_3ways(int data[], int n) {  quick_sort_3ways(data, 0, n-1); } 归并排序 void merge(int data[], int l, int mid, int r) {  int* aux = new int[r-l+1];  for (int i = l; i \u0026lt;= r; ++i)  aux[i-l] = data[i];  // merge  int i = l, j = mid+1;  for (int k = l; k \u0026lt;= r; ++k)  {  if (i \u0026gt; mid)  {  data[k] = aux[j-l];  j++;  }  else if (j \u0026gt; r)  {  data[k] = aux[i-l];  i++;  }  else if (aux[i-l] \u0026lt; aux[j-l])  {  data[k] = aux[i-l];  i++;  }  else  {  data[k] = aux[j-l];  j++;  }  }  delete [] aux;  aux = nullptr; }  void merge_sort(int data[], int l, int r) {  if (l \u0026gt;= r)  return;  int mid = l + (r - l) / 2;  merge_sort(data, l, mid);  merge_sort(data, mid+1, r);  merge(data, l, mid, r); }  // 归并排序 void merge_sort(int data[], int n) {  merge_sort(data, 0, n-1); } 堆排序 // 节点 k 进行下沉操作 // k 的左孩子:2*k+1 右孩子:2*k+2 void sink(int data[], int n, int k) {  while (2*k+1 \u0026lt; n)  {  int j = 2*k + 1; // 左孩子  if (j+1 \u0026lt; n \u0026amp;\u0026amp; data[j+1] \u0026gt; data[j]) //i 比较左右孩子  j++;  if (data[k] \u0026lt; data[j])  swap(data[k], data[j]);  k = j; // 继续下沉  } }  // 堆排序 void heap_sort(int data[], int n) {  // 从最后一个非叶子节点开始执行下沉操作  for (int k = (n-2)/2; k \u0026gt;= 0; --k)  sink(data, n, k);   for (int i = n-1; i \u0026gt; 0; --i)  {  swap(data[0], data[i]); // data[0] 为堆中的最大值  sink(data, i, 0); // 重新调整，使之满足最大堆  } } 计数排序 // 计数排序 // 0 \u0026lt;= data[i] \u0026lt; n void counting_sort(int data[], int n) { int max = data[0]; for (int i = 1; i \u0026lt; n; ++i) { if (data[i] \u0026gt; max) max = data[i]; } int* count = new int[max+1]; int* aux = new int[n]; for (int i = 0; i \u0026lt; max+1; ++i) count[i] = 0; for (int i = 0; i \u0026lt; n; ++i) count[data[i]]++; for (int i = 1; i \u0026lt;= max; ++i) count[i] += count[i-1]; for (int i = n - 1; i \u0026gt;= 0; —i) { aux[count[data[i]] - 1] = data[i]; count[data[i]]--; } for (int i = 0; i \u0026lt; n; ++i) data[i] = aux[i]; delete [] aux; delete [] count; aux = nullptr; count = nullptr; } 基数排序 int get_max(int data[], int n) {  int max = data[0];  for (int i = 1; i \u0026lt; n; ++i)  {  if (data[i] \u0026gt; max)  max = data[i];  }  return max; }  // 计数排序只需要对 0-9 进行计数排序 // 参数 place 表示位(个/十/百…) void counting_sort_ten(int data[], int n, int place) {   int max = 10; // [0...9]  int* count = new int[max];  int* aux = new int[n];   for (int i = 0; i \u0026lt; max; ++i)  count[i] = 0;   for (int i = 0; i \u0026lt; n; ++i)  count[(data[i] / place) % 10]++;   for (int i = 1; i \u0026lt; max; i++)  count[i] += count[i - 1];   for (int i = n - 1; i \u0026gt;= 0; --i)  {  aux[count[(data[i] / place) % 10] - 1] = data[i];  count[(data[i] / place) % 10]--;  }   for (int i = 0; i \u0026lt; n; ++i)  data[i] = aux[i];   delete [] aux;  delete [] count;  aux = nullptr;  count = nullptr; }  // 基数排序 void radix_sort(int data[], int n) {  int max = get_max(data, n);   for (int place = 1; max / place \u0026gt; 0; place *= 10)  counting_sort_ten(data, n, place); } 桶子排序 // 桶子排序 [0…100] void bucket_sort(int data[], int n) { // vec[0] : [0…9] // vec[1] : [10…19] // vec[2] : [20…29] ….. vector\u0026lt;vector\u0026lt;int\u0026gt; \u0026gt; vec(10); for (int i = 0; i \u0026lt; n; ++i) vec[(data[i]/10)].push_back(data[i]); int k = 0; for (int i = 0; i \u0026lt; 10; ++i) { for (int j = 0; j \u0026lt; vec[i].size(); ++j) { data[k] = vec[i][j]; k++; } } // 已经几乎有序，调用插入排序算法 insertion_sort(data, n); } 排序算法的测试 在我的电脑上，各种算法一秒内可以完成排序的整型数个数。\n","permalink":"http://landodo.github.io/posts/20191231-10-sort-algo/","summary":"十大排序算法 #面经 #算法与数据结构 冒泡排序（Bubble Sort） 选择排序（Selection Sort） 插入排序（Insertion Sort","title":"十大排序算法"},{"content":"计数排序（Counting Sort） #面经 #算法与数据结构 计数排序是一种通过对每个数组中的每个元素进行相应的计数统计，通过计数值确定元素的正确位置的排序算法。计数排序需要知道待排序数据的取值范围，以方便申请辅助空间，这是计数排序的一个缺点。\n计数的规则：如果数组中共有 i 个元素小于等于 x，那么 x 的排序后的位置为 i。\n如下图，有（2 2 3 3 1）5 个元素小于 4，则排序完成后 4 的最终索引位置为 5。\n算法步骤  以下算法待排序数据大于等于 0。\n   获得待排序数据的最大值   申请辅助空间 count 数组，大小为 (max+1)，并且初始化为 0。   统计每个元素的频次：\n  如 2 出现了 2 次，则 count[2] = 2；4 只出现了 1 次，count[4] = 1。依次类推  累加计数——计数排序算法的核心，依次累加后 count 数组如下：  在 count[] 数组中寻找每个元素的索引  每确定一个元素的最终位置后，将 count[] 减 1。 这是因为出现元素的值相同的情况时，每当将一个 data[i] 放入 sorted[] 数组中时，都应使 count[data[i]] 的值减 1。这样，当下一个值等于 data[i] 的待排序元素出现的时候，在排序过的 sorted 数组中，这个元素将会被直接放到 sorted[i] 的前一个位置上。从后向前的，计数排序的是稳定的。如果从前向后排序，重复元素的顺序，刚好相反，所以就不是稳定的算法，但如果不关注稳定性，那么结果都是正确的。  完整算法 // Counting sort in C++ programming #include \u0026lt;iostream\u0026gt;#include \u0026lt;cstdlib\u0026gt; using namespace std; void counting_sort(int data[], int size) {  int* sorted = new int[size];  int max = data[0];  for (int i = 1; i \u0026lt; size; i++) {  if (data[i] \u0026gt; max)  max = data[i];  }  int* count = new int[max+1]; // 初值为 0   // 记录频次  for (int i = 0; i \u0026lt; size; i++)  count[data[i]]++;   // 累加计数  for (int i = 1; i \u0026lt;= max; i++)  count[i] += count[i - 1];   // 确定最终位置  for (int i = size - 1; i \u0026gt;= 0; i--) {  sorted[count[data[i]] - 1] = data[i];  count[data[i]]--;  }   for (int i = 0; i \u0026lt; size; i++)  data[i] = sorted[i];   delete [] sorted;  delete [] count; }  void print_data(int data[], int size) {  for (int i = 0; i \u0026lt; size; i++)  cout \u0026lt;\u0026lt; data[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;;  cout \u0026lt;\u0026lt; endl; }  // NOTES: delete[] data int* generate_data(int size) {  srand(time(nullptr));  int* data = new int[size];  for (int i = 0; i \u0026lt; size; ++i)  data[i] = rand() % 100;  return data; }  int main() {  int size = 20;  int* data = generate_data(size);  print_data(data, size);  counting_sort(data, size);  print_data(data, size);   delete [] data;  return 0; } 参考资料 https://www.programiz.com/dsa/counting-sort\n","permalink":"http://landodo.github.io/posts/20191230-counting-sort/","summary":"计数排序（Counting Sort） #面经 #算法与数据结构 计数排序是一种通过对每个数组中的每个元素进行相应的计数统计，通过计数值确定元素的正","title":"计数排序（Counting Sort）"},{"content":"这是我学习 STL 的笔记。\n更新日志\n 2019-03-04  新增了[STL 概述与版本简介](#STL 概述与版本简介)、空间分配器（allocators）、迭代器（iterators）、序列式容器-vector\n 2019-03-05  增加了list\n 2019-03-06  增加了 deque、heap、priority_queue、slist\n STL源码剖析 SGI STL 版本源码的可读性极佳，运用也最为广泛，是 GNU C++ 的标准链接库，并且开放自由。\n日常编程广泛运用的各种数据结构（data structures）和算法（algorithms）在 STL 中有良好的实现，连内存配置与管理也都重重考虑了最佳效能。\nSTL 源码中有着 vector、list、heap、deque、RB-tree、hash-table、set/map，以及各种算法（排序、搜索、排列组合……）的底层实现。\n从技术研究与本质提升的角度看，探究 STL 的细节可以帮助彻底的掌握一切，获得深厚扎实的基础。\nSGI STL 称得上是一个一流作品，追踪一流作品并且吸取养分，远比自己关起门来写个三流作品，价值高得多。\nSTL概述与版本简介 为了建立数据结构和算法的一套标准，并且降低其间的耦合（coupling）关系以提升各自的独立性、弹性、交互操作性（相互合作性，interoperability），STL 诞生了。\nSTL 带来了一个高层次的、以泛型思维（Generic Paradigm）为基础的、系统化的、条理分明的「软件组件分类学（components taxonomy）」。\nSTL的历史 STL 的创始人：Alexander Stepanov。\nAlexander Stepanov 分别􏰄实验了多种架构和算法公式，先以 C 完成，而后再以 C++ 完成。1993 年 11 月完成正式提案，STL 成为 C++ 标准规格的一部分。\nSTL与C++标准链接库 Alexander 向 C++ 标准委员提交提案后，STL 在会议中取得了压倒性的胜利。STL 进入了 C++ 标准化的正式流程，C++ 链接库如 stream, string 等也都以template 重新写过 。\nSTL六大组件  容器（containers）  各种数据结构，如 vector, list, deque, set, map。可以理解为包含对象的类。\n算法（algorithms）  各种常用算法如 sort, search, copy, erase。适用于不同类型容器的函数。\n迭代器（iterators）  容器中的“指针”。\n仿函式（functors）  STL 包括重载函数调用操作符的类。类的实例称为函数对象或仿函数。函数允许在传递参数的帮助下自定义相关函数的工作。\n适配器（adapters）  修饰容器（containers）或仿函式（functors）或迭代器（iterators）。\n分配器（allocators）  负责空间分配与管理，即用于分配空间的对象。\nC++规范 使用无拓展名的头文件。\n#include \u0026lt;vector\u0026gt;GNU源码开放精神 STL 源码属于 HP 公司拥有，每一个头文件重都有声明。开放源码的精神，一般统称为 open source。\nStallman 于 1984 创立自由软件基金会（Free Software Foundation），简称FSF。\nHP版本 每一个 HP STL 头文件都有如下一份声明。\n * Copyright (c) 1994\r* Hewlett-Packard Company SGI-STL版本 SGI 版􏰁由 Silicon Graphics Computer Systems, Inc. 公司发展，承继 HP 版􏰁。它的每一个头文件也都有 HP 的版􏰁本声明。\n * Copyright (c) 1996\r* Silicon Graphics Computer Systems, Inc. 在我的设备上，STL 源码位于以下目录：\n/usr/include/c++/4.2.1 每一份 STL 源码都包含三个版权声明，如下：\n// Copyright (C) 2001, 2002, 2003 Free Software Foundation, Inc.   * Copyright (c) 1994  * Hewlett-Packard Company   * Copyright (c) 1996  * Silicon Graphics Computer Systems, Inc.   /** @file include/vector * This is a Standard C++ Library header. */ 空间分配器（allocators） 整个 STL 的操作对象（所有的数值）都存放在容器之内，而容器一定需要分配空间以置放数据。\narray_allocator.h\rbitmap_allocator.h\rdebug_allocator.h\t# 可以包装任意其它的allocato\rmalloc_allocator.h\t# 包装malloc和free\rmt_allocator.h\rnew_allocator.h # 默认使用的allocator\rpool_allocator.h\t# 唯一一个带内存池的allocator\rthrow_allocator.h 分配器的标准接口 打开 new_allocator.h，可以看到 allocator 所提供的接口。\n设计分配器 src/my_allocator.h  src/test_my_allocator.cc 二级分配（sub-allocation）的SGI空间分配器 vector\u0026lt;int, std::allocator\u0026lt;int\u0026gt; \u0026gt; iv; SGI标准的空间分配器std::allocator SGI 定义有一个符合部分标准、名为 allocator 的分配器，但是效率不高，只把 C++ 的 ::operator new 和 ::operator delete 做一层封装而已。\nSGI特殊的空间分配器std::alloc STL allocator 将内存申请由 alloc:allocate() 负责，内存释放由 alloc::deallocate() 负责，对象构造由::construct() 负责，对象销毁由 ::destroy() 负责。\n配置器定义于 \u0026lt;memory\u0026gt; 文件中， \u0026lt;memory\u0026gt; 中包含：\n#include \u0026lt;bits/allocator.h\u0026gt;\t// 负责内存空间的配置与释放#include \u0026lt;bits/stl_construct.h\u0026gt;\t// 负责对象的构建与销毁对象的构造和销毁：construct()和destroy() stl_construct.h 中包含以下函数：\n_Construct() 接受一个指针 p 和一个初值 value，此函式的用途就是将初值设定到指针所指的空间上。\n// 销毁将指针所指东西 _Destroy(_Tp* __pointer)  // 销毁[first, last]之间的东西 _Destroy(_ForwardIterator __first, _ForwardIterator __last) 空间的申请与释放std::alloc 空间的申请、释放由 \u0026lt;malloc_allocator.h\u0026gt; 负责。\nSGI 设计了双层级分配器，第一级分配器直接使用 malloc() 和 free()，第二级分配器则视情况采用不同的策略：当分配空间超过 128bytes，便呼叫第一级分配器；当分配空间小于 128bytes，为了降低额外负担，便采用复杂的 memory pool 整理方式，而不再求助于第一级分配器。\n第一级分配器 第一级分配器直接使用 malloc()，SGI 以 malloc 而非 ::operator new 来分配内存。\n第二级分配器 第二级分配器多了一些机制，避免􏰃多小额区块造成内存的碎片。当区块小于 128 bytes，则以 memory pool 管理，此法又称为二级分配（sub-allocation）：每次配置一大块内存，并维护对应之自由表（free-list）。下次若再有相同大小的内存需求，就直接从 free-lists 中分出。\n// pool_allocator.h union _Obj {  union _Obj* _M_free_list_link;  char _M_client_data[1]; // The client sees this. }; 控件分配函数allocate() 此函数首先判断区块大小，大于 128 bytes 就调用第一级分配器，小于 128 bytes 就检查对应的 free list。如果 free list 中有可用的区块，就直接拿来用，如果没有可用区块，就将区块大小上调至 8 倍数边界，然后呼叫refill()，准备为 free list 重新填充空间。\n控件释放函数deallocate() 此函数首先判断区块大小，大于 128 bytes 就调用第一级分配器，小于 128 bytes 就找出对应的 free list，将区块回收。\n重新填充free list free list 中没有可用区块了，就呼叫 refill() 为 free list 重新填充空间。\n内存池（memory pool） 判断内存池的剩余容量，然后将空间分配给 free list。\n// 判断内存池的剩余容量 end_free - start_free 内存基本处理工具 STL 定义有五个函数，作用于􏰅初始化空间上。\n前两个是 construct() 和 destroy()，另外三个函数位于 \u0026lt;stl_uninitialized.h\u0026gt; 中。\nuninitialized_copy()\n将内存的分配与对象的构造行为分离开来。\n它接收 3 个参数：\n 迭代器 first 指向输入端的起始位置 迭代器 last 指向输入端的结束位置 迭代器 result 指向输出端（欲初始化空间）的起始处  uninitialized_fill()\n将内存的分配与对象的构造行为分离开来，它要不就产生出所有必要元素，要不就不产生任何元素。\n它接收 3 个参数：\n 迭代器 first 指向输出端（欲初始化空间）的起始处 迭代器 last 指向输出端（欲初始化空间）的结束处 x 表示初值  uninitialized_fill_n()\n将内存的分配与对象的构造行为分离开来，它会为指定范围内的所有元素设定相同的初值。\n它接收 3 个参数：\n first：指向欲初始化空间的起始处 n：表示欲初始化空间的大小 x：初值  迭代器（iterators） iterator：提供一种方法，是的得可以按顺序访问某个聚合物（容器）所􏰈的各个元素，而又不会暴露该聚合物（容器）的内部实现。\n迭代器的设计思维——STL的关键 STL 的中心思想在于，将数据容器（containers）和算法（algorithms）分开，彼此独立设计，最后再以一种粘合剂将它们联系在一起。迭代器（iterators）就是联系容器（containers）和算法（algorithms）的粘合剂。\n迭代器是一种smart-pointer 迭代器是一种行为类似指针的对象，而指针的各种行为中最常见也最重要的便是内容提领（dereference）和成员取用（member access）。迭代器最重要的编程工作就是对 operator* 和 operator-\u0026gt; 进行重载（overloading）工程。\nauto_ptr 的源码在头文件 \u0026lt;memory\u0026gt; 中。\n// memory template\u0026lt;typename _Tp\u0026gt; class auto_ptr { private:  _Tp* _M_ptr;  public: //.... } 每一种 STL 容器都提供有专属迭代器，目的就是为了封装细节不被使用者所看到。\n迭代器相应类型 算法之中运用迭代器时，很可能会用到其相应类型􏰄（associated type）。\n可以使用利用 function template 的自变量推导（argument deducation）机制。\nTraits编程技巧 src/my_iter.cc func() 的回返型􏰄必须加上关键词 typename，关键词 typename 的用意在告诉编译器说这是一个类型􏰄。\nPartial Specialization（偏特化）的意义 template\u0026lt;typename T\u0026gt; class C\u0026lt;T*\u0026gt; { ... }; template \u0026lt;typename T\u0026gt; struct iterator_traits\u0026lt;const T*\u0026gt; {  // 当迭代器是个pointer-to-const  // 萃取出来的类型应该是 T 而非 const T  typedef value_type; };  template \u0026lt;typename I\u0026gt; typename iterator_traits\u0026lt;I\u0026gt;::value_type func(I ite) {  return *ite; } 根据经验，最常用到的迭代器相应型􏰄有五种：value type, difference type, pointer, reference,iterator catagoly。\n// stl_iterator.h typename iterator_traits\u0026lt;_Iterator\u0026gt;::iterator_category, typename iterator_traits\u0026lt;_Iterator\u0026gt;::value_type, typename iterator_traits\u0026lt;_Iterator\u0026gt;::difference_type, typename iterator_traits\u0026lt;_Iterator\u0026gt;::pointer, typename iterator_traits\u0026lt;_Iterator\u0026gt;::reference value_type\n指迭代器所指对象的类型。\ndifference_type\n表示两个迭代器之间的距离。因此，它可以用来表示一个容器的最大容量，因为对于连续空间的容器而言，头尾之间的距离就是其最大容量。\nreference\nC++ 的函数如果要传回左值，都是以 by reference 的方式进行。\npointer\n能够传回一个 pointer，指向迭代器所指之物。\nItem\u0026amp; operator*() const { return *ptr; } Item* operator-\u0026gt;() const { return ptr; } Item\u0026amp; 便是 reference type ， Item* 是 pointer type。\niterator_category\n迭代器的分类：\n Input Iterator：这种迭代器所指对象，不允许外界改变 Output Iterator：只能写（write only） Forward Iterator：读写动作 Bidirectional Iterator：可双向移动 Random Access Iterator：涵盖所有算术能力  iterator 源码 iterator\rstl_iterator.h\rstl_iterator_base_funcs.h\rstl_iterator_base_types.h\rstl_raw_storage_iter.h\rstream_iterator.h\rstreambuf_iterator.h 序列式容器（sequence containers） 容器是大多数人对 STL 的第一印象。\n序列式容器，其中的元素都可序（ordered），但􏰅未排序（sorted）。C++ 语言本􏰁身提供了一个序列式容器 array，STL 另外再提供 vector, list, deque, stack, queue, priority-queue 等等序列式容器。其中 stack 和 queue 由是将 deque 改头换面而成，技术上被归类为一种适配器（adapter）。\nvector vector 的数据安排以及操作方式，与 array 非常像似。它们之间的差别在于：\n  array 是静态空间，扩容和缩容操作首先配置一块新空间，然后将元素从旧空间一一搬往新空间，然后再把原来的空间释还给系统。\n  vector 是动态空间，随着元素的加入，它的内部机制会自行扩充空间以容纳新元素。\n  // stl_vector.h vector的迭代器 typedef __gnu_cxx::__normal_iterator\u0026lt;pointer, vector_type\u0026gt; iterator; typedef __gnu_cxx::__normal_iterator\u0026lt;const_pointer, vector_type\u0026gt; const_iterator; typedef std::reverse_iterator\u0026lt;const_iterator\u0026gt; const_reverse_iterator; typedef std::reverse_iterator\u0026lt;iterator\u0026gt;\treverse_iterator; vector 支持随机存取，提供的是 Random Access Iterators。\nvector的数据结构 vector 所采用的数据结构非常简单：线性连续空间。\nstruct _Vector_impl : public _Tp_alloc_type { \t_Tp* _M_start;\t// 表示目前使用空间的头 \t_Tp* _M_finish;\t// 表示目前使用空间的尾 \t_Tp* _M_end_of_storage;\t// 表示目前可用空间的尾 }; vector 实际配置的大小可能比客端需求量更大一些，以备将来可能的扩充。\nsize 表示元素的个数，capacity 表示 vector 的容量。当增加新元素时，size 增加，当 size 超过容量（capacity）的时候，vector 的容量会增加两倍。如果两倍容量仍不足，就扩张至足够大的容量。\n例如下面的例子：\n#include \u0026lt;vector\u0026gt;#include \u0026lt;cstdio\u0026gt; using std::vector;  int main() {  vector\u0026lt;int\u0026gt; vec;  for (int i = 0; i \u0026lt; 10; ++i) {  vec.push_back(i);  printf(\u0026#34;capacity=%d, size=%d\\n\u0026#34;, vec.capacity(), vec.size());  }  return 0; } // 运行结果 capacity=1, size=1 capacity=2, size=2 capacity=4, size=3 capacity=4, size=4 capacity=8, size=5 capacity=8, size=6 capacity=8, size=7 capacity=8, size=8 capacity=16, size=9 capacity=16, size=10 vector的构造与内存管理constructor,push_back vector 提供许多 constructors，其中一个允许我们指定空间大小及初值:\nvector(size_type n, const T\u0026amp; value) {  fill_initialize(n, value); } 当我们以 push_back() 将新元素安插于 vector 尾端，该函式首先检查是否还有备用空间？如果有就直接在备用空间上建构元素，并调整迭代器 finish，使 vector 变大。如果没有备用空间了，就扩充空间（重新分配、搬移数据、释放原空间）。\n// stl_bvector.h void _M_insert_aux(iterator __position, bool __x) { \t// 检查是否还有备用空间  if (this-\u0026gt;_M_impl._M_finish._M_p != this-\u0026gt;_M_impl._M_end_of_storage) \t{  // 构造一个新元素 \tstd::copy_backward(__position, this-\u0026gt;_M_impl._M_finish, \tthis-\u0026gt;_M_impl._M_finish + 1); \t*__position = __x;  // 调整 finish \t++this-\u0026gt;_M_impl._M_finish; \t} \telse \t{  // 如果原大小不为 0，则配置原大小的两倍 \tconst size_type __len = size() ? 2 * size() \t: static_cast\u0026lt;size_type\u0026gt;(_S_word_bit); \t_Bit_type * __q = this-\u0026gt;_M_allocate(__len); \titerator __i = _M_copy_aligned(begin(), __position, \titerator(__q, 0)); \t*__i++ = __x; \tthis-\u0026gt;_M_impl._M_finish = std::copy(__position, end(), __i); \tthis-\u0026gt;_M_deallocate(); \tthis-\u0026gt;_M_impl._M_end_of_storage = (__q + ((__len \t+ int(_S_word_bit) - 1) \t/ int(_S_word_bit))); \tthis-\u0026gt;_M_impl._M_start = iterator(__q, 0); \t} } 所谓动态增加大小，并不是在原空间之后接续新空间（因为无法保证原空间之后尚有可供插入的空间），而是以原大小的两倍另外分配一块空间，然后将原内容拷贝过来，然后才开始在原内容之后插入新元素，并释放原空间。\nvector的元素操作pop_back,erase,clear,insert pop_back\n// stl_bvector.h  // 将尾部元素拿掉，并调整大小(finish减小) void pop_back() { \t--this-\u0026gt;_M_impl._M_finish; }   size_type size() const {  return size_type(end() - begin()); }  iterator end() {  return this-\u0026gt;_M_impl._M_finish; } erase\n// stl_bvector.h  // 清除[first, finish]中的所有元素 // 使用后面的元素覆盖要删除的区间，然后修改 finish 指针 iterator erase(iterator __first, iterator __last) {  _M_erase_at_end(std::copy(__last, end(), __first));  return __first; }  void _M_erase_at_end(iterator __pos) {  this-\u0026gt;_M_impl._M_finish = __pos; }  // 清除某个位置上的元素 iterator erase(iterator __position) {  if (__position + 1 != end())  std::copy(__position + 1, end(), __position);  --this-\u0026gt;_M_impl._M_finish;  return __position; } clear\n// stl_bvector.h  // 清除所有元素 void clear() { \t_M_erase_at_end(begin()); }  void _M_erase_at_end(iterator __pos) {  this-\u0026gt;_M_impl._M_finish = __pos; } insert\nvoid _M_insert_aux(iterator __position, bool __x) {  if (this-\u0026gt;_M_impl._M_finish._M_p != this-\u0026gt;_M_impl._M_end_of_storage) \t{  // 元素后移 \tstd::copy_backward(__position, this-\u0026gt;_M_impl._M_finish, \tthis-\u0026gt;_M_impl._M_finish + 1); \t// /从安插点开始填入新值  *__position = __x;  // 修改 finish 指针 \t++this-\u0026gt;_M_impl._M_finish; \t} \telse \t{ \t// 申请 2 倍的空间 \t// ... \t} } list list 每次插入或删除一个元素，就立即分配或释放一个元素空间。list 对于空间的运用有绝对的精准，一点也不浪费。对于任何位置的元素插入或元素删除，list 永远是常数时间。可以这么理解：vector 对应数组，list 对应链表。\nlist的节点Node // stl_list.h struct _List_node_base {  _List_node_base* _M_next; ///\u0026lt; Self-explanatory  _List_node_base* _M_prev; ///\u0026lt; Self-explanatory };  /// @if maint An actual node in the %list. @endif template\u0026lt;typename _Tp\u0026gt; struct _List_node : public _List_node_base {  _Tp _M_data; ///\u0026lt; User\u0026#39;s data. }; list的迭代器 STL 的 list 是一个双向链表（double linked-list），迭代器必须具备前移、后移的能力。所以 list 提供的是 Bidirectional Iterators。\nlist 的一个重要性质：插入（insert）、连接（splice）不会造成原有的 list 迭代器失效。这在 vector 中是不成立的。\n// stl_list.h  template\u0026lt;typename _Tp\u0026gt; struct _List_iterator {  typedef _List_iterator\u0026lt;_Tp\u0026gt; _Self;  typedef _List_node\u0026lt;_Tp\u0026gt; _Node;   typedef ptrdiff_t difference_type;  typedef std::bidirectional_iterator_tag iterator_category;  typedef _Tp value_type;  typedef _Tp* pointer;  typedef _Tp\u0026amp; reference;   // 对于迭代器的取值，取的是节点的 data reference operator*() const {  return static_cast\u0026lt;_Node*\u0026gt;(_M_node)-\u0026gt;_M_data; } // 迭代器的成员存取 pointer operator-\u0026gt;() const {  return \u0026amp;static_cast\u0026lt;_Node*\u0026gt;(_M_node)-\u0026gt;_M_data; }   // 跌打器加1，就是前进一个节点 _Self\u0026amp; operator++() {  _M_node = _M_node-\u0026gt;_M_next;  return *this; }  // 迭代器减1，就是后退一个节点 _Self\u0026amp; operator--() {  _M_node = _M_node-\u0026gt;_M_prev;  return *this; }  };\t// struct _List_iterator list的数据结构 list 是一个双向的循环链表。\n_Node 可以转换为迭代器类型：\nprotected:  // Note that pointers-to-_Node\u0026#39;s can be ctor-converted to  // iterator types.  typedef _List_node\u0026lt;_Tp\u0026gt;\t_Node; // 取头节点的数值 reference front() { \treturn *begin(); } list的构造与内存管理：constructor, push_bask,insert list 使用 Alloc 做为空间分配器，并另外定义了 _Node_alloc_type ，为的是更方便的以节点大小为分配单位。\n// 使用 Alloc 做为空间分配器 template\u0026lt;typename _Tp, typename _Alloc\u0026gt;  // 每次配置一个节点大小 typedef typename _Alloc::template rebind\u0026lt;_List_node\u0026lt;_Tp\u0026gt; \u0026gt;::other  _Node_alloc_type; // 分配一个节点并返回 _List_node\u0026lt;_Tp\u0026gt;* _M_get_node() { return _M_impl._Node_alloc_type::allocate(1); }  // 释放一个节点 void _M_put_node(_List_node\u0026lt;_Tp\u0026gt;* __p) { _M_impl._Node_alloc_type::deallocate(__p, 1); }  // 分配一个节点，并赋初值 _Node* _M_create_node(const value_type\u0026amp; __x) { \t_Node* __p = this-\u0026gt;_M_get_node(); \ttry \t{  _M_get_Tp_allocator().construct(\u0026amp;__p-\u0026gt;_M_data, __x); \t} \tcatch(...)  {  _M_put_node(__p);  __throw_exception_again;  } \treturn __p; }   // 删除一个节点 // Erases element at position given. void _M_erase(iterator __position) {  __position._M_node-\u0026gt;unhook();  _Node* __n = static_cast\u0026lt;_Node*\u0026gt;(__position._M_node);  _M_get_Tp_allocator().destroy(\u0026amp;__n-\u0026gt;_M_data);  _M_put_node(__n); } // new_allocator.h void destroy(pointer __p) { __p-\u0026gt;~_Tp(); } list 提供了很多 constructors：\nexplicit list(const allocator_type\u0026amp; __a = allocator_type())  : _Base(__a) { } push_back() 将新元素插入到 list 的尾部：\nvoid push_back(const value_type\u0026amp; __x) {  this-\u0026gt;_M_insert(end(), __x); }  // Inserts new element at position given and with value given. void _M_insert(iterator __position, const value_type\u0026amp; __x) {  _Node* __tmp = _M_create_node(__x);  __tmp-\u0026gt;hook(__position._M_node); } 在 list 内的某处插入新节点，首先必须确定安插位置。\nvoid insert(iterator __position, size_type __n, const value_type\u0026amp; __x) {  list __tmp(__n, __x, _M_get_Node_allocator());  splice(__position, __tmp); }  // example: 在ls链表头插入 ls.insert(ls.begin(), 6); list 不像 vector 那样有可能在空间不足时做重新分配、数据迁移的动作，所以插入前的所有迭代器在插入动作之后都仍然有效。\nlist的元素操作 // 插入一个节点，作为头节点 void push_front(const value_type\u0026amp; __x)  { this-\u0026gt;_M_insert(begin(), __x); }  // 插入一个节点，作为尾节点 void push_back(const value_type\u0026amp; __x)  { this-\u0026gt;_M_insert(end(), __x); }  // 删除迭代器 position 所指节点 iterator erase(iterator __position);  // 删除头节点 void pop_front()  { this-\u0026gt;_M_erase(begin()); }  // 删除尾节点 void pop_back() {  this-\u0026gt;_M_erase(iterator(this-\u0026gt;_M_impl._M_node._M_prev)); } list 内部提供一个所谓的迁移动作（transfer），将某连续范围的元素迁移到某个特定位置之前。\n// Moves the elements from [first,last) before position. void _M_transfer(iterator __position, iterator __first, iterator __last) { \t__position._M_node-\u0026gt;transfer(__first._M_node, __last._M_node); } list 公开提供的是所谓的接合动作（splice）：将某连续范围的元素从一个 list 搬移到另一个（或同一个）list 的某个定点。\n// 将x连接到position所指的位置之前 void splice(iterator __position, list\u0026amp; __x) { \tif (!__x.empty())  {  _M_check_equal_allocators(__x);  this-\u0026gt;_M_transfer(__position, __x.begin(), __x.end());  } }  // 将i所指元素连接到__position所指位置之前 void splice(iterator __position, list\u0026amp; __x, iterator __i) { \titerator __j = __i; \t++__j; \tif (__position == __i || __position == __j) \treturn;  \tif (this != \u0026amp;__x) \t_M_check_equal_allocators(__x);  \tthis-\u0026gt;_M_transfer(__position, __i, __j); }  // 将 [first,last) 内的所有元素连接接于 position所指位置之前。 void splice(iterator __position, list\u0026amp; __x, iterator __first, iterator __last) { \tif (__first != __last) \t{ \tif (this != \u0026amp;__x) \t_M_check_equal_allocators(__x);  \tthis-\u0026gt;_M_transfer(__position, __first, __last); \t} } 有了 transfer()，merge()、reverse() 和 sort() 的源码并不难实现。\ntemplate\u0026lt;typename _Tp, typename _Alloc\u0026gt; void list\u0026lt;_Tp, _Alloc\u0026gt;:: merge(list\u0026amp; __x) {  // _GLIBCXX_RESOLVE_LIB_DEFECTS  // 300. list::merge() specification incomplete  if (this != \u0026amp;__x) \t{ \t_M_check_equal_allocators(__x);  \titerator __first1 = begin(); \titerator __last1 = end(); \titerator __first2 = __x.begin(); \titerator __last2 = __x.end(); \twhile (__first1 != __last1 \u0026amp;\u0026amp; __first2 != __last2) \tif (*__first2 \u0026lt; *__first1) \t{  iterator __next = __first2;  _M_transfer(__first1, __first2, ++__next);  __first2 = __next; \t} \telse \t++__first1; \tif (__first2 != __last2) \t_M_transfer(__last1, __first2, __last2); \t} } deque vector 是单向开口的连续线性空间，也就是说，只能从一端进行插入。deque 则是一种双向开口的连续线性空间，可以在头尾两端分别􏰁做元素的插入和删除动作。\ndeque 在两端插入或删除，时间复杂度都是常数级的。不同于 vector，如果在头插入，需要将所有元素后移，时间复杂度是线性级别的。\n应尽可能选择使用 vector 而非 deque。对 deque 进行的排序动作，为了最高效率，可将 deque 先完整复制到一个 vector 身上，将 vector 排序后（利用 STL sort 算法），再复制回 deque。\ndeque的中控件 deque 由一段一段的定量连续空间构成。一旦有必要在 deque 的前端或尾端增加新空间，便配置一段定量连续空间，串接在整个 deque 的头端或尾端。\n为了维护整体连续的假象，数据结构的设计及迭代器前进后退等动作都颇为繁琐。deque 的代码量远比 vector 或 list 多得多。\ndeque 采用一块 map（一小块连续的控件），其中的每个元素指向另一段较大的连续线性空间，成为缓冲区。缓冲区是 deque 的存储空间主体。\nprotected: \t// 元素的指针的指针\t typedef pointer*\t_Map_pointer;   // 每个node都指向一块缓冲区 _Map_pointer _M_node; deque的迭代器 deque 是分段连续空间。\ntemplate\u0026lt;typename _Tp, typename _Ref, typename _Ptr\u0026gt;  struct _Deque_iterator\t// 未继承 std::iterator  {  typedef _Deque_iterator\u0026lt;_Tp, _Tp\u0026amp;, _Tp*\u0026gt; iterator;  typedef _Deque_iterator\u0026lt;_Tp, const _Tp\u0026amp;, const _Tp*\u0026gt; const_iterator;   static size_t _S_buffer_size()  { return __deque_buf_size(sizeof(_Tp)); }  \t// 自行撰写 5 个必要的跌打器类型  typedef std::random_access_iterator_tag iterator_category; //(1)  typedef _Tp value_type;\t// (2)  typedef _Ptr pointer;\t// (3)  typedef _Ref reference;\t// (4)  typedef size_t size_type;  typedef ptrdiff_t difference_type;\t// (5)  typedef _Tp** _Map_pointer;  typedef _Deque_iterator _Self;   _Tp* _M_cur;\t// 此迭代器所指缓冲区的当前（current）元素  _Tp* _M_first;\t// 指向缓冲区的头  _Tp* _M_last;\t// 指向缓冲区的尾  _Map_pointer _M_node;\t// 指向控制中心   // ...  }; // 决定缓冲区大小的函数 static size_t _S_buffer_size()  { return __deque_buf_size(sizeof(_Tp)); }  // 如果 sz(元素大小，sizeof(value_type))小于 512，传回 512/sz， // 如果 sz 不小于 512，传回 1。 inline size_t  __deque_buf_size(size_t __size)  { return __size \u0026lt; 512 ? size_t(512 / __size) : size_t(1); } begin() 和 end() 所传回的两个迭代器如下图：\n一旦行进时遇到缓冲区边缘，视前进或后退而定，可能需要调用 set_node() 切换缓冲区。\nvoid _M_set_node(_Map_pointer __new_node) { \t_M_node = __new_node; \t_M_first = *__new_node; \t_M_last = _M_first + difference_type(_S_buffer_size()); }  _Self\u0026amp; operator++() { \t++_M_cur; \tif (_M_cur == _M_last)  {  // 切换下一个节点（即下一个缓冲区） \t_M_set_node(_M_node + 1); \t_M_cur = _M_first;  } \treturn *this; } deque的数据结构 deque 维护 start, finish 两个迭代器，分􏰁指向第一缓冲区的第一个元素和最后缓冲区的最后一个元素（的下一位置）。\n struct _Deque_impl\r: public _Tp_alloc_type\r{\r_Tp** _M_map;\t// 指向map，一块连续的空间\rsize_t _M_map_size;\t// map内的节点数\riterator _M_start;\t// 第一个节点\riterator _M_finish;\t// 最后一个节点\r_Deque_impl(const _Tp_alloc_type\u0026amp; __a)\r: _Tp_alloc_type(__a), _M_map(0), _M_map_size(0),\r_M_start(), _M_finish()\r{ }\r}; 有了以上的结构，很多操作便可以轻易完成：\n// iterators /** * Returns a read/write iterator that points to the first element in the * %deque. Iteration is done in ordinary element order. */ iterator  begin() { return this-\u0026gt;_M_impl._M_start; }  /** * Returns a read/write iterator that points one past the last * element in the %deque. Iteration is done in ordinary * element order. */ iterator  end() { return this-\u0026gt;_M_impl._M_finish; } deque的构造与内存管理 deque自行定义了两个专属的空间配置器:\n// 每次分配一个元素大小 typedef _Deque_base\u0026lt;_Tp, _Alloc\u0026gt; _Base; // 每次分配一个指针大小 typedef typename _Base::_Tp_alloc_type\t_Tp_alloc_type; 并提供 constructor：\nexplicit deque(size_type __n, const value_type\u0026amp; __value = value_type(), const allocator_type\u0026amp; __a = allocator_type()) : _Base(__a, __n) { _M_fill_initialize(__value); } _M_fill_initialize()产生并初始化后 deque 的结构：\ntemplate \u0026lt;typename _Tp, typename _Alloc\u0026gt;  void  deque\u0026lt;_Tp, _Alloc\u0026gt;::  _M_fill_initialize(const value_type\u0026amp; __value)  {  _Map_pointer __cur;  try  {  // 为每个节点的缓冲区设定初值  for (__cur = this-\u0026gt;_M_impl._M_start._M_node; \t__cur \u0026lt; this-\u0026gt;_M_impl._M_finish._M_node; \t++__cur)  std::__uninitialized_fill_a(*__cur, *__cur + _S_buffer_size(), \t__value, _M_get_Tp_allocator());  // 最后一个节点设定不同，尾端可能有备用空间，不必设初值  std::__uninitialized_fill_a(this-\u0026gt;_M_impl._M_finish._M_first, \tthis-\u0026gt;_M_impl._M_finish._M_cur, \t__value, _M_get_Tp_allocator());  }  catch(...)  {  std::_Destroy(this-\u0026gt;_M_impl._M_start, iterator(*__cur, __cur), \t_M_get_Tp_allocator());  __throw_exception_again;  }  } push_back() 函数\nvoid push_back(const value_type\u0026amp; __x) { \tif (this-\u0026gt;_M_impl._M_finish._M_cur \t!= this-\u0026gt;_M_impl._M_finish._M_last - 1) \t{ \t// 最后缓冲区尚有一个以上的备用空间 \tthis-\u0026gt;_M_impl.construct(this-\u0026gt;_M_impl._M_finish._M_cur, __x);\t//直接在备用空间上建构元素 \t++this-\u0026gt;_M_impl._M_finish._M_cur;\t//调整最后缓冲区的使用状态 \t} \telse\t// 最后缓冲区已无(或只剩一个)元素备用空间。 \t_M_push_back_aux(__x); }   // 只有当 finish.cur == finish.last – 1时才会被调用。 // 即：当最后一个缓冲区只剩一个备用元素空间时才会被呼叫。 template \u0026lt;typename _Tp, typename _Alloc\u0026gt; void deque\u0026lt;_Tp, _Alloc\u0026gt;::  _M_push_back_aux(const value_type\u0026amp; __t) {  value_type __t_copy = __t;  _M_reserve_map_at_back();\t// // 如果 map尾端的节点备用空间不足,则必须重换一个map  *(this-\u0026gt;_M_impl._M_finish._M_node + 1) = this-\u0026gt;_M_allocate_node();\t//配置一个新节点(缓冲区)  try  {  this-\u0026gt;_M_impl.construct(this-\u0026gt;_M_impl._M_finish._M_cur, __t_copy);\t//针对标的元素设值  this-\u0026gt;_M_impl._M_finish._M_set_node(this-\u0026gt;_M_impl._M_finish._M_node  + 1);\t//改变 finish，令其指向新节点  this-\u0026gt;_M_impl._M_finish._M_cur = this-\u0026gt;_M_impl._M_finish._M_first;\t//设定finish的状态  }  catch(...)  {  _M_deallocate_node(*(this-\u0026gt;_M_impl._M_finish._M_node + 1));  __throw_exception_again;  } } push_front() 函数：\nvoid push_front(const value_type\u0026amp; __x) {  //第一缓冲区尚有备用空间 \tif (this-\u0026gt;_M_impl._M_start._M_cur != this-\u0026gt;_M_impl._M_start._M_first) \t{  // 直接在备用空间上建构元素 \tthis-\u0026gt;_M_impl.construct(this-\u0026gt;_M_impl._M_start._M_cur - 1, __x); \t//调整第一缓冲区的使用状态  --this-\u0026gt;_M_impl._M_start._M_cur; \t} \t// 第一缓冲区已无备用空间  else \t_M_push_front_aux(__x); } deque的元素操作pop_back,pop_front,clear,erase,insert 所谓 pop，是将元素拿掉。无论从 deque 的最前端或最尾端取元素，都需考虑在某种条件下，将缓冲区释放掉:\nvoid pop_front() {  // 第一缓冲区有一个(或更多)元素 \tif (this-\u0026gt;_M_impl._M_start._M_cur \t!= this-\u0026gt;_M_impl._M_start._M_last - 1) \t{  // 将第一元素销毁 \tthis-\u0026gt;_M_impl.destroy(this-\u0026gt;_M_impl._M_start._M_cur); \t// //调整指针，相当于排除了第一元素  ++this-\u0026gt;_M_impl._M_start._M_cur; \t}  // 第一缓冲区仅有一个元素，进行缓冲区的释放工作 \telse \t_M_pop_front_aux(); }  //只有当 start.cur == start.last - 1 时才会被呼叫。 template \u0026lt;typename _Tp, typename _Alloc\u0026gt; void deque\u0026lt;_Tp, _Alloc\u0026gt;::  _M_pop_front_aux() {  // 将第一缓冲区的第一个元素解构  this-\u0026gt;_M_impl.destroy(this-\u0026gt;_M_impl._M_start._M_cur);  // //释放第一缓冲区。  _M_deallocate_node(this-\u0026gt;_M_impl._M_start._M_first);  //调整 start的状态  this-\u0026gt;_M_impl._M_start._M_set_node(this-\u0026gt;_M_impl._M_start._M_node + 1);  // 下一个缓冲区的第一个元素。  this-\u0026gt;_M_impl._M_start._M_cur = this-\u0026gt;_M_impl._M_start._M_first; } clear()，用来清除整个 deque。请注意，deque 的最初状态（无任何元素时）保有一个缓冲区，因此 clear() 完成之后回复初始状态，也一样要 保留一个缓冲区:\nvoid clear() { _M_erase_at_end(begin()); }  // Called by erase(q1, q2), resize(), clear(), _M_assign_aux, // _M_fill_assign, operator=. void _M_erase_at_end(iterator __pos) { \t_M_destroy_data(__pos, end(), _M_get_Tp_allocator()); \t// +1 的目的是：保留头尾缓冲区  _M_destroy_nodes(__pos._M_node + 1, \tthis-\u0026gt;_M_impl._M_finish._M_node + 1); \tthis-\u0026gt;_M_impl._M_finish = __pos; } insert()\ntemplate \u0026lt;typename _Tp, typename _Alloc\u0026gt; typename deque\u0026lt;_Tp, _Alloc\u0026gt;::iterator deque\u0026lt;_Tp, _Alloc\u0026gt;:: insert(iterator __position, const value_type\u0026amp; __x) {  // 如果插入点是 deque最前端  if (__position._M_cur == this-\u0026gt;_M_impl._M_start._M_cur) \t{ \tpush_front(__x); \treturn this-\u0026gt;_M_impl._M_start; \t}  // 如果插入点是 deque最尾端  else if (__position._M_cur == this-\u0026gt;_M_impl._M_finish._M_cur) \t{ \tpush_back(__x); \titerator __tmp = this-\u0026gt;_M_impl._M_finish; \t--__tmp; \treturn __tmp; \t}  else  return _M_insert_aux(__position, __x); }    template \u0026lt;typename _Tp, typename _Alloc\u0026gt;  typename deque\u0026lt;_Tp, _Alloc\u0026gt;::iterator  deque\u0026lt;_Tp, _Alloc\u0026gt;::  _M_insert_aux(iterator __pos, const value_type\u0026amp; __x)  {  // 安插点之前的元素个数  difference_type __index = __pos - this-\u0026gt;_M_impl._M_start;  value_type __x_copy = __x; // XXX copy  // 如果安插点之前的元素个数比较少  if (static_cast\u0026lt;size_type\u0026gt;(__index) \u0026lt; size() / 2)  {  // 在最前端加入与第一元素同值的元素。  push_front(front());  // 以下标示记号，然后进行元素搬移...  iterator __front1 = this-\u0026gt;_M_impl._M_start;  ++__front1;  iterator __front2 = __front1;  ++__front2;  __pos = this-\u0026gt;_M_impl._M_start + __index;  iterator __pos1 = __pos;  ++__pos1;  // 元素搬移  std::copy(__front2, __pos1, __front1);  }  else\t//安插点之后的元素个数比较少  {  push_back(back());\t// 最尾端插入与最后元素同值的元素  iterator __back1 = this-\u0026gt;_M_impl._M_finish;  --__back1;  iterator __back2 = __back1;  --__back2;  __pos = this-\u0026gt;_M_impl._M_start + __index;  // 元素搬移  std::copy_backward(__pos, __back2, __back1);  }  // 在插入点设定新值  *__pos = __x_copy;  return __pos;  } stack stack 是一种先进后出（First In Last Out，FILO）的数据结构，它只有一个出口。stack 允许新增元素、移除元素、取得最顶端元素。但除了最顶端外，没有任何其它方法可以存取 stack 的其它元素。\nstack定义式完整列表 以某种既有容器做为底部结构，将其接口改变，使符合「先进后出」的特性，形 成一个 stack，是很容易做到的。deque 是双向开口的数据结构，若以 deque 为 底部结构并封闭其头端开口，便轻而易举地形成了一个 stack。\ntemplate\u0026lt;typename _Tp, typename _Sequence = deque\u0026lt;_Tp\u0026gt; \u0026gt;\rclass stack\r{\rprotected:\r// 底层容器\r_Sequence c;\r}; /** Returns true if the %stack is empty. */ bool empty() const { return c.empty(); }  /** Returns the number of elements in the %stack. */ size_type size() const { return c.size(); } stack没有迭代器 stack 所有元素的进出都必须符合「先进后出」的条件，只有 stack 顶端的元素，才有机会被外界取用。 不提供迭代器。\n以list为stack的底层容器 除了deque 之外，list 也是双向开口的数据结构。上述 stack 源码中使用的底层容器的函数有 empty, size, back, push_back, pop_back，list 都具备。\nqueue queue 是一种先进先出（First In First Out，FIFO）的数据结构，它有两个出口。\nqueue 只能从一端插入，另一端删除。除了最底端可以加入、最顶端可以取出，没有任何其它方法可以存取 queue 的其它元素。\nqueue定义式完整列表 SGI STL 便以 deque 做为预设情况下的 queue 底层结构。\ntemplate\u0026lt;typename _Tp, typename _Sequence = deque\u0026lt;_Tp\u0026gt; \u0026gt; class queue { protected: \t// 底层容器  _Sequence c; }; void push(const value_type\u0026amp; __x) { \tc.push_back(__x); }  void pop() {  __glibcxx_requires_nonempty();  c.pop_front(); } queue没有迭代器 queue 所有元素的进出都必须符合「先进先出」的条件，只有 queue 顶端的元素，才有机会被外界取用。\n以list做为queue的底层容器 除了deque 之外，list 也是双向开口的数据结构。上述 queue 源码中使用的底层容器的函数有 empty, size, back, push_back, pop_back，list 都具备。\nheap heap 并不归属于 STL 容器组件，它的背后是 priority queue（优先队列）。priority queue 允许使用者以任何次序将任何元素推入容器内，但取出时一定是从优先权最高的元素开始取。\n使用 list 做为 priority queue 的底层机制，元素插入动作可享常数时间。但是要找到 list 中的极值，却需要对整个 list 进行线性扫描。\n使用 binary search tree 做为 priority queue 的底层机制，元素的插入和极值的取得就有 O(logN) 的表现。但是这需要确保输入数据的随机性。\npriority queue 的复杂度，最好介于 queue 和 binary search tree之间，才算适得其所。binary heap 便是这种条件下的适当候选人。\nbinary heap 是一颗完全二叉树。当完全二叉树中的某个节点位于 array 的 i 处，其左子节点必位于 array 的 2i+1 处，其右子节点必位于 array 的 2i+2 处（这里的索引从 0 开始）。\n其父节点必定是 ⌊(i - 1)/2⌋。\n根据元素排列方式，heap 可分为 max-heap 和 min-heap 两种，max-heap 的最大值在根节点，min-heap 的最小值在根节点。\nheap算法 push_heap\n为了保持完全二叉树的性质，应该将新元素插入在底层 vector 的 end() 处。\ntemplate\u0026lt;typename _RandomAccessIterator\u0026gt; inline void push_heap(_RandomAccessIterator __first, _RandomAccessIterator __last) {  std::__push_heap(__first, _DistanceType((__last - __first) - 1),  _DistanceType(0), _ValueType(*(__last - 1))); }   template\u0026lt;typename _RandomAccessIterator, typename _Compare\u0026gt; inline void push_heap(_RandomAccessIterator __first, _RandomAccessIterator __last,  _Compare __comp) { \t// 值必置于底部 -\u0026gt; 容器的最尾端  std::__push_heap(__first, _DistanceType((__last - __first) - 1),  _DistanceType(0), _ValueType(*(__last - 1)), __comp); }  // 不允许指定，（大小比较标准） template\u0026lt;typename _RandomAccessIterator, typename _Distance, typename _Tp,  typename _Compare\u0026gt;  void  __push_heap(_RandomAccessIterator __first, _Distance __holeIndex, \t_Distance __topIndex, _Tp __value, _Compare __comp) {  _Distance __parent = (__holeIndex - 1) / 2;\t// 父节点  while (__holeIndex \u0026gt; __topIndex\t \u0026amp;\u0026amp; __comp(*(__first + __parent), __value))  {  // 当尚􏰃到达顶端，且父节点小于新值  *(__first + __holeIndex) = *(__first + __parent);  __holeIndex = __parent;  __parent = (__holeIndex - 1) / 2;  }  *(__first + __holeIndex) = __value; } pop_heap\npop 动作取走根节点，必须将最下一层最右边的叶节点拿来填补跟节点的位置，并维护堆的性质。\ntemplate\u0026lt;typename _RandomAccessIterator\u0026gt;  inline void  pop_heap(_RandomAccessIterator __first, _RandomAccessIterator __last)  {  std::__pop_heap(__first, __last - 1, __last - 1,  _ValueType(*(__last - 1)));  }  template\u0026lt;typename _RandomAccessIterator, typename _Compare\u0026gt;  inline void  pop_heap(_RandomAccessIterator __first,  _RandomAccessIterator __last, _Compare __comp)  {  std::__pop_heap(__first, __last - 1, __last - 1,  _ValueType(*(__last - 1)), __comp);  }  template\u0026lt;typename _RandomAccessIterator, typename _Tp\u0026gt;  inline void  __pop_heap(_RandomAccessIterator __first, _RandomAccessIterator __last,  _RandomAccessIterator __result, _Tp __value)  {  typedef typename iterator_traits\u0026lt;_RandomAccessIterator\u0026gt;::difference_type  _Distance;  // 设定尾值为首值，于是尾值即为欲求结果  // 可由客端稍后再以底层容器之 pop_back() 取出尾值。  *__result = *__first;  // 以上欲重新调整 heap  std::__adjust_heap(__first, _Distance(0), _Distance(__last - __first),  __value);  }   template\u0026lt;typename _RandomAccessIterator, typename _Distance,  typename _Tp, typename _Compare\u0026gt;  void  __adjust_heap(_RandomAccessIterator __first, _Distance __holeIndex,  _Distance __len, _Tp __value, _Compare __comp) {  const _Distance __topIndex = __holeIndex;  _Distance __secondChild = 2 * __holeIndex + 2; // 右节点  while (__secondChild \u0026lt; __len)  {  if (__comp(*(__first + __secondChild),  *(__first + (__secondChild - 1))))  __secondChild--; // 减1后为左节点   // secondChild代表较大子节点  *(__first + __holeIndex) = *(__first + __secondChild);  __holeIndex = __secondChild;  __secondChild = 2 * (__secondChild + 1);  }  // 如果没有右节点，只有左子节点  if (__secondChild == __len)  {  *(__first + __holeIndex) = *(__first + (__secondChild - 1));  __holeIndex = __secondChild - 1;  }  std::__push_heap(__first, __holeIndex, __topIndex, __value, __comp); } pop_heap 之后，最大元素只是被置放于底部容器的最尾端，尚􏰃被取走。如果要取其值，可使用底部容器（vector）所提供的 back() 操作函数。如果要移除它，可使用底部容器（vector）所提供的 pop_back() 操作函式。\nsort_heap 排序过后，原来的 heap 就不再是个合法的 heap 了。\n// 每执行一次 pop_heap()，极值(在 STL heap 中为极大值)即被放在尾端。 // 扣除尾端再执行一次 pop_heap()，次极值又被放在新尾端。一直下去，最后即得 // 排序结果。  template\u0026lt;typename _RandomAccessIterator\u0026gt;  void  sort_heap(_RandomAccessIterator __first, _RandomAccessIterator __last)  {  while (__last - __first \u0026gt; 1)  std::pop_heap(__first, _RandomAccessIterator(__last--));  } make_heap 这个算法用来将一段现有的数据转化为一个 heap。\ntemplate\u0026lt;typename _RandomAccessIterator\u0026gt;\rvoid\rmake_heap(_RandomAccessIterator __first, _RandomAccessIterator __last)\r{\r// 如果长度为 0或 1，不必重新排列。\rif (__last - __first \u0026lt; 2)\rreturn;\rconst _DistanceType __len = __last - __first;\r_DistanceType __parent = (__len - 2) / 2;\rwhile (true)\r{\rstd::__adjust_heap(__first, __parent, __len,\r_ValueType(*(__first + __parent)));\rif (__parent == 0)\rreturn;\r__parent--;\r}\r} heap也没有迭代器 priority_queue priority_queue 是一个拥有权值观念的 queue，它允许加入新元素、移除旧元素，审视元素值等功能。由于这是一个 queue，所以只允许在底端加入元素，并从顶端取出元素，除此之外􏰁无其它存取元素的途径。\ntemplate\u0026lt;typename _Tp, typename _Sequence = vector\u0026lt;_Tp\u0026gt;,\rtypename _Compare = less\u0026lt;typename _Sequence::value_type\u0026gt; \u0026gt;\rclass priority_queue\r{\rprotected:\r// vector为底层容器\r_Sequence c;\r_Compare comp; // 元素大小的比较标准\r// ....\r}; priority_queue也没有迭代器 slist STL list 是个双向链表（double linked list）。SGI STL 另提供了一个单向串行（single linked list），名为slist。 slist 和 list 的主要差􏰁在于，前者的迭代器属于单向的 Forward Iterator，后者的迭代器属于双向的 Bidirectional Iterator。 slist 和 list 共同具有的一个相同特色是，它们的插入（insert）、移除（erase）、接合（splice）等动作并不会造成原有的迭代器失效。\n基于效率考虑，slist 不提供 push_back()，只提供 push_front()。\nslist的节点 // 单向串行的节点基􏰀结构  struct _Slist_node_base  {  _Slist_node_base* _M_next;  };   // 单向串行的节点结构  template \u0026lt;class _Tp\u0026gt;  struct _Slist_node : public _Slist_node_base  {  _Tp _M_data;  };   //已知某一节点，安插新节点于其后。  inline _Slist_node_base*  __slist_make_link(_Slist_node_base* __prev_node,  _Slist_node_base* __new_node)  {  __new_node-\u0026gt;_M_next = __prev_node-\u0026gt;_M_next;  __prev_node-\u0026gt;_M_next = __new_node;  return __new_node;  }   // 反转一个链表  inline _Slist_node_base*  __slist_reverse(_Slist_node_base* __node)  {  _Slist_node_base* __result = __node;  __node = __node-\u0026gt;_M_next;  __result-\u0026gt;_M_next = 0;  while(__node)  {  _Slist_node_base* __next = __node-\u0026gt;_M_next;  __node-\u0026gt;_M_next = __result;  __result = __node;  __node = __next;  }  return __result;  } slist的迭代器 //单向串行的迭代器基􏰀结构  struct _Slist_iterator_base  {  typedef size_t size_type;  typedef ptrdiff_t difference_type;  typedef std::forward_iterator_tag iterator_category; // 单向   _Slist_node_base* _M_node; //指向节点基􏰀结构   void _M_incr() // // 前进一个节点  { _M_node = _M_node-\u0026gt;_M_next; }   // ....  };   //单向串行的迭代器结构  template \u0026lt;class _Tp, class _Ref, class _Ptr\u0026gt;  struct _Slist_iterator : public _Slist_iterator_base  {  typedef _Slist_iterator\u0026lt;_Tp, _Tp\u0026amp;, _Tp*\u0026gt; iterator;  typedef _Slist_iterator\u0026lt;_Tp, const _Tp\u0026amp;, const _Tp*\u0026gt; const_iterator;  typedef _Slist_iterator\u0026lt;_Tp, _Ref, _Ptr\u0026gt; _Self;   _Self\u0026amp;  operator++()  {  _M_incr(); // //前进一个节点  return *this;  }   _Self  operator++(int)  {  _Self __tmp = *this;  _M_incr(); // //前进一个节点  return __tmp;  }  }; slist的数据结构 template \u0026lt;class _Tp, class _Alloc = allocator\u0026lt;_Tp\u0026gt; \u0026gt;  class slist : private _Slist_base\u0026lt;_Tp,_Alloc\u0026gt;  {  // concept requirements  __glibcxx_class_requires(_Tp, _SGIAssignableConcept)   private:  typedef _Slist_base\u0026lt;_Tp,_Alloc\u0026gt; _Base;   public:  typedef _Tp value_type;  typedef value_type* pointer;  typedef const value_type* const_pointer;  typedef value_type\u0026amp; reference;  typedef const value_type\u0026amp; const_reference;  typedef size_t size_type;  typedef ptrdiff_t difference_type;   typedef _Slist_iterator\u0026lt;_Tp, _Tp\u0026amp;, _Tp*\u0026gt; iterator;  typedef _Slist_iterator\u0026lt;_Tp, const _Tp\u0026amp;, const _Tp*\u0026gt; const_iterator;   typedef typename _Base::allocator_type allocator_type;  private:  typedef _Slist_node\u0026lt;_Tp\u0026gt; _Node;  typedef _Slist_node_base _Node_base;  typedef _Slist_iterator_base _Iterator_base;   _Node*  _M_create_node(const value_type\u0026amp; __x)  {  // 配置空间  _Node* __node = this-\u0026gt;_M_get_node();  try  {  // 构造元素  get_allocator().construct(\u0026amp;__node-\u0026gt;_M_data, __x);  __node-\u0026gt;_M_next = 0;  }  catch(...)  {  this-\u0026gt;_M_put_node(__node);  __throw_exception_again;  }  return __node;  }  };   iterator  begin()  { return iterator((_Node*)this-\u0026gt;_M_head._M_next); }   iterator  end()  { return iterator(0); }   bool  empty() const  { return this-\u0026gt;_M_head._M_next == 0; }   void  swap(slist\u0026amp; __x)  { std::swap(this-\u0026gt;_M_head._M_next, __x._M_head._M_next); }   // 取头部元素  reference  front()  { return ((_Node*) this-\u0026gt;_M_head._M_next)-\u0026gt;_M_data; }   // 删除头部元素  void  pop_front()  {  _Node* __node = (_Node*) this-\u0026gt;_M_head._M_next;  this-\u0026gt;_M_head._M_next = __node-\u0026gt;_M_next;  get_allocator().destroy(\u0026amp;__node-\u0026gt;_M_data);  this-\u0026gt;_M_put_node(__node);  } }; ","permalink":"http://landodo.github.io/posts/20190304-stl-notes/","summary":"这是我学习 STL 的笔记。 更新日志 2019-03-04 新增了[STL 概述与版本简介](#STL 概述与版本简介)、空间分配器（allocators）、迭代器（iter","title":"《STL 源码剖析》STL 学习笔记"},{"content":"认识七段数码管 数码管的一种是半导体发光器件，数码管可分为七段数码管和八段数码管，区别在于八段数码管比七段数码管多一个用于显示小数点的发光二极管单元 DP（decimal point），其基本单元是发光二极管。\n按发光二极管单元连接方式可分为共阳极数码管和共阴极数码管。共阳数码管是指将所有发光二极管的阳极接到一起形成公共阳极(COM)的数码管，共阳数码管在应用时应将公共极 COM 接到 +5V，当某一字段发光二极管的阴极为低电平时，相应字段就点亮，当某一字段的阴极为高电平时，相应字段就不亮。共阴数码管是指将所有发光二极管的阴极接到一起形成公共阴极(COM)的数码管，共阴数码管在应用时应将公共极 COM 接到地线 GND 上，当某一字段发光二极管的阳极为高电平时，相应字段就点亮，当某一字段的阳极为低电平时，相应字段就不亮。\n数码管的显示 数码管中有位选和段选，位选就是选择哪个数码管，段选就是被选择的数码管要显示什么数字！以共阳极数码管为例, 要想点亮某段, 只需要在相应的段上给低电平即可。\n数码管真值表\n共阳：\n共阴：\n静态显示\n数码管静态显示是对应动态显示而言的，静态显示对于一两个数码管还行，多个数码管，静态显示实现的就失去了存在的意义。静态驱动是指每个数码管的每一个段码都由一个单片机的 I/O 端口进行驱动，静态驱动的优点是编程简单，显示亮度高，缺点是占用I/O端口多。\n静态实现十分的简单，不在本篇中做更多的笔记。\n动态显示\n数码管动态显示接口是单片机中应用最为广泛的一种显示方式之一，动态驱动是将所有数码管的8个显示笔划 \u0026ldquo;a,b,c,d,e,f,g,dp\u0026rdquo; 的同名端连在一起，另外为每个数码管的公共极 COM 增加位选通控制电路，位选通由各自独立的 I/O 线控制，当单片机输出字形码时，所有数码管都接收到相同的字形码，但究竟是哪个数码管会显示出字形，取决于单片机对位选通 COM 端电路的控制，所以我们只要将需要显示的数码管的选通控制打开，该位就显示出字形，没有选通的数码管就不会亮。通过分时轮流控制各个数码管的的 COM 端，就使各个数码管轮流受控显示，这就是动态驱动。在轮流显示过程中，每位数码管的点亮时间为 1～2ms，由于人的视觉暂留现象及发光二极管的余辉效应，尽管实际上各位数码管并非同时点亮，但只要扫描的速度足够快，给人的印象就是一组稳定的显示数据，不会有闪烁感，动态显示的效果和静态显示是一样的，能够节省大量的 I/O 端口，而且功耗更低。\n——————百度百科\n程序分解  数码管的动态需要对定时器有一定的了解。  0、定义变量\ntab[] ：共阳数码管编码\nstrtab[]：显示缓冲区\ntime：倒计时长\nt：辅助定时器A，产生 1 S 计数\n // 0 1 2 3 4 5 6 7 8 9 - off unsigned char tab[] = {0xc0,0xf9,0xa4,0xb0,0x99,0x92,0x82,0xf8,0x80,0x90,0xbf,0xff}; unsigned char strtab[4]; // display buffer unsigned int time = 9999; // time for 9999 unsigned char t = 0; // The auxiliary timer produces one second 1、初始化\n 关闭看门狗定时器  WDTCTL = WDTPW + WDTHOLD; // Stop WDT  I／O  设置 MSP320G2553 单片机的 P1.0 ~ P1.7 和 P2.0 ~ P2.3 为输出口，用于控制数码管的段选和位选。\n P1DIR |= 0xff; // P1.0 - P1.7 output  P2DIR |= 0x0f; // P2.0 - P2.3 for COMx  定时器  中断使能、初值 5ms。\n CCTL0 = CCIE; // CCR0 interrupt enabled  CCR0 = 5000; // Set the initial value for CCR0  TACTL = TASSEL_2 + MC_1; // SMCLK, upmode 2、中断服务函数\n动态扫描数码管。\n// Timer A0 interrupt service routine #pragma vector=TIMER0_A0_VECTOR __interrupt void Timer_A (void) {  static unsigned char num = 0;  // Scanning  switch (num)  {  case 0: P2OUT = BIT0; P1OUT = strtab[num]; break;  case 1: P2OUT = BIT1; P1OUT = strtab[num]; break;  case 2: P2OUT = BIT2; P1OUT = strtab[num]; break;  case 3: P2OUT = BIT3; P1OUT = strtab[num]; break;  default: break;  }  num++;  if (num == 4)  num = 0; } 3、倒计时、显示字符\n程序段处于中断服务函数中。\n// in Timer A0 interrupt service routine t++; if (t == 200) // 5ms * 200 = 1S (one minute time--) { if (time \u0026gt; 0)  time--; else  time = 9999; }  // Displaying strtab[0] = tab[(time / 1000) % 10]; strtab[1] = tab[(time / 100) % 10]; strtab[2] = tab[(time / 10) % 10]; strtab[3] = tab[time % 10]; 完整源程序 #include \u0026lt;msp430g2553.h\u0026gt; // 0 1 2 3 4 5 6 7 8 9 - off unsigned char tab[] = {0xc0,0xf9,0xa4,0xb0,0x99,0x92,0x82,0xf8,0x80,0x90,0xbf,0xff}; unsigned char strtab[4]; // display buffer unsigned int time = 9999; // time for 9999 unsigned char t = 0; // The auxiliary timer produces one second  int main(void) {  WDTCTL = WDTPW + WDTHOLD; // Stop WDT  P1DIR |= 0xff; // P1.0 - P1.7 output  P2DIR |= 0x0f; // P2.0 - P2.3 for COMx  CCTL0 = CCIE; // CCR0 interrupt enabled  CCR0 = 5000; // Set the initial value for CCR0  TACTL = TASSEL_2 + MC_1; // SMCLK, upmode  __bis_SR_register(LPM0_bits + GIE); // Enter LPM0 w/ interrupt  }  // Timer A0 interrupt service routine #pragma vector=TIMER0_A0_VECTOR __interrupt void Timer_A (void) {  static unsigned char num = 0;  // Scanning  switch (num)  {  case 0: P2OUT = BIT0; P1OUT = strtab[num]; break;  case 1: P2OUT = BIT1; P1OUT = strtab[num]; break;  case 2: P2OUT = BIT2; P1OUT = strtab[num]; break;  case 3: P2OUT = BIT3; P1OUT = strtab[num]; break;  default: break;  }  num++;  if (num == 4)  num = 0;   // Timing \tt++; \tif (t == 200) // 5ms * 200 = 1S (one minute time--) \t{ \tif (time \u0026gt; 0) \ttime--; \telse \ttime = 9999; \t}  \t// Displaying \tstrtab[0] = tab[(time / 1000) % 10]; \tstrtab[1] = tab[(time / 100) % 10]; \tstrtab[2] = tab[(time / 10) % 10]; \tstrtab[3] = tab[time % 10]; } 图片记录\n","permalink":"http://landodo.github.io/posts/20180420-smg/","summary":"认识七段数码管 数码管的一种是半导体发光器件，数码管可分为七段数码管和八段数码管，区别在于八段数码管比七段数码管多一个用于显示小数点的发光二极","title":"【MSP430G2553】数码管"},{"content":" 这是我参加 2018 届蓝桥杯所学习的资料。虽然比赛结果不尽如人意，但是学习 51 单片机的过程是值得回忆的，也是有所收获的。\n 一、准备工作 1、硬件\nCT107D实验平台、IAP15F2K61S2转接芯片\n2、软件\n Keil uVision4 含注册机 stc-isp-15xx-v6.86I.exe  3、蓝桥官网资料：lanqiao.org\n蓝桥杯全国软件和信息技术专业人才大赛辅导资料 「http://dasai.lanqiao.cn/pages/dasai/news_detail.html?id=644」\n4、学习资料\n 51教程论坛 CT107D单片机实训平台实验历程 常用芯片资料 驱动及其下载软件stc-isp-15xx-v6.86I.exe CT107D电路原理图 CT107D使用说明书 IAP15F2K61S2-89C52转换板说明文件 IAP15F2K61S2 单片机仿真使用说明 STC15F2K60S2 单片机用户手册 转接板PCB 转接板电路原理图  5、历年真题\n6、学习网站\nCSDN——专业IT技术社区\n7、浏览器推荐\n网盘资源\n 软件安装 学习资料 历年真题   二、基础模块学习 0、74138\n1、LED\n2、蜂鸣器\u0026amp;继电器\n3、定时器 0/1\n4、数码管\n5、独立键盘\n6、矩阵键盘\n7、实时时钟模块\n8、EEPROM-AT24C02 存储芯片\n9、DS18B20 温度传感器\n10、PCF8591 光敏电阻\n网盘资源\n 各模块中文数据手册 模块参考程序 官方提供的模块驱动程序   三、调试通过的完整真题程序 学习他人优秀的编码风格，模块化编程、变量命名等。\n 四、随便说说 我失败的原因：\n  应试能力、随机应变能力差\n  忽视基础的重要性\n  平时训练的方法有误\n  总体写代码水平有待提高\n  ","permalink":"http://landodo.github.io/posts/20180417-lan-qiao/","summary":"这是我参加 2018 届蓝桥杯所学习的资料。虽然比赛结果不尽如人意，但是学习 51 单片机的过程是值得回忆的，也是有所收获的。 一、准备工作 1、硬件 CT107","title":"蓝桥杯个人赛电子类_单片机资料整理"},{"content":"HC-05 产品参数 1、PCB 尺寸：37.3mm * 15.5mm\n2、重量：3.5g\n3、输入电压：3.6 - 6V，电源自带反接，反接不工作。\n4、引出 6 个引脚：EN/VCC/GND/RX/TX/STATE（STATE：蓝牙状态引脚，未连接输出低电平，连接后输出高电平）\n5、带连接指示灯。LED 快闪表示没有蓝牙连接；LED 慢闪表示进入 AT 命令模式。\n6、板载 3.3V ，可以直接连接单片机（Arduino/C51/AVR/ARM/MSP430等），5V 单片机可以直接连接。\n7、正常情况下，蓝牙模块的通信距离是10M左右。\n8、按下按键再给蓝牙模块通电可以进入 AT 模式，设置参数和查询信息。\n9、可以通过 AT 命令切换主机和从机模式。\n10、模块默认波特率为9600、默认配对密码为 1234 或 0000 、默认名称 HC-05、LED状态灯开启。\n11、根据苹果通信协议，iPhone 系列设备可能不支持HC05模块。\nHC-05 模块实物图 HC-05 模块原理图 MSP430G2553 实物图 MSP430G2553 引脚图 接线方式 VCC（+5 V）：接电源正极（+5 V）\nGND：接电源负极（GND）\nRX：接收端，蓝牙模块接收从其它设备发来的数据；正常情况下，连接其它设备的发送端 TX。\nTX：发送端，蓝牙模块发送数据给其它设备；正常情况下，连接其它设备的接受端RX。\nEN：使能端。\nTips:\nRXD(Receive Data)\nTXD(Transmit Data)\nMSP430G2553 的数据发送端（TX）位于 P1.1、数据接收端（RX）位于 P1.2。\n官方代码参考 #include \u0026lt;msp430.h\u0026gt; //------------------------------------------------------------------------------ // Hardware-related definitions //------------------------------------------------------------------------------ #define UART_TXD 0x02 // TXD on P1.1 (Timer0_A.OUT0) #define UART_RXD 0x04 // RXD on P1.2 (Timer0_A.CCI1A)  //------------------------------------------------------------------------------ // Conditions for 9600 Baud SW UART, SMCLK = 1MHz //------------------------------------------------------------------------------ #define UART_TBIT_DIV_2 (1000000 / (9600 * 2)) #define UART_TBIT (1000000 / 9600)  //------------------------------------------------------------------------------ // Global variables used for full-duplex UART communication //------------------------------------------------------------------------------ unsigned int txData; // UART internal variable for TX unsigned char rxBuffer; // Received UART character  //------------------------------------------------------------------------------ // Function prototypes //------------------------------------------------------------------------------ void TimerA_UART_init(void); void TimerA_UART_tx(unsigned char byte); void TimerA_UART_print(char *string);  //------------------------------------------------------------------------------ // main() //------------------------------------------------------------------------------ int main(void) {  WDTCTL = WDTPW + WDTHOLD; // Stop watchdog timer  if (CALBC1_1MHZ==0xFF) // If calibration constant erased  {  while(1); // do not load, trap CPU!!  }   DCOCTL = 0; // Select lowest DCOx and MODx settings  BCSCTL1 = CALBC1_1MHZ; // Set DCOCLK to 1MHz  DCOCTL = CALDCO_1MHZ;   P1OUT = 0x00; // Initialize all GPIO  P1SEL = UART_TXD + UART_RXD; // Timer function for TXD/RXD pins  P1DIR = 0xFF \u0026amp; ~UART_RXD; // Set all pins but RXD to output   __enable_interrupt();   TimerA_UART_init(); // Start Timer_A UART  TimerA_UART_print(\u0026#34;READY.\u0026#34;);   for (;;)  {  // Wait for incoming character  __bis_SR_register(LPM0_bits);   // Update board outputs according to received byte  if (rxBuffer \u0026amp; 0x01) P1OUT ^= 0x01; else P1OUT \u0026amp;= ~0x01; // P1.0   // Echo received character  TimerA_UART_tx(rxBuffer);  } } //------------------------------------------------------------------------------ // Function configures Timer_A for full-duplex UART operation //------------------------------------------------------------------------------ void TimerA_UART_init(void) {  TACCTL0 = OUT; // Set TXD Idle as Mark = \u0026#39;1\u0026#39;  TACCTL1 = SCS + CM1 + CAP + CCIE; // Sync, Neg Edge, Capture, Int  TACTL = TASSEL_2 + MC_2; // SMCLK, start in continuous mode } //------------------------------------------------------------------------------ // Outputs one byte using the Timer_A UART //------------------------------------------------------------------------------ void TimerA_UART_tx(unsigned char byte) {  while (TACCTL0 \u0026amp; CCIE); // Ensure last char got TX\u0026#39;d  TACCR0 = TAR; // Current state of TA counter  TACCR0 += UART_TBIT; // One bit time till first bit  TACCTL0 = OUTMOD0 + CCIE; // Set TXD on EQU0, Int  txData = byte; // Load global variable  txData |= 0x100; // Add mark stop bit to TXData  txData \u0026lt;\u0026lt;= 1; // Add space start bit }  //------------------------------------------------------------------------------ // Prints a string over using the Timer_A UART //------------------------------------------------------------------------------ void TimerA_UART_print(char *string) {  while (*string) {  TimerA_UART_tx(*string++);  } } //------------------------------------------------------------------------------ // Timer_A UART - Transmit Interrupt Handler //------------------------------------------------------------------------------ #pragma vector = TIMER0_A0_VECTOR __interrupt void Timer_A0_ISR(void) {  static unsigned char txBitCnt = 10;   TACCR0 += UART_TBIT; // Add Offset to CCRx  if (txBitCnt == 0) { // All bits TXed?  TACCTL0 \u0026amp;= ~CCIE; // All bits TXed, disable interrupt  txBitCnt = 10; // Re-load bit counter  }  else {  if (txData \u0026amp; 0x01) {  TACCTL0 \u0026amp;= ~OUTMOD2; // TX Mark \u0026#39;1\u0026#39;  }  else {  TACCTL0 |= OUTMOD2; // TX Space \u0026#39;0\u0026#39;  }  txData \u0026gt;\u0026gt;= 1;  txBitCnt--;  } } //------------------------------------------------------------------------------ // Timer_A UART - Receive Interrupt Handler //------------------------------------------------------------------------------ #pragma vector = TIMER0_A1_VECTOR __interrupt void Timer_A1_ISR(void) {  static unsigned char rxBitCnt = 8;  static unsigned char rxData = 0;   switch (__even_in_range(TA0IV, TA0IV_TAIFG)) { // Use calculated branching  case TA0IV_TACCR1: // TACCR1 CCIFG - UART RX  TACCR1 += UART_TBIT; // Add Offset to CCRx  if (TACCTL1 \u0026amp; CAP) { // Capture mode = start bit edge  TACCTL1 \u0026amp;= ~CAP; // Switch capture to compare mode  TACCR1 += UART_TBIT_DIV_2; // Point CCRx to middle of D0  }  else {  rxData \u0026gt;\u0026gt;= 1;  if (TACCTL1 \u0026amp; SCCI) { // Get bit waiting in receive latch  rxData |= 0x80;  }  rxBitCnt--;  if (rxBitCnt == 0) { // All bits RXed?  rxBuffer = rxData; // Store in global variable  rxBitCnt = 8; // Re-load bit counter  TACCTL1 |= CAP; // Switch compare to capture mode  __bic_SR_register_on_exit(LPM0_bits); // Clear LPM0 bits from 0(SR)  }  }  break;  } } //------------------------------------------------------------------------------ 使用说明 首先将 HC-05 与 MSP430G2553 正确连接连接，HC-05 上的 LED 快闪说明没有设备连接；打开手机蓝牙，查找设备，与查询列表中的 HC-05 进行配对，（默认配对密码为 1234 或 0000 ），HC-05 上的 LED 慢闪说明配对完成，设备成功连接。\n使用 IAR 进行程序下载，步骤可以参照[IAR Embedded Workbench 基本使用]。\n手机应用商城搜索 「蓝牙串口」，并进行下载。\n打开蓝牙串口工具，右上角点击「连接」，与 HC-05 连接成功后，手机可以接收到 HC-05 发送的 「READ.」，按照程序  if (rxBuffer \u0026amp; 0x01) P1OUT ^= 0x01; else P1OUT \u0026amp;= ~0x01; // P1.0 此时在手机端发送1，即可点亮 MSP430G2553 的 P1.0 口的 LED。\n依照此基本程序可以实现手机蓝牙与单片机的通信。手机遥控车，控制单片机开／关空调，等等，都基于此。\n资料下载 MSP430G2553 官方数据手册\nMSP430G2x53, MSP430G2x33, MSP430G2x13, MSP430G2x03 Code Examples 官方参考程序\nMSP430G2553 ——HC-05 源程序\nHC-05 蓝牙模块原理图\nHC-05 AT指令集\n链接:https://pan.baidu.com/s/1iWQ1OlvuvIcN3NSkgQEdMg\n密码:st5t\n","permalink":"http://landodo.github.io/posts/20180414-msp430g2553-hc-05/","summary":"HC-05 产品参数 1、PCB 尺寸：37.3mm * 15.5mm 2、重量：3.5g 3、输入电压：3.6 - 6V，电源自带反接，反接不工作。 4、引出 6 个引脚：EN/V","title":"【MSP430G2553单片机】HC-05蓝牙模块"},{"content":"背景 因为急需一台 Windows 操作系统的电脑，装些短期需要用到的软件。而手里有的是一台预装了 Windows 10 操作系统的电脑，配置很差，右键刷新都得等好一会。\n安装了 Windows 7 后的截图 遇到的问题 我下载的两个 Win 7 镜像链接（使用的是迅雷下载）\n cn_windows_7_ultimate_with_sp1_x64_dvd_u_677408.iso [3.42GB]  ed2k://|file|cn_windows_7_ultimate_with_sp1_x64_dvd_u_677408.iso|3420557312|B58548681854236C7939003B583A8078|/ cn_windows_7_ultimate_x64_dvd_x15-66043.iso [3.34GB]  ed2k://|file|cn_windows_7_ultimate_x64_dvd_x15-66043.iso|3341268992|7DD7FA757CE6D2DB78B6901F81A6907A|/ 镜像比较的好找，善用搜索。\n问题一： 缺少所需的CD/DVD驱动器设备驱动程序\u0026hellip; 解决方法 1：U 盘拔了重插。[尝试失败！]\n解决方法 2：U 盘插 USB2.0 接口。 [尝试失败！]\n解决方法 3：Windows USB Installation Tool\n使用方法：Source Path(CD-ROM) 选择 None - Add USB drivers ；Destination Path(USB Drive) 选择 你的 U 盘；勾选 Add USB drivers to an offline Windows 7 image. ；点击 Start。补充：笔者通过 U 盘的方式安装。\n耗时：1 小时\n通过方法 3 可以解决「缺少所需的CD/DVD驱动器设备驱动程序…」问题。\n问题 二：无法将 Windows 安装到磁盘 0 的分区.. 解决步骤：\n把MBR磁盘转换为GPT磁盘。\n SHIFT+F10调出命令提示符 输入 diskpart 输入 list disk ，列出系统拥有的磁盘 select disk 0，选择0号磁盘 clean ，清除磁盘，原先的 Windows 10 系统等都会被抹去 convert gpt，将磁盘转换为GPT格式 刷新，继续完成安装。  问题 3 ：安装完毕后的系统无法联网 解决方法:\n下载驱动精灵的「万能网卡驱动」。（换台能用的电脑下，用 U 盘拷回来）\n问题 4 ：笔记本触控板失灵 下载笔记本厂商官方提供的驱动。笔者的电脑触控板问题未解决，原因：官方推荐使用 Windows 10 操作系统。\n问题 5 ：Windows 7 激活失败 原因：操作系统在安装的时候将磁盘转换为了 GPT 格式，普通的激活工具一般无法激活，需要使用 GPT Windows7 激活工具进行激活。搜索关键字「GPT Windows7激活」即可找到多种激活工具。\n","permalink":"http://landodo.github.io/posts/20180227-windows7/","summary":"背景 因为急需一台 Windows 操作系统的电脑，装些短期需要用到的软件。而手里有的是一台预装了 Windows 10 操作系统的电脑，配置很差，右键刷新都得等好一会。 安装了 Windows","title":"Windows 7 操作系统的安装"},{"content":"生成烧录文件  Create New Project 选择 Empty Project 。这个看个人喜好，我一般会选择创建空的工程。` 在 File 新建一个 Document 用于编写程序，保存时注意添加后缀 .c，然后（Add）添加进工程。xxx.h 等自己编写的头文件也可如此添加。 在工程的 General Option -\u0026gt; Device 选择对应的单片机型号。MSP430f149 单片机使用 BSL 工具进行烧录，所以需要生成 .txt 格式的文件，在 Linker -\u0026gt; Output 下选择 Other ，即 Output 为 msp430-txt。 Compile 编译通过后，点击Make ，就可以在工程的文件夹的 Debug/Exe/ 目录下看到生成的 .txt 文件。 使用 BSL 工具装载烧录文件，点击执行。这一步常见的错误有 ：打开串口出错、BSL 初始化失败。通过搜索可以解决。  截图  选择相应的单片机型号   生成烧录文件  在线仿真 这里指的是不需要生成烧录文件，即不需要通过 BSL 等烧录软件转载烧录文件，可以使用 IAR 的 Download and Debug 一键将程序下载到单片机中。注意以下配置：\n Option 下首先选择 Debugger ，在 Setup 页面中的 Driver 栏选择 FET Debugger 。 其次，选择 FET Debugger， 在Setup 页面中的 Connection 栏选择 Texas Instrument USB-IF ，然后再选择旁边的 COM口。  截图  配置 Debugger   配置 FET Debugger   配置完成、下载程序  ","permalink":"http://landodo.github.io/posts/20180111-iar/","summary":"生成烧录文件 Create New Project 选择 Empty Project 。这个看个人喜好，我一般会选择创建空的工程。` 在 File 新建一个 Document 用于编写程序，保存时注意添加后缀 .c，然后（Add）添","title":"IAR Embedded Workbench 基本使用"},{"content":"光标 ⌘→ ：行尾\n⌘← ：行首\n⌘↑：文件头\n⌘↓：文件尾\n词操作 ⌘D：选中光标所占的文本，继续操作则会选中下一个相同的文本\n⌘⌥F：替换\n行操作 ⌘↩ ：当前行下插入新行\n⌘⇧↩ ：当前行上插入新行\n⌃⇧K：删除光标所在行\n⌘⇧D：当前行下添加相同行\n⌘L：选中当前行（重复按下将下一行加入选择）\n⌘KK：从光标处删除至行尾\n⌃G：前往行\n块操作 ⌘／：注释\n⌘⌥/：块注释\n⌘[ ：左缩进\n⌘]：右缩进\n窗口 ⌘⌥数字：分屏\n⌃tab：窗口下的标签切换\n⌃数字：窗口间切换\n⌘⇧[：标签切换\n","permalink":"http://landodo.github.io/posts/20180105-sublime/","summary":"光标 ⌘→ ：行尾 ⌘← ：行首 ⌘↑：文件头 ⌘↓：文件尾 词操作 ⌘D：选中光标所占的文本，继续操作则会选中下一个相同的文本 ⌘⌥F：替换 行操作 ⌘↩ ：当前","title":"Sublime Text 编辑器快捷键"},{"content":"Normal 模式 编辑  i 光标前插入 I 当前行首插入 a 光标后插入 A 当前行尾插入 o 当前行上一行插入 O 当前行下一行插入 dd 删除（剪切）光标所在行 u 撤销 x 删除光标当前字符 yy 复制当前行 nyy 复制 n 行 p 光标后粘贴 P 光标前粘贴 :w filename 另存为 filename :%s/str1/str2/g  用字符串 str2 替换文件中所有的字符串 str1  查看  :set nu 显示行号 :/str 搜索字符串 str ，n 继续搜索，找出 str 下次出现的位置 ctrl + f 下一页 ctrl + b 上一页  光标   h j k l 光标键：左 下 上 右\n  w 光标到下一个单词的开头\n  b 光标到上一个单词的开头\n  e 到下一个单词的结尾\n  % 匹配括号移动v\n  # * 匹配当前光标所在单词，移动光标到 上一个／ 下一个\n  0  光标到行头（阿拉伯数字零）\n  $ 光标到行尾\n  H 光标移到显示器的最上行\n  M 光标移到中间\n  L 光标移到最下行\n  gg 光标定位到第一行\n  G 光标移动到文件的最后一行\n  NG 光标定位到第 N 行\n  :N 光标定位到第 N 行\n  多文件  ctrl + 6 两个文件之间切换 :bn／ :n 下一个文件 :bp／ :N上一个文件 :ls／ :files查看当前打开的文件 :b num 切换到第 num 个文件 :sp 上下窗口打开文件 :vsp 左右窗口打开文件 ctrl + ww 窗格间的切换  配置 插件的安装Vim配置及插件安装自动化脚本\n 脚本地址:[https://github.com/Wangzhike/VimConfigScript]\n前往地址\n // .vimrc // 我的 vim 配置 syntax on set nu hi Comment ctermfg=DarkCyan set backspace=2 set mouse=a set selection=exclusive set selectmode=mouse,key filetype on filetype plugin on filetype indent on set fileencodings=utf-8,gbk set encoding=euc-cn set ambiwidth=double \u0026#34; 高亮当前行 set cursorline set hlsearch set incsearch set cindent set tabstop=4 set shiftwidth=4 set autoindent set showmatch set matchtime=1 \u0026#34; 防止中文乱码 set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936 set termencoding=utf-8 set encoding=utf-8 ","permalink":"http://landodo.github.io/posts/20180102-vim/","summary":"Normal 模式 编辑 i 光标前插入 I 当前行首插入 a 光标后插入 A 当前行尾插入 o 当前行上一行插入 O 当前行下一行插入 dd 删除（剪切）光标所在行 u 撤销 x 删除光标当","title":"VIM 编辑器笔记"},{"content":" 以下是我的个人电脑浏览器常用的工具，这篇笔记的目的是保存下我最常使用的一些工具链接，以便于我自己的电脑不在身边的时候，我能将身边的任何一台电脑变成我称手的利器。\n 搜索 Bird.so 小众搜索引擎\nAvira SafeSearch——德国小红伞\nUOL busca\nECOSIA\nРамблер — медийный портал\nMEZW搜索\n纸飞机\nChrome 插件—谷歌访问助手\n 其他 TinyPNG——图片压缩\n变量的命名\nFree stock photos · Pexels\nBeautiful Free Images Unsplash\nRGB颜色值与十六进制颜色码转换工具\n[我的 Sublime Test——插件]\n  Alignment C++11 C++YouCompleteMe Color Highlighter Markdown Editing Package Control ClangAutoComplete ConvertToUTF8 EasyClangComplete OmniMarkupPreviewer   在线工具\n 在线运行代码 ( PHP / C / C++ / Python / Go / Java / NodeJS / Lua / Groovy / Bash) 美化代码 ( JavaScript / CSS / HTML / PHP / Python )   更新时间 2018-01-02\n 添加在线工具  2018-01-01\n 添加我的 Sublime Text 常用插件  2017-12-31\n 添加纸飞机 添加 Chrome 插件—谷歌访问助手  2017-12-29\n 添加十项常用工具  ","permalink":"http://landodo.github.io/posts/20171229-tools/","summary":"以下是我的个人电脑浏览器常用的工具，这篇笔记的目的是保存下我最常使用的一些工具链接，以便于我自己的电脑不在身边的时候，我能将身边的任何一台电","title":"我的常用工具"},{"content":" 制作一个简单的网页留言板，将游客的留言存储到数据库，管理员登录查看所有的留言并选择是否回复，游客可以查看到管理员的回复。学会使用基本的数据库操作。\n 网页的界面  环境 XAMPP\n 数据库phpMyAdmin [http://localhost:8080/phpmyadmin/]\n服务器：localhost\n数据库：dbook\n表：message\n表结构：\n name email info reply reply   留言板的基本操作 留言  将留言板信息存储到数据库中   DEMO\nName：西瓜\nEmail：watermelon@163.com\nMessage：我是西瓜！\n 查看留言 回复 查看回复  PHP 操作数据库 数据写入数据库 // 将留言信息写入数据库 \u0026lt;?php  $name = $_POST[\u0026#34;Name\u0026#34;];  $email = $_POST[\u0026#34;Email\u0026#34;];  $message = $_POST[\u0026#34;Message\u0026#34;];  if ($name == \u0026#34;\u0026#34; || $email == \u0026#34;\u0026#34; || $message == \u0026#34;\u0026#34;) {  echo \u0026#34;Name: \u0026#34;,$_POST[\u0026#34;Name\u0026#34;], \u0026#34;\u0026lt;br\u0026gt;\u0026#34;;  echo \u0026#34;Email: \u0026#34;, $_POST[\u0026#34;Email\u0026#34;], \u0026#34;\u0026lt;br\u0026gt;\u0026#34;;  echo \u0026#34;Message: \u0026#34;, $_POST[\u0026#34;Message\u0026#34;], \u0026#34;\u0026lt;br\u0026gt;\u0026#34;;  echo \u0026#34;留言失败！留言信息输入不完整!\u0026#34;;  exit(); }  else  {  echo \u0026#34;留言板信息写入数据库\u0026lt;/br\u0026gt;\u0026#34;;  $mysql_server_name=\u0026#34;localhost\u0026#34;; //数据库服务器名称  $mysql_username=\u0026#34;root\u0026#34;; // 连接数据库用户名  $mysql_password=\u0026#34;\u0026#34;; // 连接数据库密码  $db_name = \u0026#34;dbook\u0026#34;; //数据库名称 \t //连接到数据库服务器  $conn = mysqli_connect($mysql_server_name, $mysql_username,$mysql_password) or die(\u0026#39;数据库连接失败\u0026lt;/br\u0026gt;\u0026#39;. mysqli_error($conn));   // 选定数据库  $select = mysqli_select_db($conn, $db_name) or die(\u0026#39;选定数据库失败\u0026lt;/br\u0026gt;\u0026#39;. mysqli_error($conn));   // 新元素的插入  mysqli_query($conn,\u0026#39;set names utf8\u0026#39;); // 设置字符集，防止乱码  $sql = \u0026#34;INSERT INTO message (name, email, info) VALUES (\u0026#39;$name\u0026#39;, \u0026#39;$email\u0026#39;, \u0026#39;$message\u0026#39;)\u0026#34;;  if (mysqli_query($conn, $sql)){  echo \u0026#34;留言保存成功\u0026lt;/br\u0026gt;\u0026#34;;  } else{  echo \u0026#34;留言保存失败\u0026lt;/br\u0026gt;\u0026#34;;  }  }  mysqli_close($conn); ?\u0026gt;从数据库中读取数据 // 将留言信息丛数据库中读出 \u0026lt;?php  // 从数据库中读取数据 --\u0026gt;  $mysql_server_name=\u0026#34;localhost\u0026#34;; //数据库服务器名称  $mysql_username=\u0026#34;root\u0026#34;; // 连接数据库用户名  $mysql_password=\u0026#34;\u0026#34;; // 连接数据库密码  $db_name = \u0026#34;dbook\u0026#34;; //数据库名称  //连接到数据库服务器  $conn = mysqli_connect($mysql_server_name, $mysql_username,$mysql_password) or die(\u0026#39;数据库连接失败\u0026lt;/br\u0026gt;\u0026#39;. mysqli_error($conn));   // 选定数据库  $select = mysqli_select_db($conn, $db_name) or die(\u0026#39;选定数据库失败\u0026lt;/br\u0026gt;\u0026#39;. mysqli_error($conn));   mysqli_query($conn,\u0026#39;set names utf8\u0026#39;); //设置读取数据后的编码  $sql = \u0026#39;SELECT name, email, info FROM message\u0026#39;;  $result = mysqli_query($conn, $sql) or die(\u0026#39;获取数据失败\u0026lt;/br\u0026gt;\u0026#39;. mysqli_error($conn));  if (mysqli_num_rows($result) \u0026gt; 0) {   // 输出数据  while($row = mysqli_fetch_assoc($result)) {  echo \u0026#34;Name: \u0026#34;, $row[\u0026#34;name\u0026#34;], \u0026#34;\u0026lt;br\u0026gt;\u0026#34;, \u0026#34;Email: \u0026#34;, $row[\u0026#34;email\u0026#34;],\u0026#34;\u0026lt;br\u0026gt;\u0026#34;, \u0026#34;Message:\u0026#34;, $row[\u0026#34;info\u0026#34;] ;  echo \u0026#34;\u0026lt;br\u0026gt;\u0026#34;, \u0026#34;\u0026lt;br\u0026gt;\u0026#34;;  }  }  mysqli_close($conn); ?\u0026gt;更新数据库信息 \u0026lt;?php $name = $_POST[\u0026#34;Name\u0026#34;]; $reply = $_POST[\u0026#34;Message\u0026#34;]; if ($name == \u0026#34;\u0026#34; || $reply == \u0026#34;\u0026#34;) {  echo \u0026#34;回复者姓名: \u0026#34;, $_POST[\u0026#34;Name\u0026#34;], \u0026#34;\u0026lt;br\u0026gt;\u0026#34;;  echo \u0026#34;回复内容: \u0026#34;, $_POST[\u0026#34;Message\u0026#34;], \u0026#34;\u0026lt;br\u0026gt;\u0026#34;;  echo \u0026#34;留言失败！留言信息输入不完整!\u0026#34;; } else {  echo \u0026#34;留言板信息正在写入数据库\u0026lt;/br\u0026gt;\u0026#34;;  $mysql_server_name = \u0026#34;localhost\u0026#34;;  //数据库服务器名称  $mysql_username = \u0026#34;root\u0026#34;;  // 连接数据库用户名  $mysql_password = \u0026#34;\u0026#34;;  // 连接数据库密码  $db_name = \u0026#34;dbook\u0026#34;;  //数据库名称  //连接到数据库服务器  $conn = mysqli_connect($mysql_server_name, $mysql_username, $mysql_password);  if ($conn) {  echo \u0026#34;数据库连接成功\u0026lt;/br\u0026gt;\u0026#34;;  } else {  echo \u0026#34;数据库连接失败\u0026lt;/br\u0026gt;\u0026#34;;  }  // 选定数据表  $select = mysqli_select_db($conn, $db_name);  if ($select) {  echo \u0026#34;选定数据库成功\u0026lt;/br\u0026gt;\u0026#34;;  } else {  echo \u0026#34;选定数据库失败\u0026lt;/br\u0026gt;\u0026#34;;  } } // 表 message: name, email, info, reply // $name = 小王 mysqli_query($conn, \u0026#39;set names utf8\u0026#39;); // 设置字符集，防止乱码 $sql = \u0026#34;UPDATE message SET reply=\u0026#39;{$reply}\u0026#39; WHERE name=\u0026#39;{$name}\u0026#39;\u0026#34;; if (mysqli_query($conn, $sql)) {  echo \u0026#34;\u0026lt;br\u0026gt;留言回复成功\u0026lt;/br\u0026gt;\u0026#34;;  echo \u0026#34;回复者姓名：\u0026#34;, $name, \u0026#39;\u0026lt;br\u0026gt;\u0026#39;;  echo \u0026#34;回复信息：\u0026#34;, $reply; } else {  echo \u0026#34;留言回复失败\u0026lt;/br\u0026gt;\u0026#34;; } mysqli_close($conn); 留言板工程源码 网页留言板\n小作业\n更新 2018-01-02\n 增加 reply 回复留言人 留言人查看回复  ","permalink":"http://landodo.github.io/posts/20171228-message-board/","summary":"制作一个简单的网页留言板，将游客的留言存储到数据库，管理员登录查看所有的留言并选择是否回复，游客可以查看到管理员的回复。学会使用基本的数据库","title":"网页留言板"},{"content":"任务书  设计名称：学生宿舍管理 宿舍信息包括：宿舍号、性别、容纳人数、住宿费用、住宿学生姓名。试设计一学生宿舍信息管理系统，使之能提供以下功能：\n1．宿舍基本信息（包含宿舍号、性别、容纳人数、住宿费用）录入；\n2．将学生分配到指定宿舍号的宿舍；\n3．已住宿的学生退宿舍；\n4．查询宿舍信息，包括宿舍号、性别、容纳人数、住宿费用、已住宿的人数、已住宿学生姓名。\n5．系统以菜单方式工作。\n 分析逻辑图 程序结构图 项目源程序 GitHub\n [https://github.com/dongleilan/University/tree/master/data-structure/DormitoryManagement]\n ","permalink":"http://landodo.github.io/posts/20171224-data-structure/","summary":"任务书 设计名称：学生宿舍管理 宿舍信息包括：宿舍号、性别、容纳人数、住宿费用、住宿学生姓名。试设计一学生宿舍信息管理系统，使之能提供以下功能：","title":"数据结构课程设计"},{"content":"「怎样学英语才是最有效的」  我的小书屋 mebook.cc 中的一本书，书名《把你的英语用起来！新版》伍君仪\u0026amp;刘晓光\n  学英语，要么彻底放弃，要么就必须一鼓作气，彻底把英语学好。 积累单词才是正道！ 语法是非常重要的！ 口语不好的原因：听力输入量实在太少，对英文的模仿练习实在太少！ 对于专业人士来说，要么彻底放弃英语，要么就必须把一鼓作气，建立一个量化的考核标准，利用 1 ～ 2 年的时间，每天抽出 1～2 个小时的茶余饭后时间，彻底把英文学好，学到足够把英文作为自己的学习和工作语言使用的程度。否则断断续续学了 10 年，反复拿起和放下，今天看两页书，明天抛之脑后，后天捡起发现全部都忘记了，实在太难过了。英文学习若不能一鼓作气，不如彻底放弃。 误区二：语法无用论或语感重要论。如果你只需要很烂的听和说的能力，那你就不需要学习什么语法。 误区三：口语万能至上论。 英文学习，任何方法其实都是有效的，只要坚持做。 从心理学角度上来说，当你下定决心做某件事的时候，比如戒烟或者减肥，最好 keep it to yourself（只让你自己一个人知道）。因为当你和别人分享的时候，这种分享的行为会造成一种假象和错觉，大大减缓你真正做成事情之后的满足感，继而无法对你的坚持性行为产生驱动力，所以在事情没有做成前不要到处列计划宣扬，这会大大降低你的成功率。 网络论坛最具破坏性的一面：干扰性太强。 任何长期性自我改造和自我学习的过程，都必须在自己的监督和跟踪下完成。 要时刻记录下自己的收获和成就，把自己的英语水平时时记录下来。 人的大脑和潜意识都是有欺骗性的。 作息习惯绝对不要因为假期而改变。  我的英语学习库  GitHub-English 我自己推送至Kindle的时事技术新闻热点「每日更新」——百度网盘资源  ","permalink":"http://landodo.github.io/posts/20171224-study-english/","summary":"「怎样学英语才是最有效的」 我的小书屋 mebook.cc 中的一本书，书名《把你的英语用起来！新版》伍君仪\u0026amp;刘晓光 学英语，要么彻底放弃，要么就必须一鼓作","title":"每个人都要有属于自己的英语学习方法"},{"content":"背景 我接触树莓派已有几个月的时间了，一直在使用树莓派作为一个平台学习去其他方面的知识，可以很确定的说，我的 Linux 学习之路，是树莓派带我入的门。但是现在我正在使用的操作系统为 Mac OX ，所以树莓派被我闲置了有一段时间了。\n树莓派对我的帮助是巨大的，所以，我特意写这一篇博客，用来记录并保存树莓派的进度。希望几个月或者更长的时间过后再次拿起树莓派，我可以不用在网上找教程，而是复习自己的笔记，就可以从现在停留的地方继续的开始；又或者是，通过看一遍自己的笔记，并结合网上的教程，让我能在很少的时间里快速的想起自己在树莓派上曾经做过了什么、遇到的问题、解决的办法等等。\n学习的网站 关于树莓派的学习网站。我在玩树莓派的时候不知道任何有关树莓派的论坛，大多是在浏览器上直接进行搜索，但是从我的亲身经历来看，这并不是一个好的入坑方式。所以，我记录下了一些对我帮助很大的网站。\n 官方网站 - https://www.raspberrypi.org 树莓派实验室 - http://shumeipai.nxez.com CSDN http://www.csdn.net  以上三个网站是我学习树莓派的过程中使用得最多的网站。\n我首推的是树莓派的官方网站，官方网站上拥有更加权威的教程，若是英语能力强，我强烈建议阅读官方提供的指导手册，这比看博客要好多。\n然后是「树莓派实验室」，「树莓派实验室」是一个关于树莓派的中文资讯站，提供丰富的树莓派使用教程和DIY资讯。根据它提供的教程，可以很快的完成树莓派环境的搭建，还可以根据里面的博文教程，使用树莓派完成很多小项目，而且，里面还很贴心的提供给使用者可能会需要到的软件了链接，比如 PUTTY、IP SCANNER、SD格式化工具等等。\n其次是 CSDN，它是中国最大的 IT 社区和服务平台，里面有很多高质量的博客、教程、资源，可以助你一臂之力。\n目录  树莓派的简介 准备工作 安装操作系统 登录到树莓派 使用树莓派做些事  树莓派的简介 树莓派（Raspberry Pi）是一款由英国的树莓派基金会开发的、信用卡大小的单板电脑，最初目的是促进及提升在校学生的基础计算机教育水平，但由于其价格低廉，并且已经有大量操作系统（主要是 Linux）被移植到了树莓派上，越来越多的技术人员及爱好者都开始使用树莓派来开发有用或有趣的软件硬件系统。\n准备工作 以下是我在树莓派的学习过程中所使用到的东西：\n  树莓派\n  SD 卡\n  读卡器\n  网线\n  HDMI 转 VGA\n  键盘和鼠标\n  安装操作系统 树莓派的官方网站提供了针对树莓派的操作系统，如下：\n关于树莓派的操作系统可有不同的选择，我选择的是以下这个带图形界面的：\n安装的过程很简单，安装教程很容易找得到。我使用的是一台 Windows7 的台式点脑，使用 [Win32DiskImager] 将下载好的操作系统镜像写入 SD 卡，再将 SD 卡插入树莓派，通电启动即可。\n登录树莓派 操作系统安装完毕后，如何登录自己的树莓派呢？方法有两种：\n 使用 HDMI 转 VGA 连接外接显示器 通过 SSH 或 VNG 远程登录  第一种方法很简单，也最直接，但是成本过高。在没有转接线和显示器的情况下，可以使用第二种方法。通过 SSH 远程登录，SSH 远程登录需要知道树莓派的 IP 地址，我使用 [Advanced IP Scanner] 获取到树莓派的 IP。\n使用树莓派做些事 以下是我使用树莓派做的一个小工程：\n GPIO  我使用树莓派的 GPIO 外接一个小风扇，并通过一个Python 脚本开机自启、后台执行，实现对树莓派芯片的控温。只须一行代码就可读取到树莓派芯片的温度，根据此时的芯片温度判断是否开启／关闭风扇。\n USB 摄像头  我尝试安装 OpenCV，并使用摄像头做监控、识别，但是 OpenCV 在编译安装的时候频频报错，安装过程持续了好几个晚上，每次我执行 make 指令后，便回寝室休息，第二天早上再查看安装状况，可是结果总是不尽人意。目前只能监控、拍照，还无法识别。\n我用到的软件 以下的这些软件是我在学习树莓派时使用到的工具：\n PuTTY SD 卡格式化工具 —— 有墙 Win32DiskImager Advanced IP Scanne VNC Viewer  最后 关于树莓派的第一阶段学习我做了一个总结性的 PPT ，主要是一些我遇到的问题和解决方法。托管在了 GitHub 上。\n 2017-12-22更：\n树莓派的 OpenCV 已经安装完成，环境配置问题已得到解决。下载源码编译安装，出现了报错，几经尝试也无法得到解决，所以使用了 apt-get 的方式安装，很顺利。\n最近实验室购买了一台 TURTLEBOT3（http://www.turtlebot.com）。TurtleBot是一个低成本的个人机器人工具包，具有开源软件。 TurtleBot 是由 Melonee Wise 和 Tully Foote 于 2010 年 11 月由 Willow Garage 创建的。 通过TurtleBot，可以建立一个机器人，可以在房子周围导航，观看 3D，并有足够的力量来创造令人兴奋的应用程序。我对 TURTLEBOT3 上搭载的树莓派很感兴趣，目前在学习 TURTLEBOT3 的图像处理相关方面内容。\nSBC : Respberry Pi 3\nMCU : OpenCR\n目前环境基本配置完成，Ubuntu16.04、ROS。\n ","permalink":"http://landodo.github.io/posts/20171103-raspberry-pi/","summary":"背景 我接触树莓派已有几个月的时间了，一直在使用树莓派作为一个平台学习去其他方面的知识，可以很确定的说，我的 Linux 学习之路，是树莓派带我入的门。但","title":"树莓派——初探"}]