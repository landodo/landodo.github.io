[{"content":"学习计算机的过程中，那些优秀的资料、网站。\n","permalink":"http://landodo.github.io/cs-zoo/","summary":"学习计算机的过程中，那些优秀的资料、网站。","title":"Cs Zoo"},{"content":"关于我\n","permalink":"http://landodo.github.io/about/","summary":"关于我","title":"About"},{"content":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介：\n 作者：Chufeng Tang（https://chufengt.github.io/） CVPR 2021 单位：清华大学计算机科学与技术系人工智能研究所 代码：https://github.com/tinyalpha/BPR  思考：\n（0）实例分割精度要想进一步提升，应该在哪里下功夫？\nTable 1 显示了边界修复所带来的性能提升具有巨大的潜力。\n（1）由于硬件的限制，分割时一般需要 Resize 或切 Patch。那么，应该怎么切patch呢？\n  随机切固定size的patch；\n  按网格切；\n  按实例所在位置切；\n  ✅先进行一个粗分割，然后以粗分割的边界为中心，提取Patch块。\n  （2）High-level information/Low-level information？\n高层次语义信息用于提供定位和粗略分割实例（localize and roughly segment objects）；低层次语义信息用于对于分割局部边界细节更为关键。在单一模型中权衡两者是非常困难的。\n（3）本文提出的BPR作为一种后处理方案，目前已有的后处理框架是怎么做的？\n设计一个边界感知的分割模型是一个分割任务的研究热点。主要的有两个方向：\n 集成一个额外的、专门的模块(分支)来处理边界； 基于现有分割模型的结果和后处理方案来细化边界  PolyTransform：将实例的轮廓转换为一组多边形顶点，应用基于Transformer的网络来预测顶点向对象边界的偏移；缺点：巨大的实例patch和复杂的Transformer架构。     注：PolyTransform(CVPR 2020)\n Abstract 由于特征图的空间分辨率较低，以及边界像素比例极低导致的不平衡问题，预测分割图的边界通常不精确。为了解决这个问题，本论文提出了一个概念上简单但有效的后处理改进框架，将实例分割模型的结果进行边界质量改进，称为 BPR。启发自看得更近，分割边界更好（looking closer to segment boundaries better），沿着预测的实例边界提取并细化一系列小边界 patch。通过更高分辨率的边界 patch 细化网络（BPR）。BPR 框架在 Cityspace 基准上，尤其是在边界感知指标上，比 Mask R-CNN 基线有显著改进。将 BPR 框架应用于“PolyTransform+SegFix”基线后，在 Cityspace 排行榜上取得了 SOTA。\n1 Introduction 分割所面临的最重要的问题之一是实例边界分割不精确（Figure 1 Left）。校正物体边界附近的误差像素可以大大提高 Mask 质量（Table 1），对于较小的物体，在一定的欧几里德距离(1px/2px/3px)范围内的像素可以获得较大的增益（AP 为9.4/14.2/17.8）。\n导致低质量边界分割的关键问题有两个：\n（1）特征图空间分辨率的降低使得物体边界周围的细节消失，预测的边界总是粗糙和不精确的（Figure 1,4）。\n（2）物体边界周围的像素只占整个图像的一小部分（不到1%），而且本质上很难分类。不平衡导致了优化偏向，低估了边界像素的重要性。\n许多研究试图改善边界质量，但上述问题仍未得到很好的解决。\n考虑到人工标注行为， 标注人员通常首先对给定图像中的每个对象进行定位和分类，然后在低分辨率下显式或隐式地分割一些粗略的实例掩码。之后，为了获得高质量的模板，注释者需要反复放大局部边界区域，探索更清晰、分辨率更高的边界分割。直观地说，需要高级语义来定位和粗略地分割对象，而低层细节（例如颜色一致性和对比度）对于分割局部边界区域更为关键。本文受人类分割行为的启发，提出了一种概念上简单而有效的后处理框架，通过crop-then-refine策略来提高边界质量。\n 首先提取沿预测实例边界的一系列小图像块； 拼接预测图像边界块后，将送入细化网络，细化粗边界。 精确的小块随后被重新组装成紧凑且高质量的实例分割图。  我们将提出的框架称为边界补丁精化（BPR，Boundary Patch Refinement）。\n由于我们只在对象边界附近裁剪，因此可以用比以往方法高得多的分辨率来处理 patch，从而可以更好地保留低层细节。同时，小patch中边界像素的比例自然增加，可以消除优化偏差。\n在本文的框架中，采用了目前流行的HRNet，它可以在整个网络中保持高分辨率的表示。提出的方法也是一种后处理方案，重点是对边界块进行细化，以提高分割图质量。\n3 Framework 提出框架的概述如 Figure 2 所示。作为一种后处理机制，无需对预分割模型本身进行任何修改或微调。\n3.1 边界块提取 Boundary Patch Extraction 首先需要确定掩码的哪一部分应该被细化。我们提出了一种有效的滑动窗式算法来提取沿预测实例边界的一系列patch。具体地说，我们密集地分配了一组方形边界框，框的中心区域应该覆盖边界像素，如图2(B)所示。得到的方框仍然包含较大的重叠和冗余，因此我们进一步应用非最大抑制(NMS)算法过滤出patch的子集(图2c)。经验表明，重叠越大，分割性能越好，但同时也存在计算量较大的问题。我们可以调整NMS阈值来控制重叠的数量，以实现更好的速度/精度折衷。除了图像补丁（image patches）外，我们还从给定的实例掩码中提取相应的二值掩码（corresponding binary mask patches）。调整拼接image patches、mask patches后，输入到 boundary patch refinement network 中。\n3.2. 边界块细化 Boundary Patch Refinement Mask Patch\n提供的位置和语义信息，使得精化网络无需从头开始学习实例级语义。取而代之的是，优化网络只需要学习如何定位决策边界周围的硬像素，并将它们推到正确的一侧。这一目标可以通过探索局部和高分辨率图像patch中提供的低级别图像特性（例如，颜色一致性和对比度）来实现。相邻的实例可能共享一个相同的边界patch，而学习目标则完全不同且不确定。如果不使用mask patch，则模型很难收敛（Figure 3）。\nBoundary Patch Refinement Network.\n采用HRNetV2，它可以在整个网络中保持高分辨率表示。通过适当增大输入大小，可以得到比以往方法更高分辨率的边界块。\nReassembling.\n将精分割的边界块替换先前的预测，对于那些边界框外没有细化的像素，预测是不变的。对于相邻patch重叠的区域，进行简单的求和取平均，应用0.5的阈值来区分前景和背景。\n3.3. Learning and Inference 基于从训练图像中提取的边界块来训练细化网络。在训练过程，只从预测掩码与地面真实掩码的交集大于0.5的实例中提取边界块，而在推理过程中保留所有预测实例。使用像素级的二进制交叉熵损失，用对应的地面真实掩模对模型输出进行监督。在训练时将NMS消除阈值固定为0.25，而在推理时根据速度要求采用不同的阈值。\n4 Experiments 数据集：Cityscape\n评价指标：COCO-style mask AP、F-score（AF：Average F-score）\n","permalink":"http://landodo.github.io/posts/boundary-patch-refinement/","summary":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介： 作者：Chufeng Tang（https://chufengt.github.io/）","title":"Boundary Patch Refinement"},{"content":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation\n  CVPR 2021\n  代码未开源\n  关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep Supervision、Self-Taught Deep Supervision、Mean-Taught Deep Supervision\n  Abstract 针对医学图像分割任务，本论文提出一种半-弱监督方法（semi-weakly supervised segmentation algorithm ），该方法建立在深度监督和师生模式的基础上，并且容易整合不同的监督信号。考虑如何将深监督整合到较低的层次，multi-label deep supervision 是本方法成果的关键。\n通过提出的训练机制（novel training regime for segmentation），可以灵活利用图像，这些图像要么有像素级标注、要么有边框标注、要么有全局标注、要么无标注。将标注需求降低 94.22%，比全监督的方法只降低 5% mIoU。在视网膜液分割数据集上进行实验验证。\n1 Introduction 像素级标注在医学图像语义分割任务中获取代价昂贵。因此，所面临的问题是最小化所需的注释工作，同时最大化模型的准确性。\n现有的方法：结合无标注的数据，快速获取不同质量的弱标签，如从 image-level 到 bounding box 标注。半监督、弱监督方法（semi- and weakly-supervised approaches）取得了令人信服的结果。\n针对像素级标注小数据集，本文提出通过一种新的深度监督，将训练信号整合到分割网络层中，通过丰富的未标注图像放大这些信号。然后利用 mean-teacher 分割模型推理出健壮的伪标签（pseudo-labels）\n本文的贡献：\n（1）对不同数量的训练样本和大量不同的监督类型进行了深入的研究，以实现语义分割；\n（2）引入了一种新的深度监督范式，适用于所提出的 Multi-label Deep Supervision 技术，在此基础上，引入了灵活的半-弱监督路径来集成未标记或弱标记的图像：新的自学深度监督方法（Self-Taught Deep Supervision）。\n（3）Mean-Taught Deep Supervision 增加了对扰动的不变性和健壮的伪标签生成，获得了接近完全监督基线的结果，而只使用了 5.78% 的标签。\n3 Proposed Approach semi-weakly supervised semantic segmentation\nMulti-label Deep Supervision\n对于半-弱监督的语义分割（semi-weakly supervised semantic segmentation），图像数据集为：\n 图像 $x_i$ 可以有不同的标注，如：  $m_i$：pixel-wise annotated mask $b_i$：bounding box $g_i$：image-level label no annotation（无标注）    对于每张图像 $x \\in \\mathcal{D}$，并不一定包含对于的分割图 $\\mathcal{M}$。即每张图像可能有 mask、bounding box、image-level label 或无标注。对于没有 mask 情况，称为半监督分割。\nSupervision integration\n在 encoder-decoder 架构中集成额外的输出（deep supervision）。这些输出在decoder 中的特征图 $f_0, \u0026hellip;, f_h$ 上进行操作。其中，$f_0$ 是 decoder 最里面的特征图，$f_h$ 是最外边的特征图。\n特征图 $f_i$ 的空间维度为 $H_i \\times W_i$：\n 实验中：$H_0 \u0026laquo; H_h$，$W_0 \u0026laquo; W_h$ 输出头 $k_i$ 基于 $f_i$ 计算预测结果：$k_i(f_i) \\in \\mathbb{R}^{C \\times H_i \\times W_i}$  深监督信号（Supervision signals）\n不同的监督模式需要不同的损失函数，在有像素级掩码（seg map）的情况下，训练语义分割模型最常见的目标是最小化交叉熵损失：\n3.2 Multi-label Deeply Supervised Nets 参数高效的多标签深度监督\n深度监督集成到分割网络中的方式存在问题。即，全尺寸的 ground-truth 和网络特征图的空间分辨率之间的空间维度不匹配。\n The problem we identify is the way deep supervision is commonly integrated into segmentation networks. Specifically, the challenge arises due to the mismatch in spatial dimensions between the full-scale ground-truth mask and the smaller spatial resolution within the network’s feature maps (Equation 5).\n 除了使用有损的最近邻插值外，大多数工作是迫使网络学习提升尺度（up-scaling）来解决 ground-truth mask 和 low-resolution spatial features 之间的不匹配。up-scaling 后，标准的输出头将特征图转换为与 mask 相同的 size。\n总结两个缺点：\n（1）网络必须学会 up-scaling，代价是额外的参数；\n（2）中间特征担负着对输出空间中复杂的分类信息和空间关系进行建模的重任，我们怀疑这些信息和空间关系在解码过程中是有用的，但可能只是作为更稳定梯度的跳跃连接。\n (2) intermediate features are burdened to model complex classification information and spatial relations in output space that we question to be useful in the decoding process, but presumably only serve as skip-connections for more stable gradients.\n 本论文建议将特征图中每个位置（x，y）的每个特征向量 $f^{:,x,y}$ 建模为其在输入图像中感受野的 patch-descriptors。因此，我们的目标是将 patch-descriptors 的感受野中包含的所有像素的语义信息植入模型。我们认为，这可以通过强制执行多标签损失来实现，其标签包含接受域中存在的所有语义类别。\n由此可见，我们可以简单地缩小 ground-truth mask 的比例，使之与特征图的大小相匹配，而不需要花费任何参数，并包含限制在感受野中的所有类别的标签，以保留语义信息。down-scaling 过程可以通过 max-pooling 实现。降尺度后的目标 $m_i^{*} \\in \\mathbb{R}^{C \\times H_i \\times W_i}$ 包含特征 $f_i^{:,x, y}$ （patch-descriptor，由特征感受野内的空间位置汇总而成）的多标签 Ground Truth。\n down-scaled multi-label ground-truths: $m$ feature maps: $f$  Self-taught deep supervision\n使用多标签深度监督（Multi-label Deep Supervision）生成伪标签，下采样平滑了噪声监督信号（Fig 2. Right）。\nSelf-taught Deep Supervision 通过如下方式为未标注样本生成 binary ground-truth tensor。\n 将未标注图像 $x_i$ 通过分割网络获得伪标签 $p_i$； 利用伪标签实施 Multi-label Deep Supervision； 最外层对真实 ground-truth 和 pseudo-label 设置单独的输出头；  一个输出头使用干净的标签计算交叉熵损失； 另一个输出头使用公式（9）和伪标签计算损失；   伪标签的推理生成，使用的是干净标签的输出头；  如果 image-level label $g_i$ 可以获取，可以进一步将生成的伪标签约束到包含在 $g_i$ 中的类。以类似的方式，相关联的边界框标签 $b_i$ 可以将伪标签约束为位于粗略区域内。这导致了弱标签图像的灵活集成，以提高伪标签质量。\nMean-taught deep supervision.\n通过(1)强制关于扰动的一致预测和(2)使用教师模型来生成更健壮的伪标签，该教师模型是先前迭代的所有模型的组合。\nMean-Teachers（即先前模型参数的指数移动平均数）\n通过使用学生模型和先前教师模型的移动平均值不断更新教师模型的参数来维护教师模型，可以获得更好的预测。因此，教师不会单独接受培训，而只是通过以下方式进行更新：\n $\\theta$ 表示模型参数 $t$ 表示训练迭代 $\\alpha$ 平滑参数  4 Experiments output-head for pseudo-labels 和 standard output-head 有什么区别？\n goround truth 不同，噪声伪标签用于平滑    过分割问题，是否可以加入一个分类的深监督来缓解？\n多尺度 concatenate 和这个方法的区别？\n","permalink":"http://landodo.github.io/posts/every-annotation-counts/","summary":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation CVPR 2021 代码未开源 关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep S","title":"Every Annotation Counts"},{"content":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。\n1. 聚类问题简介 聚类是针对给定的样本，依据它们的特征的相似度或距离，将其归并到若干“类”或“簇”中。其目的是通过得到的类或簇来发现数据的特点或对数据进行处理。聚类作为经典的无监督学习算法，在数据挖掘/机器学习等领域有着广泛地应用。\n两种最常用的聚类算法：层次聚类和 K 均值聚类。\n 层次聚类：将每个样本各自分到一个类；之后将相聚最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化类别结构。 K-Means 聚类：选择 K 个类别的中心，将样本逐个指派到与其最近的中心的类中，得到一个聚类结果；然后更新每个类的样本的均值，作为类的新的中心；重复以上步骤，直到收敛为止。  K-Means 被选为数据挖掘十大经典算法之一。\n图神经网络已经成为深度学习领域最热门的方向之一，那么，如何利用图神经网络强大的结构捕获能力来提升聚类算法的精度呢？深度聚类是聚类方法的一种，它采用深度神经网络来学习聚类友好表征。\n2. 相关综述 综述文献：[1][2]\n深度聚类算法（Deep Clustering Algorithm）可以分解为三个基本组成部分：\n 深度神经网络 网络损失 $L_n$ 聚类损失 $L_c$  3. 论文精读 论文名称：Attributed Graph Clustering: A Deep Attentional Embedding Approach [3]\n发表期刊：International Joint Conference on Artificial Intelligence (IJCAI-19)\n论文地址：https://arxiv.org/abs/1906.06532\n摘要 最近的研究大多集中在使用深度学习方法来学习一个紧凑的图形嵌入（embedding），在此基础上应用经典的聚类方法（如 K-means）来完成聚类任务。这种两阶段的方法通常无法取得更好的结果，因为其图嵌入不是以目标为导向的，即此深度学习方法并不是为聚类任务而设计的。\n本篇论文提出一种以目标为导向的深度学习方法：Deep Attentional Embedded Graph Clustering (DAEGC)。这种方法包含三个主要核心点：\n（1）注意力机制的图自编码器（Graph Attentional Autoencoder）\n（2）自训练的图聚类（Self-optimizing Embedding）\n（3）自训练过程与图嵌入共同学习和优化（Joint Embedding and Clustering Optimization）\nDAEGC 在 Cora、Citeseer、Pubmed 数据集上都取得了最好的聚类结果。\nIntroduction 对于图聚类问题，其中的关键是如何捕捉结构关系和节点内容信息。很多近期的研究通过深度学习方法学习到节点的 Embedding，再利用简单的聚类算法（如 K-Means）进行聚类。\n很显然，这是一种两个阶段的方法（非目标为导向），其存在着如下的缺点：学习到的 Embedding 可能不是最适合随后的图聚类任务，并且图聚类任务对图的嵌入学习没有帮助。\n传统的以目标为导向的方法大多针对的是分类任务（监督学习，如图卷积实现分类）。\n本论文提出了一种以目标为导向的图注意力自动编码器的图聚类框架。Figure 1 显示了其与两阶段方法的不同，模型学习嵌入并同时在一个统一的框架内进行聚类，从而获得更好的聚类性能。\nModel 如 Figure 2 所示模型主要由两个模块构成：\n（1）Graph attentional autoencoder\n自动编码器将节点属性值和图结构作为输入，并通过最小化重建损失来学习潜在的嵌入。\n（2）Self-training clustering\n自训练模块根据学习到的表征进行聚类，反过来，根据当前的聚类结果来操作潜在的表征。\n在一个统一的框架内学习图的嵌入和聚类，这样每个部分都能使对方受益。接下来对模型的细节进行分析。\n（1）Graph Attentional Autoencoder Graph Attention Encoder\n使用一个图注意力网络（GAT）的变体作为 Graph Encoder，其核心是通过关注其邻居来学习每个节点的隐藏表征，将节点特征与潜在表征中的图结构相结合。注意力机制对邻居的表示给予不同的权重。\n $z^{l+1}_{i}$​：节点 $i$​ 的输出表征（新特征）； $N_i$​：节点 $i$​ 的所有邻居节点； $\\alpha_{ij}$​：注意力系数，衡量节点 $j$​ 对节点 $i$​​ 的重要程度； $\\sigma$​​：非线性激活函数。  注意力系数 $\\alpha_{ij}$​ 可以表示为一个单层前馈神经网络，$x_i$ 和 $x_j$ 表示节点 $i$ 和 $j$ 的特征向量。\n $c_{ij}$​ 是一个标量，衡量节点 $j$​ 对节点 $i$​ 的重要程度。  图注意网络（GAT）只考虑了一阶邻居， 由于图具有复杂的结构关系，本篇论文的编码器中利用高阶邻居，通过考虑图中的 t 阶邻居节点来获得一个接近矩阵：\n $B$ 是一个转移矩阵（非负，各行元素之和等于 1），如果节点 $i$ 和 $j$ 之间存在边，则 $B_{ij} = 1/d_i$，否则 $B_{ij} = 0$。$d_{i}$ 为节点 $i$​​ 的度； $M_{ij}$​ 表示节点 $i$ 和节点 $j$ 之间的拓扑相关性（$t$​ 阶邻居）； $N_i$ 指 $M$ 中 $i$ 的邻居节点，如果 $M_{ij} \u0026gt; 0$，则表示 $j$ 是 $i$​​​ 的邻居节点； $t$：可以针对不同的数据集灵活地选择t，以平衡模型的精度和效率。  注意力系数通常在所有邻域 $j∈N_i$​​ 中用一个 softmax 函数进行归一化，以使它们易于在各节点间进行比较。\n加上拓扑权重 M 和激活函数 δ（LeakyReLU），注意力系数可以表示为：\n$x_i = z^{0}_{i}$​ 作为输入，堆叠两个图注意力层：\n图注意力编码器将结构和节点特征编码成一个隐藏的表示，得到：$z_i = z^{(2)}_{i}$。\nInner Product Decoder\n解码器（Decoder）可以进行如下分类：重建图结构、重建节点特征属性、两种都重建。由于公式（7）获取到的潜伏嵌入（latent embedding）已经包含了内容和结构信息，因此本篇论文选择采用一个简单的内积解码器来预测节点之间的联系：\n重建损失 Reconstruction Loss\n通过衡量 $A$ 和 $\\hat{A}$ 之间的差异性来最小化重构损失：\n（2）Self-optimizing Embedding 图聚类任务是无监督的，因此在训练期间无法获得关于所学嵌入是否得到很好优化的反馈。即 GAE 所学习到的节点表示只是为了更好的重构网络结构，和聚类并没有直接联系。针对此问题，本篇论文提出了 一种自优化的嵌入算法作为解决方案，对 GAE 所学习到的 embedding 进行约束和整合，使其更适合于聚类任务。\n除了优化重建误差，还将隐藏嵌入（hidden embedding）输入到一个自优化的聚类模块中，该模块最小化了以下目标：\n $q_{iu}$ ：衡量节点的 embedding $z_{i}$​ 和聚类中心 embedding $\\mu_u$ 的相似度  使用学生分布（Student\u0026rsquo;s $t$​​-distribution）来衡量，以处理不同规模的集群。（11）式中，聚类中心 embedding 为 $\\mu_u$​​​，则节点 $i$​​​​ 属于某个类别的概率为 $q_{i u}$:\n T 分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值。\n $q_{i u}$ 可以被看作是每个节点的软聚类分配分布。为了引入聚类信息来实现聚类导向的节点表示，我们需要迫使每个节点与相应的聚类中心更近一些，以实现所谓的类内距离最小，类间距离最大（二次方后，分布会变得更加尖锐，也更置信）。定义目标分布 $p_{iu}$​ ：\n 类别 $k$  目标分布 P 将 Q 提高到二次方，以强调这些“自信的分配”的作用。然后，聚类损失迫使当前分布 Q 接近目标分布 P，以便将这些“有信心的分配”设置为软标签（“works as ground-truth labels”），监督 Q 的嵌入学习。\n将聚类损失降到最低，以帮助自动编码器利用嵌入物自身的特性操纵嵌入空间。\n（3）Joint Embedding and Clustering Optimization 共同优化自动编码器嵌入和聚类学习，总目标函数定义为：\n  $L_r$：Reconstruction loss\n  $L_c$：Clustering loss\n  $\\gamma \u0026gt;= 0$：控制两者的平衡\n  可以直接从最后一个优化的 Q 中获得聚类结果，即对节点 $i$ 的所处的簇为：\nDeep Attentional Embedded Graph Clustering Algorithm\nExperiments 三个数据集\n总结  DAEGC: an unsupervised deep attentional embedding algorithm; 在一个统一的框架内，联合进行图聚类和学习图嵌入； 学习到的图嵌入整合了结构和内容信息，并专门用于聚类任务； 针对聚类这个无监督学习任务，提出了一个自训练的聚类组件，从“置信”的分配中生成软标签，以监督嵌入的更新； 聚类损失和自动编码器重建损失被联合优化，以同时获得图嵌入和图聚类的结果； 将实验结果与各种最先进的算法进行比较，验证了 DAEGC 的图聚类性能。  References [1] Aljalbout, E., Golkov, V., Siddiqui, Y., \u0026amp; Cremers, D. (2018). Clustering with Deep Learning: Taxonomy and New Methods. ArXiv, abs/1801.07648.\n[2] Erxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing Cui, and Jun Long. A Survey of Clustering with Deep Learning: From the Perspective of Network Architecture. DOI: 10.1109/ACCESS.2018.2855437, IEEE Access, vol. 6, pp. 39501-39514, 2018.\n[3] Wang, C., Pan, S., Hu, R., Long, G., Jiang, J., \u0026amp; Zhang, C. (2019). Attributed Graph Clustering: A Deep Attentional Embedding Approach. IJCAI.\n[4] https://deepnotes.io/deep-clustering\n Can deep neural networks learn to do clustering? Introduction, survey and discussion of recent works on deep clustering algorithms.\n [5] Papers List (1.2k Stars) https://github.com/zhoushengisnoob/DeepClustering\n Deep Clustering: methods and implements\n [6] 图神经网络时代的深度聚类：https://zhuanlan.zhihu.com/p/114452245\n","permalink":"http://landodo.github.io/posts/deep-clustering-notes/","summary":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。 1. 聚类问题简介 聚类是针对给定的样本，依据它们的","title":"深度聚类 Deep Clustering"},{"content":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。\n Hugo 默认是使用 TOML，现在将此更改为更易阅读的 YAML。 Hugo 需要在源目录查找一个 config.toml 的配置文件。如果这个文件不存在，将会查找 config.yaml，然后是 config.json\n 添加站内搜索 Search Page 全局文章内容搜索，从关键字快速定位到文章。\nHogu 文档：https://adityatelange.github.io/hugo-PaperMod/\nPaperMod uses Fuse.js Basic for seach functionality\nAdd the following to site config, config.yml\noutputs:  home:  - HTML  - RSS  - JSON # is necessary Create a page with search.md in content directory with following content\n--- title: \u0026#34;Search\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # is necessary # url: \u0026#34;/archive\u0026#34; # description: \u0026#34;Description for Search\u0026#34; summary: \u0026#34;search\u0026#34; --- To hide a particular page from being searched, add it in post’s fron’t matter\n--- searchHidden: true copy\nex: [search.md]\n添加 Archives 功能 这是自动的，只需要添加一个按钮，其他的就不用管了。\nhttps://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n增加公式支持 此功能会拖慢网站的加载速度，这个以后再说！些许的加载时间能换取良好的阅读体验，目前来看是值得的。我的论文阅读笔记里充斥了大量的 LaTeX 公式。\n公式支持基于 MathJax。\nhttps://mertbakir.gitlab.io/hugo/math-typesetting-in-hugo/\n行内公式：\n给定一组训练样本 $\\mathcal{D}$，TumorCP 有 $(1 - p_{cp})$​​​ 的概率不执行任何操作；有 $p_{cp}$​ 的概率从 $\\mathcal{D}$ 中采样出一个对图像 $(x_{src}, x_{tgt}) \\sim \\mathcal{D}$​​，并执行一次 “Copy-Paste”。\n令 $\\mathcal{O}{src}$ 为图像 $x{src}$ 上肿瘤集合，$\\mathcal{V}{tgt}$ 为 $x{tgt}$ 上的器官的体积坐标集合，$\\mathcal{T}$ 是一组随机数据转换，每个转换都有一个称为 $p_{trans}$​ 的概率参数。\n一次 “Copy-Paste” 流程：\n TumorCP 首先采样一个肿瘤 $o \\sim \\mathcal{O}{src}$​、一组数据转换 $\\tau \\sim \\mathcal{T}$​ 和一个目标位置 $v \\sim \\mathcal{V}{tgt}$； 然后将 $τ(o)$ 以 $v$​ 为中心，取代原始数据和标注。  行间公式：\n$$\\sqrt{x} + \\sqrt{x^{2}+\\sqrt{y}} = \\sqrt[3]{k_{i}} - \\frac{x}{m}$$\n测试 Emoji 🧡💥💢💌💝🕎☪\n测试代码 import sys from PyQt5 import QtCore, QtGui, QtWidgets, uic from PyQt5.QtCore import Qt, QEvent import random  from PyQt5.QtGui import QPixmap from PyQt5.QtWidgets import QAction, QFileDialog   class Canvas(QtWidgets.QLabel):  def __init__(self, parent=None):  super().__init__(parent)  self.background = QPixmap(200, 200)  self.background.fill(Qt.yellow)  # self.clear(Qt.yellow)  self.last_x, self.last_y = None, None  self.pen_color = QtGui.QColor(\u0026#39;#000000\u0026#39;)  self.setPixmap(self.background)   def mouseReleaseEvent(self, e):  self.last_x = None  self.last_y = None 图片的显示 这里需要将图片拷贝的 public 对应的文章目录下，使用原本的相对路径。不需要将文件夹拷贝到 content/posts/ ，这个文件夹下至保存 Markdown 文件。\n使用 hugo 生成文章的 HTML 文件后，每一篇文章都会以 Markdown 文件中的 title 生成一个文件夹。如本篇文章的文件夹名称为 first-post （全小写）。\n在访问时，使用的是如下的链接：\nhttps://landodo.github.io/first-post/0000465981_017.jpg 如果图片实现依旧不正常，使用 F12 进行调试，快速定位问题所在。\n","permalink":"http://landodo.github.io/posts/first-post/","summary":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。 Hugo 默认是使用 TOML，现在将此更改为更易阅","title":"First Post"}]