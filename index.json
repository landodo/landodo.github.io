[{"content":"学习计算机的过程中，那些优秀的资料、网站。\n","permalink":"http://landodo.github.io/cs-zoo/","summary":"学习计算机的过程中，那些优秀的资料、网站。","title":"Cs Zoo"},{"content":"关于我\n","permalink":"http://landodo.github.io/about/","summary":"关于我","title":"About"},{"content":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介：\n 作者：Chufeng Tang（https://chufengt.github.io/） CVPR 2021 单位：清华大学计算机科学与技术系人工智能研究所 代码：https://github.com/tinyalpha/BPR  思考：\n（0）实例分割精度要想进一步提升，应该在哪里下功夫？\nTable 1 显示了边界修复所带来的性能提升具有巨大的潜力。\n（1）由于硬件的限制，分割时一般需要 Resize 或切 Patch。那么，应该怎么切patch呢？\n  随机切固定size的patch；\n  按网格切；\n  按实例所在位置切；\n  ✅先进行一个粗分割，然后以粗分割的边界为中心，提取Patch块。\n  （2）High-level information/Low-level information？\n高层次语义信息用于提供定位和粗略分割实例（localize and roughly segment objects）；低层次语义信息用于对于分割局部边界细节更为关键。在单一模型中权衡两者是非常困难的。\n（3）本文提出的BPR作为一种后处理方案，目前已有的后处理框架是怎么做的？\n设计一个边界感知的分割模型是一个分割任务的研究热点。主要的有两个方向：\n 集成一个额外的、专门的模块(分支)来处理边界； 基于现有分割模型的结果和后处理方案来细化边界  PolyTransform：将实例的轮廓转换为一组多边形顶点，应用基于Transformer的网络来预测顶点向对象边界的偏移；缺点：巨大的实例patch和复杂的Transformer架构。     注：PolyTransform(CVPR 2020)\n Abstract 由于特征图的空间分辨率较低，以及边界像素比例极低导致的不平衡问题，预测分割图的边界通常不精确。为了解决这个问题，本论文提出了一个概念上简单但有效的后处理改进框架，将实例分割模型的结果进行边界质量改进，称为 BPR。启发自看得更近，分割边界更好（looking closer to segment boundaries better），沿着预测的实例边界提取并细化一系列小边界 patch。通过更高分辨率的边界 patch 细化网络（BPR）。BPR 框架在 Cityspace 基准上，尤其是在边界感知指标上，比 Mask R-CNN 基线有显著改进。将 BPR 框架应用于“PolyTransform+SegFix”基线后，在 Cityspace 排行榜上取得了 SOTA。\n1 Introduction 分割所面临的最重要的问题之一是实例边界分割不精确（Figure 1 Left）。校正物体边界附近的误差像素可以大大提高 Mask 质量（Table 1），对于较小的物体，在一定的欧几里德距离(1px/2px/3px)范围内的像素可以获得较大的增益（AP 为9.4/14.2/17.8）。\n导致低质量边界分割的关键问题有两个：\n（1）特征图空间分辨率的降低使得物体边界周围的细节消失，预测的边界总是粗糙和不精确的（Figure 1,4）。\n（2）物体边界周围的像素只占整个图像的一小部分（不到1%），而且本质上很难分类。不平衡导致了优化偏向，低估了边界像素的重要性。\n许多研究试图改善边界质量，但上述问题仍未得到很好的解决。\n考虑到人工标注行为， 标注人员通常首先对给定图像中的每个对象进行定位和分类，然后在低分辨率下显式或隐式地分割一些粗略的实例掩码。之后，为了获得高质量的模板，注释者需要反复放大局部边界区域，探索更清晰、分辨率更高的边界分割。直观地说，需要高级语义来定位和粗略地分割对象，而低层细节（例如颜色一致性和对比度）对于分割局部边界区域更为关键。本文受人类分割行为的启发，提出了一种概念上简单而有效的后处理框架，通过crop-then-refine策略来提高边界质量。\n 首先提取沿预测实例边界的一系列小图像块； 拼接预测图像边界块后，将送入细化网络，细化粗边界。 精确的小块随后被重新组装成紧凑且高质量的实例分割图。  我们将提出的框架称为边界补丁精化（BPR，Boundary Patch Refinement）。\n由于我们只在对象边界附近裁剪，因此可以用比以往方法高得多的分辨率来处理 patch，从而可以更好地保留低层细节。同时，小patch中边界像素的比例自然增加，可以消除优化偏差。\n在本文的框架中，采用了目前流行的HRNet，它可以在整个网络中保持高分辨率的表示。提出的方法也是一种后处理方案，重点是对边界块进行细化，以提高分割图质量。\n3 Framework 提出框架的概述如 Figure 2 所示。作为一种后处理机制，无需对预分割模型本身进行任何修改或微调。\n3.1 边界块提取 Boundary Patch Extraction 首先需要确定掩码的哪一部分应该被细化。我们提出了一种有效的滑动窗式算法来提取沿预测实例边界的一系列patch。具体地说，我们密集地分配了一组方形边界框，框的中心区域应该覆盖边界像素，如图2(B)所示。得到的方框仍然包含较大的重叠和冗余，因此我们进一步应用非最大抑制(NMS)算法过滤出patch的子集(图2c)。经验表明，重叠越大，分割性能越好，但同时也存在计算量较大的问题。我们可以调整NMS阈值来控制重叠的数量，以实现更好的速度/精度折衷。除了图像补丁（image patches）外，我们还从给定的实例掩码中提取相应的二值掩码（corresponding binary mask patches）。调整拼接image patches、mask patches后，输入到 boundary patch refinement network 中。\n3.2. 边界块细化 Boundary Patch Refinement Mask Patch\n提供的位置和语义信息，使得精化网络无需从头开始学习实例级语义。取而代之的是，优化网络只需要学习如何定位决策边界周围的硬像素，并将它们推到正确的一侧。这一目标可以通过探索局部和高分辨率图像patch中提供的低级别图像特性（例如，颜色一致性和对比度）来实现。相邻的实例可能共享一个相同的边界patch，而学习目标则完全不同且不确定。如果不使用mask patch，则模型很难收敛（Figure 3）。\nBoundary Patch Refinement Network.\n采用HRNetV2，它可以在整个网络中保持高分辨率表示。通过适当增大输入大小，可以得到比以往方法更高分辨率的边界块。\nReassembling.\n将精分割的边界块替换先前的预测，对于那些边界框外没有细化的像素，预测是不变的。对于相邻patch重叠的区域，进行简单的求和取平均，应用0.5的阈值来区分前景和背景。\n3.3. Learning and Inference 基于从训练图像中提取的边界块来训练细化网络。在训练过程，只从预测掩码与地面真实掩码的交集大于0.5的实例中提取边界块，而在推理过程中保留所有预测实例。使用像素级的二进制交叉熵损失，用对应的地面真实掩模对模型输出进行监督。在训练时将NMS消除阈值固定为0.25，而在推理时根据速度要求采用不同的阈值。\n4 Experiments 数据集：Cityscape\n评价指标：COCO-style mask AP、F-score（AF：Average F-score）\n","permalink":"http://landodo.github.io/posts/boundary-patch-refinement/","summary":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介： 作者：Chufeng Tang（https://chufengt.github.io/）","title":"Boundary Patch Refinement"},{"content":"SETR  Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020).\n  Zheng, Sixiao, et al. \u0026ldquo;Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n 论文简介 SETR：用 Transformer 从 Sequence-to-Sequence 的角度重新思考语义分割。\n最早于 2020 年 12 月发布于 arXiv。\n会议：2021 CVPR\n一作单位：Fudan University \u0026amp; Tencent-Youtu Lab\nAbstract 语义分割大多使用空洞卷积和注意力模块来扩大感受野，提升对全局上下文的建模能力。本篇论文基于 ViT，使用一个纯 Transformer（没有卷积和分辨率降低）来将图片编码为一系列的 patch。通过在 Transformer 的每一层建模全局上下文，该编码器结合一个简单的解码器可以形成强大的分割模型，命名为 SEgmentation TRansformer(SETR)。\n实验结果，SETR 在 ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) 上取得了 SOTA。\n1 Introduction 语义分割一直被基于编码器-解码器（encoder-decoder）的架构主导，其中编码器可以说是最重要的模型组件。基于 CNN 的编码器通过卷积层的堆叠（考虑到计算成本，特征图的分辨率逐渐降低），感受野逐渐增加，学到更抽象/语义视觉信息（ abstract/semantic visual concepts）。但由于感受野的限制，学习对语义分割至关重要的远程依赖仍然具有挑战性（long-range dependency information）。\n克服如上限制提出的方法如下：\n 修改卷积操作（卷积核大小、空洞卷积、特征金字塔） 注意力机制  这些工作大多没有改变 FCN model 的本质：编码器对输入的空间分辨率进行下采样，学习到有助于区分语义类别的低分辨率特征图。解码器上采样特征图为全分辨率的分割图。\n本篇论文提出使用纯 Transformer 来替换 CNN 的编码器，称为 SEgmentation TRans- former(SETR)。这个基于 Transformer 的编码器将输入的图像视为一系列的图像块（image patchs），这些块通过可学习的 embedding 表示。然后利用全局自注意模型对序列进行变换，实现区分特征表征学习（discriminative feature representation learning.）。\n具体地，首先将图像分解成一个固定大小的网格，形成一系列地 patches。然后通过线性层得到每个图像块的特征嵌入向量（feature embedding vector），这些向量作为 Transformer 的输入。之后解码器将 Transformer 的输出恢复成原始分辨率。\n在编码器的每一层都没有降低空间分辨率，而是进行全局上下文建模，从而为语义分割问题提供了一个全新的视角。\nViT 在分类任务上有效性证明了图像特征不一定需要从局部逐渐学习到全局上下文（CNN 结构不是必要的）。SETR 将分类拓展到了分割上，提供了一个模型设计的新视角，并且在一些 benchmark 数据集上取得了 SOTA。\n主要贡献总结如下：\n（1）从 sequence-to-sequence 学习的角度，针对图像语义分割问题，提供了一个 FCN-based 模型的替代方案；\n（2）利用 Transformer 架构实现编码器；\n（3）为了验证 self-attention feature 的表现，介绍了 3 中不同复杂度的解码器设计。\n本篇论文提出的 SETR 模型，state of the art on ADE20K(50.28%), Pascal Context (55.83%)，在 Cityscapes 上取得了有竞争力的结果。\n3 Method 3.1 FCN-based semantic segmentation 回顾 FCN\n原始输入为 $H \\times W \\times 3$，后续层的输入为 $h \\times w \\times d$，$d$ 为特征图的通道数。感受野随着层的深度线性增加（取决于卷积核的大小）。FCN 中拥有大感受野的高层才可以建模远程依赖（long-range dependencies）。一味的加深层数以增加感受野带来的收益将迅速减少，上下文信息建模的感受野有限是 FCN 系列架构固有的局限性。\n将 FCN 和注意机制结合起来是一种更有效的学习远距离语境信息的策略。但是二次复杂性（特征图的像素个数）这些注意力方法通常作用于输入较少的高层。\n3.2. Segmentation transformers (SETR) Image to sequence\nSRTR 遵循与 NLP 相同的输入输出结构。\n","permalink":"http://landodo.github.io/posts/20211208-setr/","summary":"SETR Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020). Zheng, Sixiao, et al. \u0026ldquo;Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. 论文简介 SETR：用 Transformer 从 Sequence-to-Sequence 的角度重新思考语义分割","title":"SETR: Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers"},{"content":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation\n  CVPR 2021\n  代码未开源\n  关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep Supervision、Self-Taught Deep Supervision、Mean-Taught Deep Supervision\n  Abstract 针对医学图像分割任务，本论文提出一种半-弱监督方法（semi-weakly supervised segmentation algorithm ），该方法建立在深度监督和师生模式的基础上，并且容易整合不同的监督信号。考虑如何将深监督整合到较低的层次，multi-label deep supervision 是本方法成果的关键。\n通过提出的训练机制（novel training regime for segmentation），可以灵活利用图像，这些图像要么有像素级标注、要么有边框标注、要么有全局标注、要么无标注。将标注需求降低 94.22%，比全监督的方法只降低 5% mIoU。在视网膜液分割数据集上进行实验验证。\n1 Introduction 像素级标注在医学图像语义分割任务中获取代价昂贵。因此，所面临的问题是最小化所需的注释工作，同时最大化模型的准确性。\n现有的方法：结合无标注的数据，快速获取不同质量的弱标签，如从 image-level 到 bounding box 标注。半监督、弱监督方法（semi- and weakly-supervised approaches）取得了令人信服的结果。\n针对像素级标注小数据集，本文提出通过一种新的深度监督，将训练信号整合到分割网络层中，通过丰富的未标注图像放大这些信号。然后利用 mean-teacher 分割模型推理出健壮的伪标签（pseudo-labels）\n本文的贡献：\n（1）对不同数量的训练样本和大量不同的监督类型进行了深入的研究，以实现语义分割；\n（2）引入了一种新的深度监督范式，适用于所提出的 Multi-label Deep Supervision 技术，在此基础上，引入了灵活的半-弱监督路径来集成未标记或弱标记的图像：新的自学深度监督方法（Self-Taught Deep Supervision）。\n（3）Mean-Taught Deep Supervision 增加了对扰动的不变性和健壮的伪标签生成，获得了接近完全监督基线的结果，而只使用了 5.78% 的标签。\n3 Proposed Approach semi-weakly supervised semantic segmentation\nMulti-label Deep Supervision\n对于半-弱监督的语义分割（semi-weakly supervised semantic segmentation），图像数据集为：\n 图像 $x_i$ 可以有不同的标注，如：  $m_i$：pixel-wise annotated mask $b_i$：bounding box $g_i$：image-level label no annotation（无标注）    对于每张图像 $x \\in \\mathcal{D}$，并不一定包含对于的分割图 $\\mathcal{M}$。即每张图像可能有 mask、bounding box、image-level label 或无标注。对于没有 mask 情况，称为半监督分割。\nSupervision integration\n在 encoder-decoder 架构中集成额外的输出（deep supervision）。这些输出在decoder 中的特征图 $f_0, \u0026hellip;, f_h$ 上进行操作。其中，$f_0$ 是 decoder 最里面的特征图，$f_h$ 是最外边的特征图。\n特征图 $f_i$ 的空间维度为 $H_i \\times W_i$：\n 实验中：$H_0 \u0026laquo; H_h$，$W_0 \u0026laquo; W_h$ 输出头 $k_i$ 基于 $f_i$ 计算预测结果：$k_i(f_i) \\in \\mathbb{R}^{C \\times H_i \\times W_i}$  深监督信号（Supervision signals）\n不同的监督模式需要不同的损失函数，在有像素级掩码（seg map）的情况下，训练语义分割模型最常见的目标是最小化交叉熵损失：\n3.2 Multi-label Deeply Supervised Nets 参数高效的多标签深度监督\n深度监督集成到分割网络中的方式存在问题。即，全尺寸的 ground-truth 和网络特征图的空间分辨率之间的空间维度不匹配。\n The problem we identify is the way deep supervision is commonly integrated into segmentation networks. Specifically, the challenge arises due to the mismatch in spatial dimensions between the full-scale ground-truth mask and the smaller spatial resolution within the network’s feature maps (Equation 5).\n 除了使用有损的最近邻插值外，大多数工作是迫使网络学习提升尺度（up-scaling）来解决 ground-truth mask 和 low-resolution spatial features 之间的不匹配。up-scaling 后，标准的输出头将特征图转换为与 mask 相同的 size。\n总结两个缺点：\n（1）网络必须学会 up-scaling，代价是额外的参数；\n（2）中间特征担负着对输出空间中复杂的分类信息和空间关系进行建模的重任，我们怀疑这些信息和空间关系在解码过程中是有用的，但可能只是作为更稳定梯度的跳跃连接。\n (2) intermediate features are burdened to model complex classification information and spatial relations in output space that we question to be useful in the decoding process, but presumably only serve as skip-connections for more stable gradients.\n 本论文建议将特征图中每个位置（x，y）的每个特征向量 $f^{:,x,y}$ 建模为其在输入图像中感受野的 patch-descriptors。因此，我们的目标是将 patch-descriptors 的感受野中包含的所有像素的语义信息植入模型。我们认为，这可以通过强制执行多标签损失来实现，其标签包含接受域中存在的所有语义类别。\n由此可见，我们可以简单地缩小 ground-truth mask 的比例，使之与特征图的大小相匹配，而不需要花费任何参数，并包含限制在感受野中的所有类别的标签，以保留语义信息。down-scaling 过程可以通过 max-pooling 实现。降尺度后的目标 $m_i^{*} \\in \\mathbb{R}^{C \\times H_i \\times W_i}$ 包含特征 $f_i^{:,x, y}$ （patch-descriptor，由特征感受野内的空间位置汇总而成）的多标签 Ground Truth。\n down-scaled multi-label ground-truths: $m$ feature maps: $f$  Self-taught deep supervision\n使用多标签深度监督（Multi-label Deep Supervision）生成伪标签，下采样平滑了噪声监督信号（Fig 2. Right）。\nSelf-taught Deep Supervision 通过如下方式为未标注样本生成 binary ground-truth tensor。\n 将未标注图像 $x_i$ 通过分割网络获得伪标签 $p_i$； 利用伪标签实施 Multi-label Deep Supervision； 最外层对真实 ground-truth 和 pseudo-label 设置单独的输出头；  一个输出头使用干净的标签计算交叉熵损失； 另一个输出头使用公式（9）和伪标签计算损失；   伪标签的推理生成，使用的是干净标签的输出头；  如果 image-level label $g_i$ 可以获取，可以进一步将生成的伪标签约束到包含在 $g_i$ 中的类。以类似的方式，相关联的边界框标签 $b_i$ 可以将伪标签约束为位于粗略区域内。这导致了弱标签图像的灵活集成，以提高伪标签质量。\nMean-taught deep supervision.\n通过(1)强制关于扰动的一致预测和(2)使用教师模型来生成更健壮的伪标签，该教师模型是先前迭代的所有模型的组合。\nMean-Teachers（即先前模型参数的指数移动平均数）\n通过使用学生模型和先前教师模型的移动平均值不断更新教师模型的参数来维护教师模型，可以获得更好的预测。因此，教师不会单独接受培训，而只是通过以下方式进行更新：\n $\\theta$ 表示模型参数 $t$ 表示训练迭代 $\\alpha$ 平滑参数  4 Experiments output-head for pseudo-labels 和 standard output-head 有什么区别？\n goround truth 不同，噪声伪标签用于平滑    过分割问题，是否可以加入一个分类的深监督来缓解？\n多尺度 concatenate 和这个方法的区别？\n","permalink":"http://landodo.github.io/posts/every-annotation-counts/","summary":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation CVPR 2021 代码未开源 关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep S","title":"Every Annotation Counts"},{"content":"论文简介 论文名称：TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation\n arXiv：https://arxiv.org/pdf/2107.09843.pdf 单位：（美国）加利福尼亚大学、中科院计算所 会议：MICCAI 2021 source code：https://github.com/YaoZhang93/TumorCP  Motivation：Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation(CVPR 2021) https://arxiv.org/abs/2012.07177，针对自然图像\nAbstract 启发自最近提出的 “Copy-Paste” 数据增强方法，本篇论文提出 TumorCP：一种简单而有效的 object-level 数据增强方法，适用于肿瘤分割。\nTumorCP 在肿瘤分割任务上， Dice 以 7.12% 的显著幅度超过了 baseline。TumorCP + image-level 增强方法，比目前的 SOTA 方法 Dice 提升了 2.32%。\n1 Introduction 高质量的带注释的数据集需要大量的工作和领域知识，这在医学领域尤为显著。为了提高数据高效学习，从不同的角度提出了几种成功的方法：\n leveraging unlabeled data for semi-supervised self-training self-supervised pre-training distilling priors from data 通过不同形态的解剖成像生成新的数据 利用适当的数据增强方法增加数据多样性  不同于复杂的 GAN，“Copy-Paste” 是一种简洁的增强方法，其在自然图像的实例分割中取得了新的突破（CVPR 2021）。但在医学图像领域中，这种方法在很大程度上是未被探索的，因为 “Copy-Paste” 中往往忽略上下文信息，这在直觉上是不可取的！\n“Copy-Paste” 增强方法通过简单地将标注实例粘贴到新的背景图像上作为额外的训练数据，从而避免了从表示空间到像素空间的昂贵生成过程。\n Copy-Paste augmentation avoids costly generation processes from representation space to pixel space by simply pasting the labeled instance onto new background images as additional training data.\n 此外，由于在 “Copy-Paste” 中上下文信息往往被忽略，因此它对医疗任务的有效性仍有待观察。例如，在肿瘤分割中，周围视觉线索，即上下文环境（Context），对肿瘤出现的重要性；医学图像固有的解剖结构使得肿瘤分割离不开上下文。\n本篇论文旨在通过检验 “Copy-Paste” 增强技术在肿瘤分割中的有效性，来填补对上下文在医学领域中作用的理解的空白。\nTumorCP 从源图像中随机选取一个肿瘤，经过一系列空间增强、对比度增强、模糊增强后，将其粘贴到目标图像中的器官上。 TumorCP randomly chooses a tumor from a source image and paste it onto the organs in the target image after a series of spatial, contrast, and blurring augmentations.\n 使用肾肿瘤分割数据集（KiTS19 dataset），nnUNet 来评估此方法。\n2 Method TumorCP 是一种用于肿瘤分割的在线随机增强过程，它的实现是简单和直接的。\n给定一组训练样本 $\\mathcal{D}$，TumorCP 有 $(1 - p_{cp})$ 的概率不执行任何操作；有 $p_{cp}$ 的概率从 $\\mathcal{D}$ 中采样出一个对图像 $(x_{src}, x_{tgt}) \\sim \\mathcal{D}$，并执行一次 “Copy-Paste”。\n令 $\\mathcal{O}{src}$ 为图像 $x{src}$ 上肿瘤集合，$\\mathcal{V}{tgt}$ 为 $x{tgt}$ 上的器官的体积坐标集合，$\\mathcal{T}$ 是一组随机数据转换，每个转换都有一个称为 $p_{trans}$ 的概率参数。\n一次 “Copy-Paste” 流程：\n TumorCP 首先采样一个肿瘤 $o \\sim \\mathcal{O}{src}$、一组数据转换 $\\tau \\sim \\mathcal{T}$ 和一个目标位置 $v \\sim \\mathcal{V}{tgt}$； 然后将 $τ(o)$ 以 $v$ 为中心，取代原始数据和标注。  为了充分利用 TumorCP 的优势，精心设计了两种肿瘤 “Copy-Paste” 模式：\n intra-patient Copy-Paste inter-patient Copy-Paste  2.1 TumorCP’s augumentation Intra-/Inter- Copy-Paste\n为了研究病人之间的差异对 TumorCP 的影响，使用两种设置：\n（1）intra-CP：源图像和目标图像来自同一病人；\n（2）inter-CP：源图像和目标图像来自不同的病人。\n从数据分布的角度来看，由于其强度与数据整体一致，所以首选 intra-CP，但这限制了数据的多样性。从数据多样性的角度来看，inter-CP 更受青睐，因为它打开了利用其他患者的新背景和前景的途径，但它也带来了分布差异。（实验表明 inter-CP 要优于 intra-CP）\n使用三个不同的 object-level 转换来对 “Copy-Paste” 进行扩展。\n Spatial transformation decouples context and improves morphology diversity.  空间转换解耦了上下文，改善了形态的多样性。image-level 的一些增强方法（镜像、旋转）仍然作为一个整体处理图像，保持前景和背景之间的耦合。因此，模型可能会寻找并倾向于过度拟合周围看似合理但实际上不相干的线索。Figure 1 是应用缩放、旋转和镜像的刚性变换和使肿瘤变形的弹性变换来增加形态的多样性。\n Gamma transformation enhances contrast and improves intensity diversity.  伽马变换增强了对比度，提高了强度的多样性。随机采样的伽玛参数增强了肿瘤的强度多样性；幂律非线性增强局部对比度，有利于肿瘤鉴别。\n Blurring transformation improves texture diversity  模糊化改造提高了纹理多样性。使用高斯滤波器作为模糊变换，聚集噪声干扰的低层次纹理可以间接增加相对高层次纹理的纹理多样性。\n整个 Pipeline 可以结合 image-level 增强方法。\n2.2 Intuitions on TumorCP’s Effectiveness TumorCP 有两个目标：i) 增加数据多样性，ii) 学习高层次和抽象肿瘤的不变表示。数据多样性随着肿瘤的新组合和周围环境的增加而增加。为了了解高层次的信息，如下讨论了 TumorCP 的三个特性来解释其有效性。 通过语境不变的预测消除背景偏差（Eliminated Background Bias by Context-Invariant Prediction）  CNN 不可避免地将周围的视觉上下文与物体本身进行卷积，可能会使得模型偏向于看似合理但实际上与肿瘤无关的线索，从而增加过度拟合的风险。\nTumorCP 则为肿瘤提供了一个更为独立的区域，从而为肿瘤周围上下文环境提供了无限的可能性。提升了模型的泛化能力、消除了背景偏差。\n 通过转换-不变预测提高泛化能力（Improved Generalizability by Transformation-Invariant Prediction）  该模型应同时捕获高级语义信息和低级边界信息，以实现成功的分割。TumorCP 可以生成不同大小、形状、颜色和纹理的肿瘤，增加了类内差异。\n它能够帮助模型从数据中捕获更好的语义信息。换句话说，它使模型的预测在不同的数据转换（可能类似于真实世界的数据）中保持不变，并提高了通用性。\n Oversampling Behavior  数据不平衡是一个普遍存在的问题，典型的解决方案通常是根据类分布重新加权损失函数或重新采样训练数据。在肾脏肿瘤分割任务中，背景、器官、肿瘤极度不平衡，TumorCP 就像一个数据重采样器，以较小的成本显著增加肿瘤的增殖程度。\n3 Experiments and Discussion 在 KiTS19 数据集（肾肿瘤的分割）上评估 TumorCP，使用 Sørensen-Dice系数 (Dice) 评分。\n消融学习\n（1）Ablation on intra-CP with different transformations.\n（2）Ablation on intra-/inter-CP\n（3）Ablation on compatibility\n  TumorCP（Object-level）和 Image-level 的图像增强是兼容的；\n  TumorCP 也改善了器官分割；\n  极低数据量的实验\n4 Conclusion  提出了 TumorCP： a simple but effective object-level data augmentation for tumor segmentation； 在肾脏肿瘤分割任务上，比目前的 SOTA 提升了 2.31% Dice； 实验验证了 TumorCP 在极低数据量情况下的潜力； TumorCP 不直接处理 Inter-CP（不同的病人）之间的分布不匹配问题，但仍然取得了惊人的性能提升。  ","permalink":"http://landodo.github.io/posts/20211119-tumorcp/","summary":"论文简介 论文名称：TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation arXiv：https://arxiv.org/pdf/2107.09843.pdf 单","title":"TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation"},{"content":"🎯Topic: 2.5D methods for Volumetric Medical Image Segmentation论文简介 （1）2.5D 医学图像分割网络综述，发表于 2020 年 10 月。\n Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D Method: An Empirical Study\n （2）2.5D 分割网络实例，发表在 MICCAI 2019\n Learning shape priors for robust cardiac MR segmentation from multi-view images\n Abstract 医学图像数据（CT/MRI）大多是 3D 的，然而 3D CNN 并不一定是好的选择。更多的推理时间、计算代价、参数的增加有过拟合的风险（医学图像标注获取昂贵）。为了解决这一问题，人们提出了许多 2.5D 分割方法，以更低的计算成本利用体积空间信息。\n1. Introduction 3D 医学图像，如 CT 和 MRI 已广泛应用于临床诊断。体积图像的自动分割在生物医学应用中变得越来越重要。目前主要两种策略：（1）将三维体积切割成二维切片，并根据切片内信息训练二维 CNN 进行分割；（2）3D CNN。\n两种方法有其各自的优缺点：\n ✅ 2D CNN 更轻的计算和更高的推理速度。 ❌ 2D CNN 忽略相邻切片之间的信息，阻碍了分割精度的提高。 ✅ 3D CNN 具有对空间信息的感知能力。 ❌ 3D CNN 需要较高的计算成本，更高的推理延迟，参数越多，过拟合风险越大（尤其是小的数据集），这阻碍了进一步临床应用。  为了弥补 2D 和 3D CNN 之间的差距，提出了许多 2.5D 分割方法（也称为伪 3D 方法），通过设计新的体系结构或使用策略将体积信息融合到 2D CNN 中来实现高效的医学图像体积分割。\n本篇文章的主要贡献可以总结如下：\n 本文综述了 2.5D 医学图像分割方法的最新进展，将 2.5D 医学图像分割方法分为多视图融合（multi-view fusion）、融合层间信息（incorporating inter-slice information）和融合二维/三维特征（fusing 2D/3D features）三大类。 对这些 2.5D 方法在 3 各具有代表性的数据集（CT/MRI、心脏/前列腺/腹部）上进行了大规模的评估、比较。  2. Related Work 本节对多视图融合（multi-view fusion）、融合层间信息（incorporating inter-slice information）和融合二维/三维特征（fusing 2D/3D features）三大类 2.5D 分割方法进行综述。\n2.1 多视图融合（multi-view fusion）\n为了将体积空间信息整合到二维 CNN 中，一个简单而直观的解决方案是多视图融合（MVF）。\n多视图包括矢状面（sagittal）、冠状面（coronal）、横断面（axial）。训练 3 个2D CNN 分别从矢状面、冠状面和横断面进行分割，再将 3 个视图的结果融合在一起，可以充分利用 3D 空间信息，从而获得比 2D CNN 更好的分割结果。\n融合的方式有：多数投票、 Volumetric Fusion Net（一个浅层的 3D CNN）。\n2.2. 融合层间信息（Incorporating inter-slice information）\n三维空间的分辨率并不总是相同的，有时 x 和 y 轴的图像分辨率比 z 轴高十倍以上（直肠癌数据就是这样 shape=768x696x48）。这种情况下训练长轴（冠状面：xz 或 矢状面：yz）是不可取的。\n另一种策略将层间信息整合到二维 CNN 中，以探索空间相关性。网络不仅可以利用切片的信息，还可以利用相邻切片的信息。网络的输入是连续的切片，输出是对应中间切片的分割结果。通过整合片间信息，可以利用体积的空间相关性，同时避免了 3D 计算的沉重负担。\n不少方法都是喂入 3 张 slices，但是直接添加相邻的片作为多通道输入可能效率低下。当相邻切片混合成通道维时，输入切片的信息在第一卷积层融合，这一过程网络很难提取用于区分每个片的有用信息。\n计新的层间信息抽取体系结构成为研究的重点。\n bi-directional convolutional long short-term memory (BC-LSTM) ：3D 体的 2D 切片被视为一个时间序列，以提取切片间的上下文和特征； inter-slice attention module：利用相邻切片的信息生成 attention mask，为分割提供先验形状调节；contextual-attention block：使用片与片之间的元素减法，强制模型聚焦于边界区域。  2.3. 融合二维/三维特征（Fusing 2D/3D features）\n一些工作也集中在融合从二维和三维 CNN 提取的特征，虽然这些方法仍然使用三维卷积来提取空间信息，但是与训练纯三维 CNN 相比，总体计算成本降低了。\n#########################################################################\n2.5D 分割网络实例，MICCAI 2019\nAbstract 受经验丰富的临床医生如何通过多个标准视图（即长轴和短轴视图）评估心脏形态和功能的启发，本篇论文提出了一种新的方法，在不同的二维标准视图中学习解剖形状先验，并利用这些先验从短轴（SA） MR 图像中分割左心室 (LV) 心肌。\n提出的分割方法具有二维网络的优点，但同时结合了空间背景。在不同的短轴切片上实现了准确和稳健的心肌分割。\n1. Introduction 基于二维的分割网络，以 slice-by-slice 的方式训练，对于复杂形状的目标，小目标的情况，分割的结果不太理想。这是由于 2D 网络没有结合相邻的 short-axis(SA) 图像或 long-axis(LA) 图像的空间信息。\n对于三维的分割网络，心脏的三维空间背景可能受到潜在的层间运动伪影和低平面空间（SA）分辨率的影响，从而限制了它们的分割性能。（直肠癌使用单纯的 3D 网络，估计无法取得良好的效果）\n x轴和y轴保持的分辨率远高于z轴，3D cnn的性能优势并不明显，有时甚至不如一些2.5D方法。\n 经验丰富的临床医生能够从多个标准视图评估心脏形态和功能，即使用 SA 和 LA 图像来形成对心脏解剖的理解。直觉上，从多个标准视图学习到的表示对 SA 切片的分割任务是有益的。受此启发，论文提出了一种通过四个标准视图学习解剖学先验知识的方法，并利用该方法对二维 SA 图像进行分割。\n贡献总结：\n a) developed a novel autoencoder architecture (Shape MAE)，它从多个标准视图中学习心脏形状的潜在表示； b) developed a segmentation network (multi-view U-Net)，结合多视图图像的解剖形状先验来指导SA图像的分割； c) 与 2D/3D 方法进行评估，表明该方法具有更强的鲁棒性，且对训练数据大小的依赖性较小。  2. Methods 提出的方法包含两个新的架构：\n shape-aware multi-view autoencoder (Shape MAE)：从标准心脏采集平面（包括短轴和长轴视图）学习解剖形状先验； multi-view U-Net：通过将 Shape MAE 学习到的解剖学先验信息整合到改进的 U-Net 体系结构中，实现心脏图像分割。  Shape MAE: Shape-aware multi-view autoencoder\n通过多任务学习从标准心脏视图学习解剖形状先验。对于给定输入的原图 $X_i$，网络学习 $X_i$ 的低维表示 $z_i$，它最能重建所有 $j$ 个目标视图分割 $Y_j$。\n本论文采用 4 source views $X_i \\ (i = 1, 2, 3, 4)$，分别是 3 long-axis(LA) views，和 1 short-axis(SA) views，。\n LA: two-chamber view (LA1), three-chamber view (LA2), the four-chamber view (LA3)\nSA: mid-ventricular slice (Mid-V) from the SA view\n 网络从其中一个视图学习低维表征 $z_i$ 来重建分割图。分割图的视图 $Y_i$ 有 6 个，4 个对应源视图，另外两个为 SA lices：apical 和 basal。\n损失函数：\n 前两项为交叉熵； 最后一项是 latent representations $z_i$ 的正则化项；  该网络的原理是，不同的视图需要独立的函数将它们映射到描述全局形状特征的潜空间；而将这个潜空间转换到另一个视图或平面也需要一个特定的投影函数。根据六个目标视图而不是单一视图来预测心肌的形状，鼓励网络学习和利用不同视图之间的相关性，从而形成一个全局的、视图不变的形状表征，而不是一个特定视图的局部表征。这个框架中的所有编码器和解码器都是以多任务学习的方式联合训练的，这样做的好处是避免过度拟合，鼓励模型的泛化。\nMV U-Net: Multi-view U-Net.\nMV U-Net 相比于原始的 U-Net 卷积核更少一些（取决于当前的任务），并且结合 shape MAE 学习到的解剖形状先验信息。\nFuse Block 模块由两个卷积核 (3 × 3) 和一个残差连接组成，通过可学习权值将不同视图的形状表示结合起来。给定任意短轴图像切片 $I_p$，和它相应的 shape representation $z^{p}_1, z^{p}_2, z^{p}_3, z^{p}4$（通过 Shape MAE 获得），网络可以将先验知识提炼为网络的高级特征，使其能够通过多视图信息有效地细化分割：$S_p = f{MV\\ U-Net}(I^p, z^{p}_1, z^{p}_2, z^{p}_3, z^{p}_4; \\theta)$。\n该网络采用标准的交叉熵损失进行训练。\n3. Experiments and Results 4 Conclusion 提出了一个形状感知的多视图自动编码器，一个能够从多个标准视图学习解剖形状先验信息的多视图 U-Net，该网络是对原始 U-Net 架构的修改，合并了学习的形状先验信息，以提高心脏分割的鲁棒性。\n本论文将长轴 LA 和短轴 SA 结合起来，利用长轴图像的空间背景来指导短轴图像的分割。从 LA 视图中提取的额外解剖信息，对那些具有挑战性的切片的分割特别有利。\nMV U-Net 保持了 2D 网络的计算优势，在有限的训练数据下实现较高的分割性能。\n 什么是 LA？什么是 SA？\n✅答：即长轴和短轴数据。例如在直肠数据中，一个 .nii.gz 的 shape 为 768x696x48，那么 xy 为短轴，xz/yz 为长轴。xy 为短轴，横断面，分辨率较高。长轴分辨率低，数据模糊。\n","permalink":"http://landodo.github.io/posts/20211105-2-5d-network/","summary":"🎯Topic: 2.5D methods for Volumetric Medical Image Segmentation论文简介 （1）2.5D 医学图像分割网络综述，发表于 2020 年 10 月。 Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D Method: An Empirical Study （2）","title":"2.5D methods for Volumetric Medical Image Segmentation"},{"content":"Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation\n论文题目：协同训练和隐空间数据增强的鲁棒性医学图像分割\n https://arxiv.org/abs/2107.01079  会议：MICCAI 2021\n作者：Chen CHEN 陈晨\n  https://sites.google.com/view/morningchen-site/home\n  Research Assistant/PhD 伦敦帝国理工学院 Research Assistant/PhD\n  本科哈工大 物联网工程\n  代码：https://github.com/cherise215/Cooperative_Training_and_Latent_Space_Data_Augmentation\nAbstract 基于深度学习的分割方法在部署过程中容易受到不可预见的数据分布（unforeseen data distribution）变化的影响，例如，不同扫描仪引起的图像外观或对比度的变化，伪影等。本篇论文提出了一个用于图像分割模型的协同训练框架和一个用于生成困难样本的隐空间数据增强方法，提升了模型的泛化性和有限数据的稳健性。\n 协同训练框架由 fast-thinking network(FTN) 和 slow-thinking network(STN) 构成。FTN 学习解耦的图像特征和形状特征，用于图像重建和分割任务； STN 学习形状先验，用于分割校正和细化。 隐空间数据增强（latent space DA）是通过在通道和空间上遮盖解耦的隐空间，为训练生成具有挑战性的样本。  在公共心脏成像数据集进行了广泛的实验，证实了分割性能的提高，以及提高了对各种不可预见的成像伪影的稳健性。与标准的训练方法相比，具有隐空间数据增强的协同训练在平均 Dice 提高了15%。\n1 Introduction 将基于深度学习的方法部署到实际应用中的一个主要障碍是临床部署过程中的域转移（domain shift），其中包括不同医疗中心和扫描仪的图像外观和对比度的变化，以及各种成像伪影。在 Multi-domain datasets 上学习域不变的特征进行分割可以解决上面的问题，但是 Multi-domain 数据的获取成本较高，因此从 single-domain 和有限的数据中学习稳健的网络，对医学影像研究具有重要的实用价值。\n本篇论文提出了一个新的协同训练框架，用于从 single-domain 数据中学习到一个稳健的分割网络。主要贡献可以总结如下：\n（1）设计了一个由两个网络组成的协同训练框架这个框架启发自人类行为中的双系统模型：快速思维系统做出直觉判断，而慢速思维系统则通过逻辑推理进行纠正。\n对应到框架中，fast-thinking network (FTN) 旨在理解图像的上下文，并提取与任务相关的图像和形状特征进行初始分割；slow-thinking network (STN) 根据学习到的形状先验来完善初始分割。\n（2）提出了一种隐空间数据增强方法 latent space data augmentation (DA) method随机和有针对性的方式对从 FTN 中学习到的隐编码（latent code）进行通道和空间的遮盖（屏蔽），这样就会重建出一组多样化的挑战性图像和对应的分割图，以加强两个网络的训练。\n2 Methodology single domain dataset $D_{tr} = {(x_i, y_i)}^{n}_{i=1}$\nimage $x_i \\in \\mathbb{R}^{H \\times W}$\none-hot encoded C-class label maps $y_i \\in {0, 1}^{H \\times W \\times C}$ （ground truth）\n2.1 Framework 给定输入图像 x，FTN 提取特定任务的形状特征 $z_s$ 来执行分割任务，提取图像上下文特征 $z_i$ 来执行图像重建任务。共享 encoder $E_{\\theta}$，特征解耦器 $\\mathcal{H}$， 两个特定任务的解码器 $D_{\\phi_s}$、$D_{\\phi_i}$ 用于图像分割和重建任务。\n对 $z_i$ 应用特征解耦器 $\\mathcal{H}$，使与分割任务无关的信息（如图像纹理信息、亮度）在 $z_s$ 中被停用，稀疏的 $z_s$ 有益于模型的稳健性。\n$\\mathcal{H}$ 由两个卷积层堆叠，后接 ReLU 激活函数。图像重建需要低层次的信息，而图像分割则依赖于更集中的高层次信息。引入 $\\mathcal{H}$ 明确地定义了一个分层的特征结构，以提高模型的通用性；\nSTN 是一个去噪自动编码器网络 $C_{\\psi}$，学习形状先验来纠正 FTN 预测的分割。在推理时，FTN 进行对给定的图像 x 进行快速分割：$p = D_{\\phi_s}(\\mathcal{H}(E_{\\theta}(x)))$；STN 完善分割结果：$p\u0026rsquo; = C_{\\psi}(p)$。\n2.2 Standard Training 用监督的多任务损失函数联合训练三个编码器-解码器对，损失包含：图像重建 $L_{rec}$、图像分割 $L_{seg}$、形状校正 $L_{shp}$。\n $L_{rec}$ 是 Mean Squared Error(MSE) $L_{seg}, L_{shp}$ 是 cross-entropy function $y\u0026rsquo; = C_{\\psi}(y)$  2.3 Latent Space Data Augmentation for Hard Example Generation 为了缓解过拟合，提出了 latent space DA method，使得 FTN 可以自动构建具有挑战性的样本。\n掩码生成器 $\\mathcal{G}$ 在 latent code $z$ 上产生一个掩码 m，之后 $\\hat{z} = z \\cdot m$ 输入到解码器重构出被破坏的图像 $\\hat{x} = D_{\\phi_i}(\\hat{z_i})$ 和其对应的分割图 $\\hat{p} = D_{\\phi_s}(\\hat{z_s})$。这就是 latent code masking 数据增强方法。\n通过动态屏蔽 latent code，所提出的方法可以生成具有广泛多样性的图像外观和分割的样本。如下介绍几种 latent-code masking 方案：\n（1）Random Masking with Dropout\nlatent-code 的整个通道在训练时以 p 的概率被掩盖为全零。\n被遮盖后第 i 个通道的结果为：$\\hat{z}^{(i)} = z^{(i)} \\cdot m^{(i)}$。$z \\in \\mathbb{R}^{c \\times h \\times w}$。\n（2）Target Masking\n提出了有针对性的 latent-code masking 方案，该方案以梯度为线索来识别要掩蔽的“突出”特征。 采取图像重建损失和图像分割损失，分别计算 $z_i$ 和 $z_s$ 的梯度 $g_{z_i}$和 $g_{z_s}$。\n对梯度值进行排序，可以确定出那些最具预测性的元素。作者假设，对损失函数反应较高的元素是导致在不可预见的域转移下性能下降的主要原因。因此，对这种元素进行有针对性的遮蔽，以模拟数据分布的转移（data distribution shifts）。可以在通道维度和空间维度遮蔽 latent-code $z$ 的特征。\n $z^{ch}_p, z^{sp}_p$ 阈值 a 是 (0, 0.5) 之间随机抽取的退火系数，创建 soft masks；  Channel-wise masked code at i-th channel：\n$\\hat{z}^{(i)} = z^{(i)} \\cdot m^{(i)}$\nSpatial-wise masked code at (j, k) position：\n$\\hat{z}^{(j, k)} = z^{(j, k)} \\cdot m^{(j, k)}$\n2.4 协同训练 训练过程中，随机地将上面介绍的三种 mask generator 应用于 $z_i, z_s$，这个操作生成一组丰富的增强图像 $\\hat{x}$ 和分割图 $\\hat{p}$。这就得到 3 example pairs 用于训练：\n corrupted images-clean images $(\\hat{x}, x)$ corrupted images-GT $(\\hat{x}, y)$ corrupted GT-GT $(\\hat{p}, y)$  合作训练的最终损失定义为简单例子和困难例子上的损失的组合。\n3 Experiments and Results 应用与心脏图像分割任务，从 MR 图像中分割出左心室腔、左心室心肌和右心室。\n数据集：\n  ACDC：训练、intra-domain 测试\n  M\u0026amp;Ms：cross-domain 测试\n  ACDC-C (corrupted ACDC)：评估数据增强方法的鲁棒性\n  （1）实验 1：Standard Training vs Cooperative Training\n 两种方法在域内测试集上取得了相近的性能； 双网络（FTN+STN）的协同训练在域外测试集上取得了更高的性能； 隐空间数据增强方法数据上，协同训练也表现出了其优越性。  （2）实验 2: Latent Space DA vs Image Space DA\nTable 1 中 AdvBias 在 M\u0026amp;Ms 数据集和 RandBias 上取得了最好的性能，但这种方法有一个副作用，使其对尖峰伪影更加敏感（Dice 得分 0.4901 vs 0.3840）。latent-space DA 在六个数据集上取得了最高的平均性能。\n图 3. 本篇论文的方法不仅可以生成扰动的图像，还可以生成不确定性增加的真实的损坏的分割。\n（3）实验 3：Ablation Study\n(a) the proposed targeted masking; b) latent code decoupler H; c) cooperative training.\n 禁用 $\\mathcal{G}{ch}、\\mathcal{G}{sp}$ 后，平均 Dice 得分从 0.6901 降至 0.6584； 图像重建需要低层次的信息，而图像分割则依赖于更集中的高层次信息。引入 H明确地定义了一个分层的特征结构，以提高模型的通用性； 突出了合作训练策略加强基于学习的形状细化和修正；  4 Conclusion  提出了一个新的协同训练框架，以及一个 latent space masking-based 的数据增强方法； 实验验证了模型的通用性和对不可预见的领域转变的鲁棒性； latent-space DA 方法只需要很少的代价； 目前只在心脏图像分割上展示了其性能，该通用框架有可能扩展到广泛的数据驱动的应用。  ","permalink":"http://landodo.github.io/posts/20211024-latent-space-data-augmentation/","summary":"Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation 论文题目：协同训练和隐空间数据增强的鲁棒性医学图像分割 https://arxiv.org/abs/2107.01079 会议：MICCAI 2021 作者：Chen CHEN 陈晨 https://sites.google.com/view/morningchen-site/home Research Assistant/PhD 伦敦帝国理工学","title":"20211024 Latent Space Data Augmentation"},{"content":"3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation #结直肠癌分割📌\n  Cited by: 18\n  Publish Year: 2020\n  Published in: IEEE Transactions on Cybernetics\n  地址：https://ieeexplore.ieee.org/document/9052757\n  Code：https://github.com/huangyjhust/3D-RU-Net\n  0. Abstract 基于深度学习的方法在 3D 图像分割任务中提供了一个良好的 baseline，但是由于内存的限制，较小的 patch 限制了有效的感受野，影响分割性能。\n将 RoI 定位作为前项操作，在速度、目标完整性、减少假阳性（FN）等方面具有多重优势。本篇论文：\n  提出了一种==多任务框架（3D RoI-aware U-Net）==，用于 ROI 定位和区域分割；\n  设计了一个==基于 Dice 的损失函数（MHL）==，用于从全局（RoI 定位）到局部（区域内分割）的多任务学习过程。\n  在 64 例癌症病例上进行实验，结果表明，该方法明显优于传统的方法，具有较强的泛化性、拓展潜力，可用于医学图像的其他 3D 目标分割任务。\n1. Introduction 基于深度学习的方法在医学图像检测和分割领域处于领先地位，然而，依旧面临着许多挑战：强度特异性弱、缺乏形状特征、缺乏位置先验、类别不平衡，以及在较差的 GPU/CPU-only 上的处理时间过长。除此之外，patch 大小受限于 GPU 显存，扩大感受野和减少降采样过程细粒度丢失是一个至关重要的问题。\n✅在医学应用中，由于目标和背景高度相关，因此==全局理解==甚至更为重要。\n🚩本篇论文贡献总结如下：\n  提出一种新的联合 RoI 定位-分割框架（3D RoI-aware U-Net），具有如下优势：fast RoI localization；target completeness； large effective receptive field；easy-to-train；detail-preserving；end-to-end；volume-to-volume segmentation。\n  设计的混合损失函数（Dice formulated global-to-local multi-task hybrid loss, MHL）帮助网络既处理大体积的小目标，又专注于准确识别局部 RoI 中的边界。\n  通过实验验证了所提出的框架的有效性、通用性；\n  2. Related Work 现有的 3D 图像病变检测和分割方法一般可以分为：基于局部的模型（part based models）和 non-joint localization-segmentation based methods。\n  part based：FCN、V-Net，有效感受野有限。\n  non-joint localization-segmentation based：RoI 定位模块作为独立的部分，外部模块 Selective Search、Multiscale Combinatorial Grouping、FPN 提取候选区域；\n  上述两种方式存在的问题：使用基于 patch 的分割无法解决感受野有限的问题；使用独立的外部模块进行候选区域的提取，再独立的 FCN 进行 RoI 分割时，无法共享特征。\n联合 RoI 定位-分割模型是一种很有前景的发展，共享 backbone 来实现区域候选、区域分类和区域内分割，消除了冗余特征提取。\n  Multi-task Network Cascades\n  Mask R-CNN: FPN\n  类别不平衡问题：\n V-Net：Dice loss Deep Contour-aware Network Multilevel Contextual 3D CNNs DeepMedic \u0026hellip;  3. Methodology 3D RU-Net 结构如上图所示。\n  将整个 image volumes 输入 ==Global Image Encoder==，进行多层次编码；\n  采用编码器专用的 ==RoI locator== 进行 RoI 定位；\n  利用 ==RoI Pyramid Layer== 从多尺度特征图中裁剪区域内特征张量，获得多尺度的 RoI 区域，图中称为 RoI Tensor Pyramid $(f^I, f^{II}, f^{III})$；\n  设计一个 ==Local Region Decoder== 来进行多级特征融合，用于高分辨率癌症病灶分割。\n  3.1 Main Modules （1）Global Image Encoder\n谨慎设计 3D backbone feature extractor 以避免 GPU 内存溢出和过拟合。构建一个紧凑的仅有编码器的网络，名为 Global Image Encoder，用于处理 whole volume images。\nResBlocks + MaxPooling 堆叠。\nResidual Block:\n 3 convolutional layer 3 Instance Normalization (batch size = 1) 3 ReLU Skip Connection  （2）RoI Locator\nRoI Locator 是一个模板，以特征图 $F^{III}$ 作为输入，得到 $Bbox^{III}$ 输出。任何采用纯编码骨干的目标检测方法都可以被采用。\n由于数量有限的训练样本的长宽比多样性，学习准确的边界框可能是困难的，建议充分利用可用的体素级掩码。为了解决前景与背景比例极不平衡的问题，采用基于 Dice 的损失来训练 ROI Locator。\n进行快速的三维连通性分析（Fast 3D connectivity analysis）计算出所需的 Bounding Box（$Bbox^{III}$）。\n📌**（3）==RoI Pyramid Layer==**\n从每个特征尺度提取一组多层次的特征张量，充分利用多尺度特征。\n为了提取检测目标的 RoI Tensor Pyramid，首先从(2) RoI Locator 计算得到的边界框（Bounding box）$Bbox^{III}=(z^3, y^3, x^3, d^3, h^3, w^3)$ ，公式(1)构建 Bounding Box Pyramid $(Bbox^{I}, Bbox^{II}, Bbox^{III})$。\n $(s_{z}^{i}, s_{y}^{i}, s_{x}^{i})$ 表示 $MaxPooling^{i}$ 的 stride；  得到了 Bounding Box Pyramid $(Bbox^{I}, Bbox^{II}, Bbox^{III})$ 后，从 $F^{I},F^{II}, F^{III}$ 中裁剪出 RoI Tensor Pyramid $(f^I, f^{II}, f^{III})$。\n（4）Local Region Decoder\n得到了 RoI Tensor Pyramid $(f^I, f^{II}, f^{III})$ 之后，构建一个名为 Local Region Decoder 的区域内分割网络，这个网络融合了多尺度的特征。\n3.2 Loss Function design 文章的另一个核心点在于 ==Dice-based Multi-task Hybrid Loss Function (MHL)== 的设计。\n上图的网络结构属于多任务学习（Localization + Segmentation），Global Image Encoder 主要面临类别不平衡问题；而 Local Region Decoder 则是目标区域的精确边界分割问题。\n（1）Dice Loss\n N voxels $p_i \\in P$ ：predicted volume $g_i \\in G$：ground truth volume $\\epsilon = 10^{-4}$ ：平滑参数  （2）Dice Loss for Global Localization\n $P_{global}$ and $G_{global}$ denotes predictions of the localization top and down-sampled annotations.  ❓这个我不太明白。我的理解是不同的检测方法，这是只是提供了一个范式。    （3）Dice-based Contour-aware Loss for Local Segmentation\n Contour 表示边轮廓。（3D 空间中轮廓标签的极端稀疏性）； 在分割的输出端增加一个额外的由 Sigmoid 激活的 1 × 1 × 1 卷积层来预测轮廓体素，并与区域分割任务并行训练； $\\lambda_c = 0.5$，辅助任务的权重，确保区域分割任务占主导地位。  ===\u0026gt; 最后得到总的损失函数，Dice-based Multi-task Hybrid Loss Function (MHL)：\n $\\beta = 10^{-4}$  3.3 多感受野模型集成 本文提出采用多感受野模型集成策略，融合结构相同但感受野设置不同的模型。如下图，将三个网络的输出取平均，生成最终的预测。\n不同感受野模型，实现的方法是控制空洞卷积的 dilation rate。下表感受野为 26 × 64 × 64 为原始的 3D R-U-Net，记为 3D RU-Net-RF-64。\n4. Experiments 4.1 数据集和预处理 64 例 MRI 图像，T2 模态。目标区域由经验丰富的放射科医生进行标注，一个 3D 图像通常有一到两个含有癌组织的 RoI。癌组织轮廓标签 contour labels were automatically generated from the region labels of one-voxel thickness using erosion and subtraction operations.\ncrop 黑边、重采样 4.0 × 2.0 × 2.0 mm、强度归一化。\n下图是归一化（intensity-normalized）的效果。\n4.2 实现细节 网络结构如 Table 1 所示。\n Optimizer: Adam batch size = 1 输入的 shape = ？ learning rate: 10e-4 L2 norm: 10e-4 先训练 RoI Locator，直到评估 Loss 不在降低； 再联合训练 RoI Locator 和分割分支。联合训练过程的 Loss 来自 RoI Locator + SegHead1 + SegHead2。  评估指标：Dice Similarity Coefficient (DSC)、Voxel-wise Recall Rate、Average Symmetric Surface Distance (ASD)。\n2 块 NVIDIA Titan(12 GB GPU memory)\n4.3 实验结果 Table 2（消融学习）、Figure 5、Figure 6\n（1）（2）5. Discussion 相比于传统的 Encoder-Decoder 在每个路径上各花费 50%，本文通过构建 Local Region Decoder，GPU 可以将其 90% 的 GPU 内存分配给 Encoder，以处理更大的输入体积，只在分割阶段花费 10% 的内存。因此，可以处理的体积大小被大大的扩大了！（有几率不用预先切 Patch 了）\n提出的方法的局限性：\n  模型经常混淆哪个切片开始或结束，这对得分的影响较大（Figure 6）\n   这个困难是与数据相关的，由于癌组织边界的对比度较弱，沿 Z 轴的分辨率较低，开始和结束切片指数的决定可能取决于观察者。\n     没有进行实例分割相关的探索  Conclusion  提出了联合 RoI localization-segmentation-based 框架（3D RoI-aware U-Net）； 强调了将 RoI 定位和区域内分割结合==全局编码特征==的重要性和有效性； 提出多任务混合损失(MHL)来平滑训练过程； 实验结果表明，该方法在速度和准确性方法具有较大优势； 原则上，此框架具有良好的可拓展性，可以用于其他医学图像分割任务。  ","permalink":"http://landodo.github.io/posts/20210924-3d-roi-aware-u-net/","summary":"3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation #结直肠癌分割📌 Cited by: 18 Publish Year: 2020 Published in: IEEE Transactions on Cybernetics 地址：https://ieeexplore.ieee.org/document/","title":"3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation"},{"content":"论文简介 基于三维卷积神经网络的头颈部血管图像快速分割与重建\n Fu, F., Wei, J., Zhang, M., Yu, F., Xiao, Y., Rong, D., Shan, Y., Li, Y., Zhao, C., Liao, F., Yang, Z., Li, Y., Chen, Y., Wang, X., \u0026amp; Lu, J. (2020). Rapid vessel segmentation and reconstruction of head and neck angiograms using 3D convolutional neural network. Nature communications, 11(1), 4829. https://doi.org/10.1038/s41467-020-18606-2\n Publish Year: 2020\n期刊：Nature Communications\n下载地址：https://www.nature.com/articles/s41467-020-18606-2.pdf\n摘要 计算机断层扫描血管造影图像（ computed tomography angiography, CTA）的后处理（分割、重建）操作时一项非常消耗时间、且易于出错的任务。本片论文提出了一种充分利用生理解剖学的 3D 卷积神经网络 CTA 重建系统。\n数据集来自中国 5 家三甲医院的 18766 例头颈部 CTA 扫描图像。最后的结果为 DSC = 0.931，该系统的结果与人工处理的图像具有一致性，符合临床要求，临床评估结果合格率为 92.1%。\n该系统部署应用 5 个月之后，对于每一个病例的处理时间从 $14.22\\pm 3.64$ 减少到 $4.96 \\pm 0.36$ 分钟，促进了临床工作流程。\nIntroduction 计算机断层血管造影（CTA）是一种广泛的、微创的、经济有效的成像方式，用于头颈部血管的常规临床诊断。可视化分析头颈部的血管系统（血管成像重建）通常由经验丰富的计算机断层扫描技术人员进行。\n血管是曲折和有分支的，因此，准确的血管分割、保证连续性（不中断）是一个具有挑战性的任务。除此之外，分割还容易受到其他组织的影响，如颅内入口血管的 CT值与颅骨 CT 值高度相似，这可能导致血管外组织粘连，影响后续疾病诊断。\n深度神经网络架构是克服上述技术障碍的最有效途径。U-Net 已经被证实了其在医学图像分析中的应用前景。3D CNN 符合生理解剖，目标图像的形态学特征是专门为分割任务设计的。\n本篇论文开发了一种基于优化解剖学先验知识的 3D-CNN 自动成像重建系统（CerebralDoc，脑科医生），用于重建原始的头颈部 CTA 图像，帮助工作人员建立节省时间的工作流程。\n最后，对 CerebralDoc 进行全面评估，结果表明，将人工智能技术集成到放射科工作流程中，能大幅度提高工作流程效率，降低医疗成本。\nResults 患者及影像特征\n数据集包含 18766 例头颈部 CTA 扫描的患者，手工筛选排除 507 例低质量图像。表 1 总结了用于训练、验证和独立测试的 CTA 扫描数据集。\n模型性能\n数据增强后，有 91,295 例（扩增了 5 倍）用于训练。3D-CNN 使用使得 U-Net + bottleneck-ResNet（ResU-Net）。CerebralDoc 系统分为两个主要部分：\n ResU-Net：主要负责骨分割和血管提取； connected growth prediction model (CGPM). 关联增长预测模型：负责曹正血管的连续性；  模型训练过程包括骨分割和血管分割。血管分割（Vessel Segmentation）和 Bone Segmentation 的 DSC、V-score、Recall 如下 Fig.2。\nCerebralDoc 的临床评估\n AI 和手工处理在整体临床评估数据集和不同疾病中的 5 分评级分布； Fig 4 直接呈现由 CerebralDoc 和手工处理生成的图像； 两图都得出相同结论：CerebralDoc 和人类输出之间没有统计学上的显著差异（图 3a和 4）。   VR 和 MIP 中 AI 与人工处理的比较； 人工处理中右侧大脑中动脉闭塞，未重建侧支循环（a），AI 成功重建侧支循环（b）； AI 比人工处理的图像更清晰，尤其是海绵窦段（c-d） 在 CerebralDoc 中，由于算法中的自动骨分割，所有的 MIP 图像都被评分为3分（被认为有中度骨残留但不影响血管观察）。  CerebralDoc 的临床应用效果\n 所有接受头颈部 CTA 检查的患者，包括动脉粥样硬化性溃疡、动脉瘤、烟雾病、颈部支架植入术的患者，均使用 CerebralDoc。   2019 年 7 月至 11 月，宣武医院临床共 3430 例患者行头颈部 CTA 扫描；经经验丰富的技术人员评价，成功重建 3122 例(91.0%)。最后，将 2649 张重建图像应用于临床(图7a)。而且，随着医生信任度的提高，重建图像被推送到诊所的概率从 243 逐渐增加到 753 (图7b)。  比较 CerebralDoc 与手工处理的性能\n两位技术人员手工处理一个病例的平均耗时为：13.48 ± 3.67、14.95 ± 3.65 分钟，使用 CerebralDoc 后耗时为 14.22 ±3.64、4.94 ± 0.36 分钟。点击次数的差异更为巨大。\nDiscussion 提出了 CGPM 来修正血管分割错误，避免部分缺失血管。此外，作者们在宣武医院将 CerebralDoc 应用于临床场景，利用人工智能重建直接用于头颈部血管疾病诊断的 CTA 图像。\n结果表明：3D-CNN 深度学习算法可以在各种增强 CTA 扫描中以高灵敏度和特异性自动完成骨骼和血管分割（包括主动脉、颈动脉和颅内动脉）。\nCerebralDoc 重建的图片质量由两名临床经验丰富的放射科医师进行评估，合格率为 92.1%。\n以往将深度学习用于血管分割任务的研究大多集中在算法优化和模型结构方面，这些工作促使我们开发了一种有针对性的自动化临床血管分割工具(补充表2)。\n Y-Net：对 49 例磁共振血管造影（MRA）数据进行颅内动脉三维分割，测试集的精度为 0.819； HalfU-Net：脑血管病患者(TOF)-MRA 血管分割，DSC=0.88； MS-Net：全分辨率分割算法，提高了分割精度和分割精度，并显著降低了监督成本；  大部分的工作主要集中在特定血管区域的分割，而完整的头颈部 CT 扫描包括三种不同大小的血管（主动脉、颈动脉和颅内血管），这使得模型很难捕捉到跨尺寸级别的血管特征。\n另一方面，以前的大多数研究都是由相对较小的有限的数据集得来的，缺乏来自真实临床数据的验证。\n因此，通过优化的基于生理解剖学的 3D-CNN，在保证最大 patch 大小（256 × 256 × 256 体素）的前提下，建立一个全面的管道，实现头颈部 CTA 自动后处理（分割重建），具有重要的临床意义。\nU-Net 于 2015 年提出，之前的研究使用了这种策略，将大图像裁剪成小碎片，导致整体血管的结构特征丧失。考虑到血管完整性的重要性，本片论文的策略采用裁剪的方式将原始的 3D 切片输入到 3D CNN 中，并使用残差块 bottleneck-ResNet (BR) 对 U-Net 架构进行修改，从而自动找到优化的模型参数。\n连接三个不同大小层次的血管只仍然是一个挑战。基于生理解剖信息，我们将整个体积划分为三个区域（主动脉、颈动脉和颅内区域）用于网络，以更好地提取不同大小血管的特征。\n此外，还提出了 CGPM，通过对没有标注的原始 CTA 图像的输入数据、ResNet1、ResNet2 和 ResNet3 产生的当前分割结果以及标注的 CTA 图像进行学习，修正血管分割的错误，有效避免部分血管的缺失。\n由于从主动脉到颅骨范围广，且脑动脉血管曲折分支，很少有研究从头颈部 CTA 扫描中对血管分割进行研究（文中又列出一些）。本文的研究是一项多中心、大样本的研究，按照临床后处理过程，完成主动脉至颅内动脉的骨分割和血管分割的自动框架。\nCerebralDoc 在的评估结果在不同疾病中的整体合格率和表现与手工加工没有显著差异。\n3D-CNN 可以学习血管特征，更准确地改善和捕获血管信号。\nCerebralDoc 仍存在一定的局限性：\n（1）头颈部动脉严重异常起源（除左椎体直接产生于主动脉和严重狭窄起源外的异常血管常规）的患者图像不包括在训练和验证集中。因此，需要进一步的临床收集和测试来评估各种形式的颅骨和颈部血管的临床准确性。由于这种畸形的发生率相对较低，所以这个问题并不影响最终的整体结论；\n（2）未进行噪声图像进行测试；\n（3）CerebralDoc 目前只适用于 head and neck CTA images。\n总结，CerebralDoc 是一个实用的头颈部 CTA 重建系统。与现有的 CTA 图像优化重建技术相比，它提供了一种省时、不依赖主观性的方法，节省了成本，提高了效率。基于人工智能的标准化自动 U-Net 和可视化后处理图像的生成，CerebralDoc 有潜力融入目前的放射手术工作流程。\nMethods 数据的准备\n数据集包含 18766 例头颈部 CTA 原始造影增强图像，有中国 5 家三甲医院提供。所有头颈部 CTA 图像扫描包括主动脉、颈动脉和颅内动脉，Z 轴的切片为 561~967 slices。\n人工初步筛查排除了 507 例低质量的数据。所有患者 CTA 图像均为 DICOM 格式。\n本研究的初步获取了 2018 年 1 月至 2019 年 2 月期间，4 家不同的 CT 制造商的 18259 例患者的 14461128 张头颈部 CTA 扫描图像。\n训练集：16433 例患者\n测试机：1826 例患者\n训练时，将 16433 例患者分给两个模型，Model 1 进行骨分割（6387 例），Model 2 进行血管分割（10046）。\n在进行临床评估时，从宣武医院获取 2019 年 5 月至 2019 年 6 月期间额外的 152 例进行评估。这些数据依然是有标注的数据，两名经验丰富的临床医生，随机采用 5 分制评估影像质量是否满足诊断要求（医生不被告知来源手工标注还是 AI 输出）。\n最后，搜集临床应用集，主要用于验证模型在 7 ~ 11 月真实临床应用场景下的性能。\n放射科医生标注\n为了减少人为误差，采样分层标注的方法。具体来说：\n（1）首先使用 ITK-SNAP 对 18259 例样本进行预标注，分别标记骨、主动脉、颈动脉和颅内动脉区域，它们被分为四类。\n（2）由 10 名 2+ 年经验的技术人员矫正预标注图像；\n（3）由 2 名 5+ 年经验的放射科医生进行复核。如果两名医生意见不一致，则请 10+ 年经验的医生做最后决策；\n每例数据所需的平均处理时间约为 22 分钟。\n数据预处理和图像增强\n尽管标注非常专业和严谨，但是仍然会存在零散噪声。解决的方法是连通分量检测算法（a connected component detection algorithm），消除了散射噪声，提高了数据的鲁棒性。\n数据增强策略：\n 水平翻转 不高于 25° 的旋转 不高于 20 pixel 的水平/垂直平移 选择一个矩形区域，随机擦除像素、随机遮挡  经过增强后，数据量由 N 增大到 5N。为了增强鲁棒性，在训练时还加入了高斯噪声。\n模型开发\n自动分割框架包含三个级联的 ResU-Net，完成去骨、提取血管。ResU-Net2 是专门为 Bone edge optimization 设计的，CGPM 用于消除血管分割错误，有效避免部分或缺失的血管节段。\nResU-Net1 和 ResUNet2 模型负责在相同的高分辨率下使用语义分割对整个骨骼进行分割，并根据动脉的大小学习将血管划分为主动脉、颈动脉和颅内动脉的生理解剖结构特征。\n根据生理解剖结构，采用 ResU-Net3 模型实现血管分割。\n最后，提出的 CGPM 被应用于固定任何破裂的血管，并确保它们可以在形态学水平上被可行地预测和识别。\nCGPM 基于 3D-CNN，输入数据包括：\n original CTA image without labeling 由 ResU-Net1,2,3 生成的分割结果 标注的 CTA 图像  CGPM 能准确定位部分缺失的位置，完成对缺失血管的补充。\n消融学习，证明管道中每个部件的贡献。\n模型性能评估指标\n（1）DSC\n  Y: ground truth\n  $\\hat{Y}$: binary predictions from the neural networks\n  $|Y| + |\\hat{Y}|$: indicates the sum of each pixel value after calculating the dot product between the ground truth and the prediction.\n  （2）V-score\n、\n V-score 是根据血管的解剖和形态位置来计算的，用于揭示血管分割的连续性和完整性。 V-score 越高，血管位置越关键  （3）Recall\n被正确预测为阳性的病例所占的比例。\n临床评估\n3 =血管描绘良好，无间断，血管侧支重建良好，血管图像清晰；\n2 =正常血管轮廓，部分中断，狭窄程度与无遗漏的源图像相比有偏差；\n1 =整个血管中断，与源图像相比有严重的狭窄遗漏。\n2019年7月至11月，在宣武医院对 CerebralDoc 的临床应用价值进行了评估，主要从以下几个方面进行：\n 所有接受头颈部CTA检查的患者的覆盖率； 整体、每月、每天的后处理数量，以及成功推送到 PACS 的数量； 后处理所需的平均时间和 后处理的平均时间和 CerebralDoc 和技术员的点击次数（针对100个随机选择的病人）； 测试月份的临床生产力； 模型的错误率（探讨了原因）。  ===\n一些缩写\nvolume rendering (VR)\nmaximum intensity projection (MIP)\ncurve planar reconstruction (CPR)\ncurved multiple planar reformation (MPR)\nconnected growth prediction model (CGPM)\ncomputed tomography angiography (CTA)\nbottleneck-ResNet (BR)\ntime-of-flight(TOF)-MRA\nabsolute volume difference (AVD)\n===\n","permalink":"http://landodo.github.io/posts/20210912-rapid-vessel-segmentation/","summary":"论文简介 基于三维卷积神经网络的头颈部血管图像快速分割与重建 Fu, F., Wei, J., Zhang, M., Yu, F., Xiao, Y., Rong, D., Shan, Y., Li, Y., Zhao, C., Liao, F., Yang, Z., Li, Y., Chen, Y., Wang, X., \u0026amp; Lu, J. (2020). Rapid vessel segmentation and reconstruction of head and neck","title":"Rapid vessel segmentation and reconstruction of head and neck angiograms using 3D convolutional neural network."},{"content":"Structure Boundary Preserving Segmentation for Medical Image\nwith Ambiguous Boundary\n边界模糊医学图像的结构边界保持分割\n简介 韩国一个大学的文章。\n CVPR 2020 与医学图像分割有关 Cited by: 5 没有代码，没啥意思😭，想去看看它的一些细节！  摘要 医学图像分割存在两个关键性问题：\n 医学图像域结构边界的模糊 没有专业领域知识的细分领域的不确定性。  为了解决这两个问题，提出保存边界的分割框架（boundary preserving segmentation framework）。\n 边界关键点选择算法 边界保留模块（BPB）  提出了一种新型的形状边界感知评价器（SBE），它具有专家指出的地面真实结构信息。SBE 可以根据结构边界关键点向分割网络提供反馈。\nIntroduction 本论文主要解决如下两个问题：\n 大多数医学图像由于较低的图像分辨率和模糊的纹理，而存在边界模糊问题。不同于自然图像中的对象，医学图像由于分辨率较低，可能没有明显的构造边界。 在没有专家知识的情况下，很难自动预测正确的目标区域  提出了一个新型的全自动医学图像分割框架，保留目标区域的结构边界。\n 边界点选择算法：自动选择最适合目标区域的关键点，这些点放置于目标对象的结构边界上； 边界保留块（BPB）：将点编码到网络中，进一步利用结构边界信息； Shape Boundary-aware Evaluator(SBE)：将专家知识嵌入分割模型；  在训练阶段：它试图利用关键图来评估分割图中有多少结构边界得到了很好的保存； 根据专家标注的 Ground truth，将预测区域反馈给分割网络。    任何分割模型都可以融合 BPB 和 SBE 来更精确的分割目标区域。\n论文的贡献总结如下：\n Boundaries key point selection algorithm SEB：将分割图与边界关键点图是否重合反馈给分割网络。 SOTA  Boundary Key Point Selection Algorithm 首先使用传统的边缘检测算法从 ground truth 分割图中得到目标物体的边界，随机选择 n 个点；然后依次连接 n 个点构造边界区域。为了获得 ground truth 的边界关键点图，测量边界区域之间的重叠区域的数量，通过 IoU 得到 ground-truth 的分割图。\n最后，选择使得 IOU 值最大的边界点作为结构边界关键点。\n为什么需要 4 个 BPB。\n论文有 4 个关键点：\n（1）Boundary Key Point Selection Algorithm\n（2）Boundary Preserving Block (BPB)\n（3）Boundary Key Point Map Generator\n（4）Shape Boundary-aware Evaluator (SBE)\nExperimental Results 在两个医学图像分割数据集上进行验证，\n PH2 + ISBI 2016 dataset：皮肤病灶分割 Transvaginal Ultrasound (TVUS) dataset：子宫内膜分割   什么是传统的边界检测算法（conventional edge detection algorithm）？\n我觉得作者应该是从标注的 Mask 获取到边界，不然的话整张图像的边界点太多了。\nA computational approach to edge detection.\n Canny, J. (1986). A Computational Approach to Edge Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8, 679-698.\nCited by: 38734（这惊人的引用量，ResNet 也就 3K）\n 彩色图像转为灰度图像的计算公式，RGB2Gray\n$$Gray = R \\times 0.299 + G \\times 0.587 + B \\times 0.114$$\nmatlab rgb2gray 0.299 * R + 0.587 * G + 0.114 * B 没有想到边缘检测（Edge Detection）也是一个很有意思的方向\n 一个开放的数据集（检索）下载地址：https://gas.graviti.com/open-datasets\nhttps://gas.graviti.cn/open-datasets\n  YOLOP：这是第一个可以在嵌入式设备 Jetson TX2上以 23 FPS 速度实时同时处理目标检测/可行驶区域分割和车道线检测这三个视觉感知任务并保持出色精度的工作。\n NVIDIA Pascal™ GPU 架构配有 256 个 NVIDIA® CUDA® 核心和高达 8GB 的内存。\n我可以考虑在这个嵌入式设备上部署 YOLOP。师兄的这个应该是 NVIDIA® Jetson Nano™，https://www.nvidia.cn/autonomous-machines/embedded-systems/jetson-nano/education-projects/。我下载了它的文档。\nYOLOP 可以同时完成 3 个任务： traffic object detection（交通目标检测）、 drivable area segmentation（可驾驶区域分割）和 lane detection（车道检测）。\n 本周看 YOLO 系列的论文，并且做好笔记！\n问题一：怎么知道物体的中心落在 grid cell 中？如何计算物体的中心？\n😘 中心点其实非常好求，去看看源代码，主要去看怎么处理 label 的那部分。\ncenter_x= (bbox[0]+bbox[2])*0.5 center_y= (bbox[1]+bbox[3])*0.5 https://zhuanlan.zhihu.com/p/183261974\n非极大值抑制，抑制的是与之重合的框。具体操作如下：\n 网络输出很多的预测框，选择置信度最高的框，肯定包含目标，其作为第一个框； 利用 IoU，把与第一个框重合的其他框抑制掉； 剩下还没有被抑制掉的框，取置信度最高的，得到第二个框；抑制重合框； 剩下的没有被抑制的框，取最高，得到第三个框，抑制重合框； ……直到没有剩下的框，结束。  多目标检测时，使用非极大值抑制。\nYOLOv1 输出为 shape=$7 \\times 7 \\times 30$。所以标签 label 的 shape 应该为 [batch, 7, 7, 30]。\nlabel 和 output 的 size 为：[batch_size, 7, 7, 30]。每个 output[bi, wi, hi] 是一个 30 维向量。\nYOLOv1 的检测头就是最好的 2 个全连接层，参数量很大，存在很大的改进空间。YOLOv1 一共预测 49 个目标，一共 98 个框。\nYOLOv2 归一化后的预测值为一个很小的偏移量，有利于神经网络的学习，并且使用偏移量会使得训练过程更加稳定。\nAnchor 是从数据集中统计得到的。\n什么是 DarkNet？\n答：这是 YOLO 作者自己写的一个深度学习框架。\nYOLOv3，3 个分支，32 倍下采样（大目标）、16 倍下采样、8 倍下采样（小目标）。\nLoss = 定位损失 + 置信度损失 + 分类损失\n什么是 geo_loss？\ngeo_loss + confidence_loss + class_loss。\n为什么 Head 变得越来越复杂了？\n答：因为特征提取网络变强了，能够支撑检测头做更加复杂的操作。\n可视化模型：https://netron.app/\n深入浅出Yolo系列之Yolov3\u0026amp;Yolov4\u0026amp;Yolov5\u0026amp;Yolox核心基础知识完整讲解：https://zhuanlan.zhihu.com/p/143747206\nyolo系列之yolo v3【深度解析】：https://blog.csdn.net/leviopku/article/details/82660381\n目标检测难的地方在于 pipeline 很长，细节很多，“the devil is in the detail”。\nYOLO 复现还得看大厂写的代码：https://github.com/Tencent/ObjectDetection-OneStageDet\n可以看看 DarkNet 的源码实现！\n1.5K 的解读，有非常多的中文注释：https://github.com/hgpvision/darknet\n17.2K，在官方的基础上添加了很多的新特性、新算法，新 backbone，是最流行的目标检测开源项目之一：https://github.com/AlexeyAB/darknet\n21.3K（作者源码）：https://github.com/pjreddie/darknet\n","permalink":"http://landodo.github.io/posts/20210831-structure-boundary-preserving-segmentation/","summary":"Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary 边界模糊医学图像的结构边界保持分割 简介 韩国一个大学的文章。 CVPR 2020 与医学图像分割有关 Cited by: 5 没有代码，没啥意思😭，想去看看它的","title":"Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary"},{"content":"目标检测任务（1） ⛳本报告主要是针对目标检测任务中正负样本不平衡、难易样本不平衡这两个问题进行简要讨论。\n论文简介  Lin, T., Goyal, P., Girshick, R.B., He, K., \u0026amp; Dollár, P. (2017). Focal Loss for Dense Object Detection. 2017 IEEE International Conference on Computer Vision (ICCV), 2999-3007.\n   Focal Loss\n  ICCV 2017, Best student paper.\n  https://arxiv.org/abs/1708.02002\n  1. 背景 以 2014 年为分界，目标检测的发展历程可以分为两大部分：传统目标检测时期、基于深度学习的目标检测时期。\n Zou, Z., Shi, Z., Guo, Y., \u0026amp; Ye, J. (2019). Object Detection in 20 Years: A Survey. ArXiv, abs/1905.05055.\n 在基于深度学习的目标检测算法中，又可以分为单阶段（One/Single-stage）和两阶段（Two-stage）两种大类。当然还有多阶段（Multi-stage），但是其速度和精度都比较低，已经被淘汰了。\n1.1 One-Stage and Two-stage One-stage 和 Two-stage 的主要区别在于受否存在 Region Proposal（可能包含待检物体的预选框）操作。以 Two-stage 方法中的代表 Faster RCNN 为例，算法会先生成候选框（Region proposals，可能包含物体的区域），然后再对每个候选框进行分类和修正位置；而 One-stage 算法会直接在网络中提取特征来预测物体分类和位置。\n两种方法都存在各自的优缺点。一般来说，One-stage 方法在速度上存在优势，但是在精度上会差于 Two-stage，主要原因可以总结为：正负样本不平衡（和难易样本不平衡）造成了 One-stage 方法在精度上的劣势。具体分析如下：\n  One-stage 网络最终学习的 Anchor 有很多，但是只有少数 Anchor 对最终网络的学习是有利的，而大部分 Anchor 对最终网络的学习都是不利的，这部分的 Anchor 很大程度上影响了整个网络的学习，拉低了整体的准确率；\n  Two-stage 网络最终学习的 Anchor 虽然不多，但是背景 Anchor 也就是对网络学习不利的 Anchor 也不会特别多，它虽然也能影响整体的准确率，但是肯定没有 One-stage 影响得那么严重，所以它的准确率比 One-stage 肯定要高。\n  对于正负样本不平衡问题，是比较好解决的，也存在不少的现有方法。Focal Loss 的提出，主要针对难易样本的不平衡问题。有了 Focal Loss，训练过程关注对象的次序为：（正/难） \u0026gt; （负/难） \u0026gt; （正/易） \u0026gt; （负/易）。该损失函数通过抑制那些容易分类样本的权重，将注意力集中在那些难以区分的样本上，有效控制正负样本比例，防止失衡现象。\n============================================================\n在开始介绍 Focal Loss 之前，我补充一些目标检测的基础知识，我也是才刚开始学习目标检测。\n（1）目标检测中的各种“框”\n ground truth：标注框 Anchor：人为设置的初始先验框 proposal：RPN 的输出（可能包含物体的候选框），即对 Anchor 第一次做回归得到的结果 RoI：RPN 阶段输出的 Proposal 经过排序取 topK，然后做 NMS 取一定数量的框，用于第二阶段的再次精修 bounding box：proposal 经过再次精修后的预测框，由于计算 AP，AP 指的是 bounding box AP。  （2）目标检测任务的评估指标 mAP\nhttps://www.zhihu.com/question/53405779/answer/993913699\nAP：PR 曲线下面积，先考虑计算 AP，即一个类别。\n 7 张图像（假设是一个 Batch），15 个 ground truth，24 个预测框 （1）计算预测框是 TP or FP（计算 bbox 与 Ground truth 的 IoU，根据阈值判断），如果一个 Ground Truth 有多个预测框，则 IoU 最大为 TP，其他为 FP； （2）根据置信度从大到小排序所有的预测框； （3）计算 Precision = TP / (ACC_TP + ACC_FP)、Recall = TP / (all grouth truth)；  ACC 表示累加，all ground truth 是一个固定值   （4）绘制 PR 曲线； （5）计算曲线下的面积，11 个点 [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] 的插值进行计算（10 个矩形的面积之和） （6）所有类别的 AP 计算都分别出来，然后求取平均得到 mAP。  2. Focal Loss 先从二分类的交叉熵开始：\n $y \\in {\\pm 1}$ 代表 ground truth（真实值） $p \\in [0, 1]$ 表示模型输出标签为 1 的概率（预测值）  定义一个 $p_t$：\n则交叉熵可以写为 $CE(p, y) = CE(p_t) = -log(p_t)$\n交叉熵损失存在一个问题，如 Figure 1.，即使那些很容易 easily classified 的样本（$p_t \u0026raquo; 0.5$），仍会造成很显著的损失值。当这些 easy example 数量庞大时，其累计起来的损失可能会远远大于（overwhelm）那些 rare class。\n 我认为这很类似于政治上的民主暴政。\n 2.1 Balanced Cross Entropy 解决正负样本不平衡的常用方法是对类别 1 引入权重因子 α∈[0,1]，对类别 -1 引入权重因子 1-α。得到 α-balanced CE loss：\n即，\n2.2 Focal Loss Easily classified negatives comprise the majority of the loss and dominate the gradient. α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples.\n因此，Focal Loss 的主要目标是 down-weight easy example，使得能够 focus training on hard example。Focal Loss 定义如下：\n $(1 - p_t)^{\\gamma}$ 是 modulating factor（调节因子） $\\gamma$ 是一个可调节的超参数，focusing parameter（聚焦参数）  Focal Loss 有两个性质：\n（1）当一个样本出现了错误分类，且 $p_t$ 非常小，则 $(1 - p_t)^{\\gamma}$ 非常接近 1，Loss 不受影响；当 $p_t$ 接近于 1 时，$(1 - p_t)^{\\gamma}$ 接近 0，对于 well-classified examples 的 Loss 将会降低权重（down-weighted）。因此模型的 Loss 就集中在那些错误分类样本上了（hard example）。\n（2）Focusing parameter $\\gamma$ 用于调整简单的样本（easy examples, well-classified examples）的 Loss 降低权重（down-weighted）的速率。$\\gamma=0$ 时，FL == CE；$\\gamma = 2$ 是实验得到的最好值。\nIntuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss.\n例如：\n $\\gamma = 2$，一个样本的分类结果为 $p_t = 0.9$（这就是一个 easy example），则 $(1 - 0.9)^{2} = 0.01$。Focal Loss 比 CE 小 100 倍； $p_t = 0.968$ 时，FL 比 CE 小近 1000 倍。 同理，$p_t = 0.4$ 时（hard example），则 $(1 - 0.4)^{2} = 0.36$。相当于变相给错误分类的难样本的 Loss 增加了权重。“increases the importance of correcting misclassifified examples”  为了平衡正负样本，使用 α 权重，得到最终的 Focal Loss 表达式：\nFL 更像是一种思想，其精确的定义形式并不重要。\n在 Two-stage 方法中，对于正负样本不平衡问题，主要是通过如下方法缓解：\n （1）object proposal mechanism：reduces the nearly infifinite set of possible object locations down to one or two thousand. （2） biased sampling：1:3 ratio of positive to negative examples.  同时，在模型初始化时，可以加入一下先验知识，可以缓解训练初期的不稳定现象。FL 通过直接通过损失函数解决！\nFocal Loss 的代码可以参考 MMdetection：https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/losses/focal_loss.py\n3. RetinaNet 为了验证 Focal loss 的有效性，设计了一个叫 RetinaNet 的网络进行评估。实验结果表明，RetinaNet 能够在实现保持 one-stage 速度优势的基础上，在精度上超越所有（2017 年）two-stage 的检测器（ achieves state-of-the-art accuracy and run time on the challenging COCO dataset）。\nRetinaNet 的卷积过程用的是 ResNet，上采样和侧边连接还是 FPN 结构。通过主干网络，产生了多尺度的特征金字塔。然后后面连接两个子网，分别进行分类和回归。\n4 实验结果 作者做了很多消融学习，可以总结如下：\n 作者有很多卡； $\\alpha$ 和 $\\gamma$ 这两个超参数是互相影响的； Focal loss 的威力还是很大的； $\\gamma=2, \\alpha=0.25$ 时，ResNet-101+FPN 作为 backbone 的结构有最优的性能；  下图是收敛模型中不同 γ 值的正负样本归一化损失的累积分布函数。\n改变 γ 对正样本的损失分布的影响很小。然而，对于负样本，增加 γ 会使得模型几乎所有的注意力从负样本上离开，实现了 down-weight easy example。\nFocal Loss 使得 One-stage 方法在精度上超越了 Two-stage 方法。\nFocal Loss 的缺点：在速度上还存在很大的改进空间。\n总结   One-stage 方法相比于 Two-stage 方法，在精度稍有劣势。研究发现，是正负样本不平衡和难易样本不平衡这两个问题所导致的；\n  Focal Loss 函数通过抑制那些容易分类样本的权重，将注意力集中在那些难以区分的样本上，有效控制正负样本比例，防止失衡现象。\n  Focal Loss 的主要目标是 down-weight easy example，使得模型能够 focus training on hard example。\n  具体做法是，对于难易样本不平衡问题，引入 modulating factor $(1 - p_t)^{\\gamma}$， Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss.\n  为了验证 Focal Loss 的有效性，设计了 RetinaNet 用于实验评估。\n  RetinaNet 能够在实现保持 one-stage 速度优势的基础上，在精度上超越所有（2017 年）two-stage 的检测器（ achieves state-of-the-art accuracy and run time on the challenging COCO dataset）。\n  RetinaNet 也存在缺点，其在速度上仍有很大的改进空间。\n  扩展学习 Focal Loss 存在缺点：\n 让模型过多关注那些特别难分的样本肯定是存在问题的，样本中有离群点（outliers），可能模型已经收敛了但是这些离群点还是会被判断错误，让模型去关注这样的样本，可能对最后的结果造成不利的影响； $\\alpha$ 和 $\\gamma$ 互相影响，全凭经验得到（不同的数据集都要寻找的最佳的 $\\alpha, \\gamma$，代价昂贵）； 速度上仍存在改进空间。  （1）GHM(gradient harmonizing mechanism) 解决了上述前两个问题。Focal Loss 是从置信度 p 的角度入手衰减 Loss，而 GHM 是一定范围置信度p的样本数量的角度衰减 Loss。\n（2）Generalized Focal Loss，不会带来额外的 Cost，提升 1% 的 AP。\n","permalink":"http://landodo.github.io/posts/20210827-focal-loss/","summary":"目标检测任务（1） ⛳本报告主要是针对目标检测任务中正负样本不平衡、难易样本不平衡这两个问题进行简要讨论。 论文简介 Lin, T., Goyal, P., Girshick, R.B., He, K., \u0026amp; Dollá","title":"Focal Loss for Dense Object Detection"},{"content":"医学影像基础 现行医学影像设备所得到的医学影像在许多方面还存在不尽如人意之处。例如，某些影像的==空间分辨率不够高==，某些影像的==信噪比不够好==，某些影像==需要多模态融合==才能更好地进行诊断，等等。\n医学影像学部分主要涵盖了 X 线、CT、MRI、超声、核素显像五类医学影像。\n0 医学图像中的一些专业术语   X-ray：X 射线成像\n  Ultrasound：超声成像\n  Positron emissions tomography, PET：正电子发射断层扫描成像\n  Computed tomography, CT：计算机断层扫描成像\n  Magnetic Resonance Imaging, MRI：磁共振成像\n   缺点：贵、慢、吵\n     Computer-aided Diagnosis, CAD：计算机辅助诊断\n  MRI T1, T2\n   MRI 可以实现多变的图像对比度，该过程的实现是通过使用不同的脉冲序列和改变成像参数对应纵向松弛时间（T1）和横向松弛时间（T2），T1 加权和 T2 加权成像的信号强度与特定组织的特征有关。MR 成像中，图像的对比度依赖于相位对比脉冲序列参数，最常见的脉冲序列是 T1 加权和 T2 加权自旋回波序列．身体的 MR 成像是为了观察大脑、肝脏、胸、腹部和骨盆的结构细节，这有利于诊断检测或治疗。\n     Medical Image Computing and Computer Assisted Intervention, MICCAI：医学图像计算与计算机辅助干预\n  Emission Computed Tomography, ECT：发射型计算机断层成像技术\n  Positron Emission Tomography, PET：正电子发射型断层成像\n   属于核素显像\n     Optical Coherence Tomography, OCT：相干光断层成像\n   眼底血管图像\n     Cone-Beam Computed Tomography, CBCT：锥状射束计算机断层扫描\n  1. X 射线 X 线是由高速运行的电子群撞击物质突然受阻时产生的。撞击发生能量转换，其中仅约 1% 的能量形成 X 线，其余 99% 左右的能量则转换为热能，由散热设施散发。\nX 线与临床医学成像有关的主要特性如下。\n（1）穿透\n 波长越短，X 线的穿透作用越强 X 线对人体各种组织结构穿透力的差别是X线成像的基础。  （2）荧光\n X 线作用于荧光物质，使波长短的 X 线转换成波长较长的可见荧光。  （3）感光\n（4）电离\n（5）生物效应\n 生物细胞在一定量的 X 线照射下，可产生抑制、损害甚至坏死，称为 X 线的生物效应，是放射治疗学的基础  X 线成像原理\n1）X 线能使人体在荧光屏上或胶片上形成影像，必须具备三个基本条件：① X 线要具备一定的==穿透力==；② 被穿透的组织结构必须==存在密度和厚度的差异==，从而导致穿透物质后剩余 X 线量的差别；③ 有差别的剩余 X 线量，仍为不可见的，必须经过载体（如X线片、荧屏等）的过程才获得有黑白对比、层次差异的X线影像。\n2）人体组织结构根据其密度的高低及其对X线吸收的不同可分三类：① ==骨骼==比重高、吸收 X 线量多，X 线片上骨骼部位显示白色，称为高密度影像；② ==软组织==包括皮肤、肌肉、结缔组织等，彼此之间密度差别不大，X 线片上显示灰白色，称为中等密度影像；脂肪及气体，脂肪组织较一般软组织密度低，在 X 线片上显示灰黑色；③ ==气体==吸收 X 线最少，在 X 线片上呈深黑色，称为低密度影像。\n⛳吸收 X 射线越多（密度越高），CT 图像越亮！\nX 线成像技术\n 普通 X 线摄影 数字化 X 线摄影技术：计算机 X 线摄影（computed radiography，CR）和数字 X 线摄影（digital radiography，DR） 特殊 X 线摄影  X 线成像的临床应用\n 胸部病变 腹盆部病变 骨关节病变  2. CT 诊断 CT 解决了普通 X 线摄影不能解决的很多问题。CT 图像是真正的断层图像。CT 图像相对空间分辨率高，解剖关系明确，病变显影更好。\n基本概念\nCT 图像是真正的断面图像，它显示的是人体某个断面的组织密度分布图。CT 以 X 线作为投射源，由探测器接收人体某断面上的各个不同方向上人体组织对 X 线衰减值，经模/数转换输入计算机，通过计算机处理后得到扫描断面的组织衰减系数的数字矩阵，然后将矩阵内的数值通过数/模转换，用黑白不同的灰度等级在荧光屏上显示出来。CT 图像具有图像清晰，密度分辨率高，无断面以外组织结构干扰等特点。\nCT 图像是人体某一部位有一定厚度的体层图像。\nCT 值：体素的相对 X 线衰减度（即该体素组织对 X 线的吸收系数），表现为相应像素的 CT 值，单位名称为Hu（Hounsfield Unit，Hu）。规定以水的CT值为0Hu，骨皮质最高，为1000Hu。人体组织的CT值界限可分为2000个分度，上界为骨的CT值（1000Hu），下界为空气的CT值（-1000Hu）\n人体组织CT值范围有2000个分度（-1000～+1000），如在荧屏上用2000个不同灰阶来表示2000个分度，由于灰度差别小，人眼不能分辨（一般仅能分辨16个灰阶）。为了提高组织结构细节的显示，使CT值差别小的两种组织能够分辨，则要采用不同的==窗宽==来观察荧屏上的图像。\n伪影：是指在被扫描物体中并不存在的而图像中却显示出来的各种不同类型的影像。病人不自主运动及病人躁动可产生伪影。另外，病人体内高密度的异物也可形成伪影。\n成像原理\n（1）X线扫描数据的收集和转换\nX线射入人体后，因被人体吸收而衰减，其衰减的程度与受检层面的组织、器官和病变的密度（原子序数）有关，密度越高，对X线衰减越大。\n（2）扫描数据处理和重建图像\n（3）图像的显示及贮存\nCT 临床应用\n 中枢神经系统病变 头颈部病变 胸部病变 腹盆部病变  3. MRI 诊断 MRI 技术原理和基本概念\n通过对静磁场中的人体施加某种特定频率的视频脉冲，使人体组织中的氢质子受到激励而发生磁共振现象，当终止射频脉冲后，质子在弛豫过程中感应出MR信号；经过对MR信号的接收、空间编码和图像重建等处理过程，即产生MR图像，这种成像技术就是MRI技术。\n人体内氢核丰富，而且用它进行磁共振成像的效果最好，因此，目前MRI常规用氢核来成像。\nMRI 图像特点\n（1）多参数成像：成像参数主要包括 T1、T2 和质子密度等，\n（2）多方位成像：MRI可获得人体轴位、冠状位、矢状位及任意倾斜层面的图像，有利于解剖结构和病变的三维显示和定位。\n（3）流动效应\n（4）质子弛豫增强效应与对比增强\nMRI 的优点和限制\nMRI的优点：（1）无X线电离辐射，对人体安全无创；（2）图像对==脑和软组织==分辨率极佳，解剖结构和病变形态显示清晰；（3）多方位成像，便于显示体内解剖结构和病变的空间位置和相互关系；（4）多参数成像；（5）除可显示形态变化外，还能进行功能成像和生化代谢分析。\nMRI 的限制：（1 ）对带有心脏起搏器或体内有铁磁性物质的患者不能进行检查；（2）需要监护设备的危重患者不能进行检查；（3）对钙化的显示远不如CT，难以对以病理性钙化为特征的病变作诊断；（4）常规扫描时间较长，对胸腹检查受限；（5）对质子密度低的结构如肺和骨皮质显示不佳；（6）设备昂贵，尚未普及。\nMRI成像中的伪影\nMRI图像中的假影像为伪影，常见的伪影有装备伪影、运动伪影、金属异物伪影等。\nMRI的临床应用\n 中枢神经系统病变 头颈部病变 胸部病变 腹盆部病变 骨关节病变  4. 超声诊断 利用超声在人体器官组织传播过程中产生透射、折射、反射等的信息，加以接收、放大和处理形成曲线的方法，称为超声诊断。超声波在生物组织中的传播规律是超声诊断的基础，对超声诊断最重要的生物组织是软组织和血液。当超声经过不同性质的软组织和血液或当组织发生病理变化时，其在组织器官中的传播发生相应的改变，最终体现为超声曲线或图像上的差异。\n超声临床应用\n 乳腺病变 甲状腺超声诊断 腹部病变超声诊断  5. 核素显像诊断 核医学（nuclear medicine）显像是显示放射学核素标记的放射性药物在体内的分布图。放射性药物根据自己的代谢特点和生物学特性，能特异地分布于体内特定的器官或病变组织，并参与体内的代谢，标记在放射性药物分子上的放射性核素由于放出射线能在体外被检测。\n显像类型与特点\n（1）根据影像获取的状态分为静态显像（static imaging）和动态显像（dynamic imaging）\n（2）局部显像（regional imaging）和全身显像（whole body imaging）\n（3）早期显像（early imaging）和延迟显像（delay imaging）\n（4）热区显像（hot spot imaging）和冷区显像（cold spot imaging）\n（5）静息显像（rest imaging）和介入显像（interventional imaging）\n临床应用\n 骨骼系统 内分泌系统 泌尿系统  PET 成像 PET显像是利用人体正常结构组织含有的必需元素，葡萄糖、氨基酸、胆碱、胸腺嘧啶、受体的配体及血流显像剂等药物为显像剂，以解剖图像方式从分子水平显示机体及病灶组织细胞的代谢、功能、血流、细胞增殖和受体分布状况等，为临床提供更多的生理和病理方面的诊断信息\n 向人体内注射带有放射性核素的药剂（例如18F-氟代脱氧葡萄糖）后，在一段时间内肿瘤区域的放射性核素浓度便会明显高于正常组织。通过捕捉这些放射性核素发出的信号，我们便能准确地定位肿瘤的区域与大小，方便医生开展进一步的诊疗工作。\n PET显像所用的放射性药物是一类采用正电子核素标记的显像剂（正电子核数多为机体组成的基本元素）。\nPET/CT：由于CT提供的是人体解剖结构的信息，因此称为结构成像；PET提供的主要是目标区域的功能信息，称为功能成像。两者的融合能够得到更多的病理信息。\n","permalink":"http://landodo.github.io/posts/20210806-medical-image-basic/","summary":"医学影像基础 现行医学影像设备所得到的医学影像在许多方面还存在不尽如人意之处。例如，某些影像的==空间分辨率不够高==，某些影像的==信噪比不","title":"医学图像基础"},{"content":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。\n1. 聚类问题简介 聚类是针对给定的样本，依据它们的特征的相似度或距离，将其归并到若干“类”或“簇”中。其目的是通过得到的类或簇来发现数据的特点或对数据进行处理。聚类作为经典的无监督学习算法，在数据挖掘/机器学习等领域有着广泛地应用。\n两种最常用的聚类算法：层次聚类和 K 均值聚类。\n 层次聚类：将每个样本各自分到一个类；之后将相聚最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化类别结构。 K-Means 聚类：选择 K 个类别的中心，将样本逐个指派到与其最近的中心的类中，得到一个聚类结果；然后更新每个类的样本的均值，作为类的新的中心；重复以上步骤，直到收敛为止。  K-Means 被选为数据挖掘十大经典算法之一。\n图神经网络已经成为深度学习领域最热门的方向之一，那么，如何利用图神经网络强大的结构捕获能力来提升聚类算法的精度呢？深度聚类是聚类方法的一种，它采用深度神经网络来学习聚类友好表征。\n2. 相关综述 综述文献：[1][2]\n深度聚类算法（Deep Clustering Algorithm）可以分解为三个基本组成部分：\n 深度神经网络 网络损失 $L_n$ 聚类损失 $L_c$  3. 论文精读 论文名称：Attributed Graph Clustering: A Deep Attentional Embedding Approach [3]\n发表期刊：International Joint Conference on Artificial Intelligence (IJCAI-19)\n论文地址：https://arxiv.org/abs/1906.06532\n摘要 最近的研究大多集中在使用深度学习方法来学习一个紧凑的图形嵌入（embedding），在此基础上应用经典的聚类方法（如 K-means）来完成聚类任务。这种两阶段的方法通常无法取得更好的结果，因为其图嵌入不是以目标为导向的，即此深度学习方法并不是为聚类任务而设计的。\n本篇论文提出一种以目标为导向的深度学习方法：Deep Attentional Embedded Graph Clustering (DAEGC)。这种方法包含三个主要核心点：\n（1）注意力机制的图自编码器（Graph Attentional Autoencoder）\n（2）自训练的图聚类（Self-optimizing Embedding）\n（3）自训练过程与图嵌入共同学习和优化（Joint Embedding and Clustering Optimization）\nDAEGC 在 Cora、Citeseer、Pubmed 数据集上都取得了最好的聚类结果。\nIntroduction 对于图聚类问题，其中的关键是如何捕捉结构关系和节点内容信息。很多近期的研究通过深度学习方法学习到节点的 Embedding，再利用简单的聚类算法（如 K-Means）进行聚类。\n很显然，这是一种两个阶段的方法（非目标为导向），其存在着如下的缺点：学习到的 Embedding 可能不是最适合随后的图聚类任务，并且图聚类任务对图的嵌入学习没有帮助。\n传统的以目标为导向的方法大多针对的是分类任务（监督学习，如图卷积实现分类）。\n本论文提出了一种以目标为导向的图注意力自动编码器的图聚类框架。Figure 1 显示了其与两阶段方法的不同，模型学习嵌入并同时在一个统一的框架内进行聚类，从而获得更好的聚类性能。\nModel 如 Figure 2 所示模型主要由两个模块构成：\n（1）Graph attentional autoencoder\n自动编码器将节点属性值和图结构作为输入，并通过最小化重建损失来学习潜在的嵌入。\n（2）Self-training clustering\n自训练模块根据学习到的表征进行聚类，反过来，根据当前的聚类结果来操作潜在的表征。\n在一个统一的框架内学习图的嵌入和聚类，这样每个部分都能使对方受益。接下来对模型的细节进行分析。\n（1）Graph Attentional Autoencoder Graph Attention Encoder\n使用一个图注意力网络（GAT）的变体作为 Graph Encoder，其核心是通过关注其邻居来学习每个节点的隐藏表征，将节点特征与潜在表征中的图结构相结合。注意力机制对邻居的表示给予不同的权重。\n $z^{l+1}_{i}$​：节点 $i$​ 的输出表征（新特征）； $N_i$​：节点 $i$​ 的所有邻居节点； $\\alpha_{ij}$​：注意力系数，衡量节点 $j$​ 对节点 $i$​​ 的重要程度； $\\sigma$​​：非线性激活函数。  注意力系数 $\\alpha_{ij}$​ 可以表示为一个单层前馈神经网络，$x_i$ 和 $x_j$ 表示节点 $i$ 和 $j$ 的特征向量。\n $c_{ij}$​ 是一个标量，衡量节点 $j$​ 对节点 $i$​ 的重要程度。  图注意网络（GAT）只考虑了一阶邻居， 由于图具有复杂的结构关系，本篇论文的编码器中利用高阶邻居，通过考虑图中的 t 阶邻居节点来获得一个接近矩阵：\n $B$ 是一个转移矩阵（非负，各行元素之和等于 1），如果节点 $i$ 和 $j$ 之间存在边，则 $B_{ij} = 1/d_i$，否则 $B_{ij} = 0$。$d_{i}$ 为节点 $i$​​ 的度； $M_{ij}$​ 表示节点 $i$ 和节点 $j$ 之间的拓扑相关性（$t$​ 阶邻居）； $N_i$ 指 $M$ 中 $i$ 的邻居节点，如果 $M_{ij} \u0026gt; 0$，则表示 $j$ 是 $i$​​​ 的邻居节点； $t$：可以针对不同的数据集灵活地选择t，以平衡模型的精度和效率。  注意力系数通常在所有邻域 $j∈N_i$​​ 中用一个 softmax 函数进行归一化，以使它们易于在各节点间进行比较。\n加上拓扑权重 M 和激活函数 δ（LeakyReLU），注意力系数可以表示为：\n$x_i = z^{0}_{i}$​ 作为输入，堆叠两个图注意力层：\n图注意力编码器将结构和节点特征编码成一个隐藏的表示，得到：$z_i = z^{(2)}_{i}$。\nInner Product Decoder\n解码器（Decoder）可以进行如下分类：重建图结构、重建节点特征属性、两种都重建。由于公式（7）获取到的潜伏嵌入（latent embedding）已经包含了内容和结构信息，因此本篇论文选择采用一个简单的内积解码器来预测节点之间的联系：\n重建损失 Reconstruction Loss\n通过衡量 $A$ 和 $\\hat{A}$ 之间的差异性来最小化重构损失：\n（2）Self-optimizing Embedding 图聚类任务是无监督的，因此在训练期间无法获得关于所学嵌入是否得到很好优化的反馈。即 GAE 所学习到的节点表示只是为了更好的重构网络结构，和聚类并没有直接联系。针对此问题，本篇论文提出了 一种自优化的嵌入算法作为解决方案，对 GAE 所学习到的 embedding 进行约束和整合，使其更适合于聚类任务。\n除了优化重建误差，还将隐藏嵌入（hidden embedding）输入到一个自优化的聚类模块中，该模块最小化了以下目标：\n $q_{iu}$ ：衡量节点的 embedding $z_{i}$​ 和聚类中心 embedding $\\mu_u$ 的相似度  使用学生分布（Student\u0026rsquo;s $t$​​-distribution）来衡量，以处理不同规模的集群。（11）式中，聚类中心 embedding 为 $\\mu_u$​​​，则节点 $i$​​​​ 属于某个类别的概率为 $q_{i u}$:\n T 分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值。\n $q_{i u}$ 可以被看作是每个节点的软聚类分配分布。为了引入聚类信息来实现聚类导向的节点表示，我们需要迫使每个节点与相应的聚类中心更近一些，以实现所谓的类内距离最小，类间距离最大（二次方后，分布会变得更加尖锐，也更置信）。定义目标分布 $p_{iu}$​ ：\n 类别 $k$  目标分布 P 将 Q 提高到二次方，以强调这些“自信的分配”的作用。然后，聚类损失迫使当前分布 Q 接近目标分布 P，以便将这些“有信心的分配”设置为软标签（“works as ground-truth labels”），监督 Q 的嵌入学习。\n将聚类损失降到最低，以帮助自动编码器利用嵌入物自身的特性操纵嵌入空间。\n（3）Joint Embedding and Clustering Optimization 共同优化自动编码器嵌入和聚类学习，总目标函数定义为：\n  $L_r$：Reconstruction loss\n  $L_c$：Clustering loss\n  $\\gamma \u0026gt;= 0$：控制两者的平衡\n  可以直接从最后一个优化的 Q 中获得聚类结果，即对节点 $i$ 的所处的簇为：\nDeep Attentional Embedded Graph Clustering Algorithm\nExperiments 三个数据集\n总结  DAEGC: an unsupervised deep attentional embedding algorithm; 在一个统一的框架内，联合进行图聚类和学习图嵌入； 学习到的图嵌入整合了结构和内容信息，并专门用于聚类任务； 针对聚类这个无监督学习任务，提出了一个自训练的聚类组件，从“置信”的分配中生成软标签，以监督嵌入的更新； 聚类损失和自动编码器重建损失被联合优化，以同时获得图嵌入和图聚类的结果； 将实验结果与各种最先进的算法进行比较，验证了 DAEGC 的图聚类性能。  References [1] Aljalbout, E., Golkov, V., Siddiqui, Y., \u0026amp; Cremers, D. (2018). Clustering with Deep Learning: Taxonomy and New Methods. ArXiv, abs/1801.07648.\n[2] Erxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing Cui, and Jun Long. A Survey of Clustering with Deep Learning: From the Perspective of Network Architecture. DOI: 10.1109/ACCESS.2018.2855437, IEEE Access, vol. 6, pp. 39501-39514, 2018.\n[3] Wang, C., Pan, S., Hu, R., Long, G., Jiang, J., \u0026amp; Zhang, C. (2019). Attributed Graph Clustering: A Deep Attentional Embedding Approach. IJCAI.\n[4] https://deepnotes.io/deep-clustering\n Can deep neural networks learn to do clustering? Introduction, survey and discussion of recent works on deep clustering algorithms.\n [5] Papers List (1.2k Stars) https://github.com/zhoushengisnoob/DeepClustering\n Deep Clustering: methods and implements\n [6] 图神经网络时代的深度聚类：https://zhuanlan.zhihu.com/p/114452245\n","permalink":"http://landodo.github.io/posts/deep-clustering-notes/","summary":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。 1. 聚类问题简介 聚类是针对给定的样本，依据它们的","title":"深度聚类 Deep Clustering"},{"content":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。\n Hugo 默认是使用 TOML，现在将此更改为更易阅读的 YAML。 Hugo 需要在源目录查找一个 config.toml 的配置文件。如果这个文件不存在，将会查找 config.yaml，然后是 config.json\n 添加站内搜索 Search Page 全局文章内容搜索，从关键字快速定位到文章。\nHogu 文档：https://adityatelange.github.io/hugo-PaperMod/\nPaperMod uses Fuse.js Basic for seach functionality\nAdd the following to site config, config.yml\noutputs:  home:  - HTML  - RSS  - JSON # is necessary Create a page with search.md in content directory with following content\n--- title: \u0026#34;Search\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # is necessary # url: \u0026#34;/archive\u0026#34; # description: \u0026#34;Description for Search\u0026#34; summary: \u0026#34;search\u0026#34; --- To hide a particular page from being searched, add it in post’s fron’t matter\n--- searchHidden: true copy\nex: [search.md]\n添加 Archives 功能 这是自动的，只需要添加一个按钮，其他的就不用管了。\nhttps://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n增加公式支持 此功能会拖慢网站的加载速度，这个以后再说！些许的加载时间能换取良好的阅读体验，目前来看是值得的。我的论文阅读笔记里充斥了大量的 LaTeX 公式。\n公式支持基于 MathJax。\nhttps://mertbakir.gitlab.io/hugo/math-typesetting-in-hugo/\n行内公式：\n给定一组训练样本 $\\mathcal{D}$，TumorCP 有 $(1 - p_{cp})$​​​ 的概率不执行任何操作；有 $p_{cp}$​ 的概率从 $\\mathcal{D}$ 中采样出一个对图像 $(x_{src}, x_{tgt}) \\sim \\mathcal{D}$​​，并执行一次 “Copy-Paste”。\n令 $\\mathcal{O}{src}$ 为图像 $x{src}$ 上肿瘤集合，$\\mathcal{V}{tgt}$ 为 $x{tgt}$ 上的器官的体积坐标集合，$\\mathcal{T}$ 是一组随机数据转换，每个转换都有一个称为 $p_{trans}$​ 的概率参数。\n一次 “Copy-Paste” 流程：\n TumorCP 首先采样一个肿瘤 $o \\sim \\mathcal{O}{src}$​、一组数据转换 $\\tau \\sim \\mathcal{T}$​ 和一个目标位置 $v \\sim \\mathcal{V}{tgt}$； 然后将 $τ(o)$ 以 $v$​ 为中心，取代原始数据和标注。  行间公式：\n$$\\sqrt{x} + \\sqrt{x^{2}+\\sqrt{y}} = \\sqrt[3]{k_{i}} - \\frac{x}{m}$$\n测试 Emoji 🧡💥💢💌💝🕎☪\n测试代码 import sys from PyQt5 import QtCore, QtGui, QtWidgets, uic from PyQt5.QtCore import Qt, QEvent import random  from PyQt5.QtGui import QPixmap from PyQt5.QtWidgets import QAction, QFileDialog   class Canvas(QtWidgets.QLabel):  def __init__(self, parent=None):  super().__init__(parent)  self.background = QPixmap(200, 200)  self.background.fill(Qt.yellow)  # self.clear(Qt.yellow)  self.last_x, self.last_y = None, None  self.pen_color = QtGui.QColor(\u0026#39;#000000\u0026#39;)  self.setPixmap(self.background)   def mouseReleaseEvent(self, e):  self.last_x = None  self.last_y = None 图片的显示 这里需要将图片拷贝的 public 对应的文章目录下，使用原本的相对路径。不需要将文件夹拷贝到 content/posts/ ，这个文件夹下至保存 Markdown 文件。\n使用 hugo 生成文章的 HTML 文件后，每一篇文章都会以 Markdown 文件中的 title 生成一个文件夹。如本篇文章的文件夹名称为 first-post （全小写）。\n在访问时，使用的是如下的链接：\nhttps://landodo.github.io/first-post/0000465981_017.jpg 如果图片实现依旧不正常，使用 F12 进行调试，快速定位问题所在。\n","permalink":"http://landodo.github.io/posts/first-post/","summary":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。 Hugo 默认是使用 TOML，现在将此更改为更易阅","title":"First Post for Hugo"}]