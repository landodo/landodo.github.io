[{"content":"学习计算机的过程中，那些优秀的资料、网站。\n","permalink":"http://landodo.github.io/cs-zoo/","summary":"学习计算机的过程中，那些优秀的资料、网站。","title":"Cs Zoo"},{"content":"关于我\n","permalink":"http://landodo.github.io/about/","summary":"关于我","title":"About"},{"content":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介：\n 作者：Chufeng Tang（https://chufengt.github.io/） CVPR 2021 单位：清华大学计算机科学与技术系人工智能研究所 代码：https://github.com/tinyalpha/BPR  思考：\n（0）实例分割精度要想进一步提升，应该在哪里下功夫？\nTable 1 显示了边界修复所带来的性能提升具有巨大的潜力。\n（1）由于硬件的限制，分割时一般需要 Resize 或切 Patch。那么，应该怎么切patch呢？\n  随机切固定size的patch；\n  按网格切；\n  按实例所在位置切；\n  ✅先进行一个粗分割，然后以粗分割的边界为中心，提取Patch块。\n  （2）High-level information/Low-level information？\n高层次语义信息用于提供定位和粗略分割实例（localize and roughly segment objects）；低层次语义信息用于对于分割局部边界细节更为关键。在单一模型中权衡两者是非常困难的。\n（3）本文提出的BPR作为一种后处理方案，目前已有的后处理框架是怎么做的？\n设计一个边界感知的分割模型是一个分割任务的研究热点。主要的有两个方向：\n 集成一个额外的、专门的模块(分支)来处理边界； 基于现有分割模型的结果和后处理方案来细化边界  PolyTransform：将实例的轮廓转换为一组多边形顶点，应用基于Transformer的网络来预测顶点向对象边界的偏移；缺点：巨大的实例patch和复杂的Transformer架构。     注：PolyTransform(CVPR 2020)\n Abstract 由于特征图的空间分辨率较低，以及边界像素比例极低导致的不平衡问题，预测分割图的边界通常不精确。为了解决这个问题，本论文提出了一个概念上简单但有效的后处理改进框架，将实例分割模型的结果进行边界质量改进，称为 BPR。启发自看得更近，分割边界更好（looking closer to segment boundaries better），沿着预测的实例边界提取并细化一系列小边界 patch。通过更高分辨率的边界 patch 细化网络（BPR）。BPR 框架在 Cityspace 基准上，尤其是在边界感知指标上，比 Mask R-CNN 基线有显著改进。将 BPR 框架应用于“PolyTransform+SegFix”基线后，在 Cityspace 排行榜上取得了 SOTA。\n1 Introduction 分割所面临的最重要的问题之一是实例边界分割不精确（Figure 1 Left）。校正物体边界附近的误差像素可以大大提高 Mask 质量（Table 1），对于较小的物体，在一定的欧几里德距离(1px/2px/3px)范围内的像素可以获得较大的增益（AP 为9.4/14.2/17.8）。\n导致低质量边界分割的关键问题有两个：\n（1）特征图空间分辨率的降低使得物体边界周围的细节消失，预测的边界总是粗糙和不精确的（Figure 1,4）。\n（2）物体边界周围的像素只占整个图像的一小部分（不到1%），而且本质上很难分类。不平衡导致了优化偏向，低估了边界像素的重要性。\n许多研究试图改善边界质量，但上述问题仍未得到很好的解决。\n考虑到人工标注行为， 标注人员通常首先对给定图像中的每个对象进行定位和分类，然后在低分辨率下显式或隐式地分割一些粗略的实例掩码。之后，为了获得高质量的模板，注释者需要反复放大局部边界区域，探索更清晰、分辨率更高的边界分割。直观地说，需要高级语义来定位和粗略地分割对象，而低层细节（例如颜色一致性和对比度）对于分割局部边界区域更为关键。本文受人类分割行为的启发，提出了一种概念上简单而有效的后处理框架，通过crop-then-refine策略来提高边界质量。\n 首先提取沿预测实例边界的一系列小图像块； 拼接预测图像边界块后，将送入细化网络，细化粗边界。 精确的小块随后被重新组装成紧凑且高质量的实例分割图。  我们将提出的框架称为边界补丁精化（BPR，Boundary Patch Refinement）。\n由于我们只在对象边界附近裁剪，因此可以用比以往方法高得多的分辨率来处理 patch，从而可以更好地保留低层细节。同时，小patch中边界像素的比例自然增加，可以消除优化偏差。\n在本文的框架中，采用了目前流行的HRNet，它可以在整个网络中保持高分辨率的表示。提出的方法也是一种后处理方案，重点是对边界块进行细化，以提高分割图质量。\n3 Framework 提出框架的概述如 Figure 2 所示。作为一种后处理机制，无需对预分割模型本身进行任何修改或微调。\n3.1 边界块提取 Boundary Patch Extraction 首先需要确定掩码的哪一部分应该被细化。我们提出了一种有效的滑动窗式算法来提取沿预测实例边界的一系列patch。具体地说，我们密集地分配了一组方形边界框，框的中心区域应该覆盖边界像素，如图2(B)所示。得到的方框仍然包含较大的重叠和冗余，因此我们进一步应用非最大抑制(NMS)算法过滤出patch的子集(图2c)。经验表明，重叠越大，分割性能越好，但同时也存在计算量较大的问题。我们可以调整NMS阈值来控制重叠的数量，以实现更好的速度/精度折衷。除了图像补丁（image patches）外，我们还从给定的实例掩码中提取相应的二值掩码（corresponding binary mask patches）。调整拼接image patches、mask patches后，输入到 boundary patch refinement network 中。\n3.2. 边界块细化 Boundary Patch Refinement Mask Patch\n提供的位置和语义信息，使得精化网络无需从头开始学习实例级语义。取而代之的是，优化网络只需要学习如何定位决策边界周围的硬像素，并将它们推到正确的一侧。这一目标可以通过探索局部和高分辨率图像patch中提供的低级别图像特性（例如，颜色一致性和对比度）来实现。相邻的实例可能共享一个相同的边界patch，而学习目标则完全不同且不确定。如果不使用mask patch，则模型很难收敛（Figure 3）。\nBoundary Patch Refinement Network.\n采用HRNetV2，它可以在整个网络中保持高分辨率表示。通过适当增大输入大小，可以得到比以往方法更高分辨率的边界块。\nReassembling.\n将精分割的边界块替换先前的预测，对于那些边界框外没有细化的像素，预测是不变的。对于相邻patch重叠的区域，进行简单的求和取平均，应用0.5的阈值来区分前景和背景。\n3.3. Learning and Inference 基于从训练图像中提取的边界块来训练细化网络。在训练过程，只从预测掩码与地面真实掩码的交集大于0.5的实例中提取边界块，而在推理过程中保留所有预测实例。使用像素级的二进制交叉熵损失，用对应的地面真实掩模对模型输出进行监督。在训练时将NMS消除阈值固定为0.25，而在推理时根据速度要求采用不同的阈值。\n4 Experiments 数据集：Cityscape\n评价指标：COCO-style mask AP、F-score（AF：Average F-score）\n","permalink":"http://landodo.github.io/posts/boundary-patch-refinement/","summary":"Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation论文简介： 作者：Chufeng Tang（https://chufengt.github.io/）","title":"Boundary Patch Refinement"},{"content":"SETR  Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020).\n  Zheng, Sixiao, et al. \u0026ldquo;Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n 论文简介 SETR：用 Transformer 从 Sequence-to-Sequence 的角度重新思考语义分割。\n最早于 2020 年 12 月发布于 arXiv。\n会议：2021 CVPR\n一作单位：Fudan University \u0026amp; Tencent-Youtu Lab\nAbstract 语义分割大多使用空洞卷积和注意力模块来扩大感受野，提升对全局上下文的建模能力。本篇论文基于 ViT，使用一个纯 Transformer（没有卷积和分辨率降低）来将图片编码为一系列的 patch。通过在 Transformer 的每一层建模全局上下文，该编码器结合一个简单的解码器可以形成强大的分割模型，命名为 SEgmentation TRansformer(SETR)。\n实验结果，SETR 在 ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) 上取得了 SOTA。\n1 Introduction 语义分割一直被基于编码器-解码器（encoder-decoder）的架构主导，其中编码器可以说是最重要的模型组件。基于 CNN 的编码器通过卷积层的堆叠（考虑到计算成本，特征图的分辨率逐渐降低），感受野逐渐增加，学到更抽象/语义视觉信息（ abstract/semantic visual concepts）。但由于感受野的限制，学习对语义分割至关重要的远程依赖仍然具有挑战性（long-range dependency information）。\n克服如上限制提出的方法如下：\n 修改卷积操作（卷积核大小、空洞卷积、特征金字塔） 注意力机制  这些工作大多没有改变 FCN model 的本质：编码器对输入的空间分辨率进行下采样，学习到有助于区分语义类别的低分辨率特征图。解码器上采样特征图为全分辨率的分割图。\n本篇论文提出使用纯 Transformer 来替换 CNN 的编码器，称为 SEgmentation TRans- former(SETR)。这个基于 Transformer 的编码器将输入的图像视为一系列的图像块（image patchs），这些块通过可学习的 embedding 表示。然后利用全局自注意模型对序列进行变换，实现区分特征表征学习（discriminative feature representation learning.）。\n具体地，首先将图像分解成一个固定大小的网格，形成一系列地 patches。然后通过线性层得到每个图像块的特征嵌入向量（feature embedding vector），这些向量作为 Transformer 的输入。之后解码器将 Transformer 的输出恢复成原始分辨率。\n在编码器的每一层都没有降低空间分辨率，而是进行全局上下文建模，从而为语义分割问题提供了一个全新的视角。\nViT 在分类任务上有效性证明了图像特征不一定需要从局部逐渐学习到全局上下文（CNN 结构不是必要的）。SETR 将分类拓展到了分割上，提供了一个模型设计的新视角，并且在一些 benchmark 数据集上取得了 SOTA。\n主要贡献总结如下：\n（1）从 sequence-to-sequence 学习的角度，针对图像语义分割问题，提供了一个 FCN-based 模型的替代方案；\n（2）利用 Transformer 架构实现编码器；\n（3）为了验证 self-attention feature 的表现，介绍了 3 中不同复杂度的解码器设计。\n本篇论文提出的 SETR 模型，state of the art on ADE20K(50.28%), Pascal Context (55.83%)，在 Cityscapes 上取得了有竞争力的结果。\n3 Method 3.1 FCN-based semantic segmentation 回顾 FCN\n原始输入为 $H \\times W \\times 3$，后续层的输入为 $h \\times w \\times d$，$d$ 为特征图的通道数。感受野随着层的深度线性增加（取决于卷积核的大小）。FCN 中拥有大感受野的高层才可以建模远程依赖（long-range dependencies）。一味的加深层数以增加感受野带来的收益将迅速减少，上下文信息建模的感受野有限是 FCN 系列架构固有的局限性。\n将 FCN 和注意机制结合起来是一种更有效的学习远距离语境信息的策略。但是二次复杂性（特征图的像素个数）这些注意力方法通常作用于输入较少的高层。\n3.2. Segmentation transformers (SETR) Image to sequence\nSRTR 遵循与 NLP 相同的输入输出结构。\n","permalink":"http://landodo.github.io/posts/20211208-setr/","summary":"SETR Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020). Zheng, Sixiao, et al. \u0026ldquo;Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. 论文简介 SETR：用 Transformer 从 Sequence-to-Sequence 的角度重新思考语义分割","title":"SETR: Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers"},{"content":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation\n  CVPR 2021\n  代码未开源\n  关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep Supervision、Self-Taught Deep Supervision、Mean-Taught Deep Supervision\n  Abstract 针对医学图像分割任务，本论文提出一种半-弱监督方法（semi-weakly supervised segmentation algorithm ），该方法建立在深度监督和师生模式的基础上，并且容易整合不同的监督信号。考虑如何将深监督整合到较低的层次，multi-label deep supervision 是本方法成果的关键。\n通过提出的训练机制（novel training regime for segmentation），可以灵活利用图像，这些图像要么有像素级标注、要么有边框标注、要么有全局标注、要么无标注。将标注需求降低 94.22%，比全监督的方法只降低 5% mIoU。在视网膜液分割数据集上进行实验验证。\n1 Introduction 像素级标注在医学图像语义分割任务中获取代价昂贵。因此，所面临的问题是最小化所需的注释工作，同时最大化模型的准确性。\n现有的方法：结合无标注的数据，快速获取不同质量的弱标签，如从 image-level 到 bounding box 标注。半监督、弱监督方法（semi- and weakly-supervised approaches）取得了令人信服的结果。\n针对像素级标注小数据集，本文提出通过一种新的深度监督，将训练信号整合到分割网络层中，通过丰富的未标注图像放大这些信号。然后利用 mean-teacher 分割模型推理出健壮的伪标签（pseudo-labels）\n本文的贡献：\n（1）对不同数量的训练样本和大量不同的监督类型进行了深入的研究，以实现语义分割；\n（2）引入了一种新的深度监督范式，适用于所提出的 Multi-label Deep Supervision 技术，在此基础上，引入了灵活的半-弱监督路径来集成未标记或弱标记的图像：新的自学深度监督方法（Self-Taught Deep Supervision）。\n（3）Mean-Taught Deep Supervision 增加了对扰动的不变性和健壮的伪标签生成，获得了接近完全监督基线的结果，而只使用了 5.78% 的标签。\n3 Proposed Approach semi-weakly supervised semantic segmentation\nMulti-label Deep Supervision\n对于半-弱监督的语义分割（semi-weakly supervised semantic segmentation），图像数据集为：\n 图像 $x_i$ 可以有不同的标注，如：  $m_i$：pixel-wise annotated mask $b_i$：bounding box $g_i$：image-level label no annotation（无标注）    对于每张图像 $x \\in \\mathcal{D}$，并不一定包含对于的分割图 $\\mathcal{M}$。即每张图像可能有 mask、bounding box、image-level label 或无标注。对于没有 mask 情况，称为半监督分割。\nSupervision integration\n在 encoder-decoder 架构中集成额外的输出（deep supervision）。这些输出在decoder 中的特征图 $f_0, \u0026hellip;, f_h$ 上进行操作。其中，$f_0$ 是 decoder 最里面的特征图，$f_h$ 是最外边的特征图。\n特征图 $f_i$ 的空间维度为 $H_i \\times W_i$：\n 实验中：$H_0 \u0026laquo; H_h$，$W_0 \u0026laquo; W_h$ 输出头 $k_i$ 基于 $f_i$ 计算预测结果：$k_i(f_i) \\in \\mathbb{R}^{C \\times H_i \\times W_i}$  深监督信号（Supervision signals）\n不同的监督模式需要不同的损失函数，在有像素级掩码（seg map）的情况下，训练语义分割模型最常见的目标是最小化交叉熵损失：\n3.2 Multi-label Deeply Supervised Nets 参数高效的多标签深度监督\n深度监督集成到分割网络中的方式存在问题。即，全尺寸的 ground-truth 和网络特征图的空间分辨率之间的空间维度不匹配。\n The problem we identify is the way deep supervision is commonly integrated into segmentation networks. Specifically, the challenge arises due to the mismatch in spatial dimensions between the full-scale ground-truth mask and the smaller spatial resolution within the network’s feature maps (Equation 5).\n 除了使用有损的最近邻插值外，大多数工作是迫使网络学习提升尺度（up-scaling）来解决 ground-truth mask 和 low-resolution spatial features 之间的不匹配。up-scaling 后，标准的输出头将特征图转换为与 mask 相同的 size。\n总结两个缺点：\n（1）网络必须学会 up-scaling，代价是额外的参数；\n（2）中间特征担负着对输出空间中复杂的分类信息和空间关系进行建模的重任，我们怀疑这些信息和空间关系在解码过程中是有用的，但可能只是作为更稳定梯度的跳跃连接。\n (2) intermediate features are burdened to model complex classification information and spatial relations in output space that we question to be useful in the decoding process, but presumably only serve as skip-connections for more stable gradients.\n 本论文建议将特征图中每个位置（x，y）的每个特征向量 $f^{:,x,y}$ 建模为其在输入图像中感受野的 patch-descriptors。因此，我们的目标是将 patch-descriptors 的感受野中包含的所有像素的语义信息植入模型。我们认为，这可以通过强制执行多标签损失来实现，其标签包含接受域中存在的所有语义类别。\n由此可见，我们可以简单地缩小 ground-truth mask 的比例，使之与特征图的大小相匹配，而不需要花费任何参数，并包含限制在感受野中的所有类别的标签，以保留语义信息。down-scaling 过程可以通过 max-pooling 实现。降尺度后的目标 $m_i^{*} \\in \\mathbb{R}^{C \\times H_i \\times W_i}$ 包含特征 $f_i^{:,x, y}$ （patch-descriptor，由特征感受野内的空间位置汇总而成）的多标签 Ground Truth。\n down-scaled multi-label ground-truths: $m$ feature maps: $f$  Self-taught deep supervision\n使用多标签深度监督（Multi-label Deep Supervision）生成伪标签，下采样平滑了噪声监督信号（Fig 2. Right）。\nSelf-taught Deep Supervision 通过如下方式为未标注样本生成 binary ground-truth tensor。\n 将未标注图像 $x_i$ 通过分割网络获得伪标签 $p_i$； 利用伪标签实施 Multi-label Deep Supervision； 最外层对真实 ground-truth 和 pseudo-label 设置单独的输出头；  一个输出头使用干净的标签计算交叉熵损失； 另一个输出头使用公式（9）和伪标签计算损失；   伪标签的推理生成，使用的是干净标签的输出头；  如果 image-level label $g_i$ 可以获取，可以进一步将生成的伪标签约束到包含在 $g_i$ 中的类。以类似的方式，相关联的边界框标签 $b_i$ 可以将伪标签约束为位于粗略区域内。这导致了弱标签图像的灵活集成，以提高伪标签质量。\nMean-taught deep supervision.\n通过(1)强制关于扰动的一致预测和(2)使用教师模型来生成更健壮的伪标签，该教师模型是先前迭代的所有模型的组合。\nMean-Teachers（即先前模型参数的指数移动平均数）\n通过使用学生模型和先前教师模型的移动平均值不断更新教师模型的参数来维护教师模型，可以获得更好的预测。因此，教师不会单独接受培训，而只是通过以下方式进行更新：\n $\\theta$ 表示模型参数 $t$ 表示训练迭代 $\\alpha$ 平滑参数  4 Experiments output-head for pseudo-labels 和 standard output-head 有什么区别？\n goround truth 不同，噪声伪标签用于平滑    过分割问题，是否可以加入一个分类的深监督来缓解？\n多尺度 concatenate 和这个方法的区别？\n","permalink":"http://landodo.github.io/posts/every-annotation-counts/","summary":"Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation CVPR 2021 代码未开源 关键词：semi-weakly supervised、pseudo-labels、Multi-label Deep S","title":"Every Annotation Counts"},{"content":"论文简介 论文名称：TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation\n arXiv：https://arxiv.org/pdf/2107.09843.pdf 单位：（美国）加利福尼亚大学、中科院计算所 会议：MICCAI 2021 source code：https://github.com/YaoZhang93/TumorCP  Motivation：Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation(CVPR 2021) https://arxiv.org/abs/2012.07177，针对自然图像\nAbstract 启发自最近提出的 “Copy-Paste” 数据增强方法，本篇论文提出 TumorCP：一种简单而有效的 object-level 数据增强方法，适用于肿瘤分割。\nTumorCP 在肿瘤分割任务上， Dice 以 7.12% 的显著幅度超过了 baseline。TumorCP + image-level 增强方法，比目前的 SOTA 方法 Dice 提升了 2.32%。\n1 Introduction 高质量的带注释的数据集需要大量的工作和领域知识，这在医学领域尤为显著。为了提高数据高效学习，从不同的角度提出了几种成功的方法：\n leveraging unlabeled data for semi-supervised self-training self-supervised pre-training distilling priors from data 通过不同形态的解剖成像生成新的数据 利用适当的数据增强方法增加数据多样性  不同于复杂的 GAN，“Copy-Paste” 是一种简洁的增强方法，其在自然图像的实例分割中取得了新的突破（CVPR 2021）。但在医学图像领域中，这种方法在很大程度上是未被探索的，因为 “Copy-Paste” 中往往忽略上下文信息，这在直觉上是不可取的！\n“Copy-Paste” 增强方法通过简单地将标注实例粘贴到新的背景图像上作为额外的训练数据，从而避免了从表示空间到像素空间的昂贵生成过程。\n Copy-Paste augmentation avoids costly generation processes from representation space to pixel space by simply pasting the labeled instance onto new background images as additional training data.\n 此外，由于在 “Copy-Paste” 中上下文信息往往被忽略，因此它对医疗任务的有效性仍有待观察。例如，在肿瘤分割中，周围视觉线索，即上下文环境（Context），对肿瘤出现的重要性；医学图像固有的解剖结构使得肿瘤分割离不开上下文。\n本篇论文旨在通过检验 “Copy-Paste” 增强技术在肿瘤分割中的有效性，来填补对上下文在医学领域中作用的理解的空白。\nTumorCP 从源图像中随机选取一个肿瘤，经过一系列空间增强、对比度增强、模糊增强后，将其粘贴到目标图像中的器官上。 TumorCP randomly chooses a tumor from a source image and paste it onto the organs in the target image after a series of spatial, contrast, and blurring augmentations.\n 使用肾肿瘤分割数据集（KiTS19 dataset），nnUNet 来评估此方法。\n2 Method TumorCP 是一种用于肿瘤分割的在线随机增强过程，它的实现是简单和直接的。\n给定一组训练样本 $\\mathcal{D}$，TumorCP 有 $(1 - p_{cp})$ 的概率不执行任何操作；有 $p_{cp}$ 的概率从 $\\mathcal{D}$ 中采样出一个对图像 $(x_{src}, x_{tgt}) \\sim \\mathcal{D}$，并执行一次 “Copy-Paste”。\n令 $\\mathcal{O}{src}$ 为图像 $x{src}$ 上肿瘤集合，$\\mathcal{V}{tgt}$ 为 $x{tgt}$ 上的器官的体积坐标集合，$\\mathcal{T}$ 是一组随机数据转换，每个转换都有一个称为 $p_{trans}$ 的概率参数。\n一次 “Copy-Paste” 流程：\n TumorCP 首先采样一个肿瘤 $o \\sim \\mathcal{O}{src}$、一组数据转换 $\\tau \\sim \\mathcal{T}$ 和一个目标位置 $v \\sim \\mathcal{V}{tgt}$； 然后将 $τ(o)$ 以 $v$ 为中心，取代原始数据和标注。  为了充分利用 TumorCP 的优势，精心设计了两种肿瘤 “Copy-Paste” 模式：\n intra-patient Copy-Paste inter-patient Copy-Paste  2.1 TumorCP’s augumentation Intra-/Inter- Copy-Paste\n为了研究病人之间的差异对 TumorCP 的影响，使用两种设置：\n（1）intra-CP：源图像和目标图像来自同一病人；\n（2）inter-CP：源图像和目标图像来自不同的病人。\n从数据分布的角度来看，由于其强度与数据整体一致，所以首选 intra-CP，但这限制了数据的多样性。从数据多样性的角度来看，inter-CP 更受青睐，因为它打开了利用其他患者的新背景和前景的途径，但它也带来了分布差异。（实验表明 inter-CP 要优于 intra-CP）\n使用三个不同的 object-level 转换来对 “Copy-Paste” 进行扩展。\n Spatial transformation decouples context and improves morphology diversity.  空间转换解耦了上下文，改善了形态的多样性。image-level 的一些增强方法（镜像、旋转）仍然作为一个整体处理图像，保持前景和背景之间的耦合。因此，模型可能会寻找并倾向于过度拟合周围看似合理但实际上不相干的线索。Figure 1 是应用缩放、旋转和镜像的刚性变换和使肿瘤变形的弹性变换来增加形态的多样性。\n Gamma transformation enhances contrast and improves intensity diversity.  伽马变换增强了对比度，提高了强度的多样性。随机采样的伽玛参数增强了肿瘤的强度多样性；幂律非线性增强局部对比度，有利于肿瘤鉴别。\n Blurring transformation improves texture diversity  模糊化改造提高了纹理多样性。使用高斯滤波器作为模糊变换，聚集噪声干扰的低层次纹理可以间接增加相对高层次纹理的纹理多样性。\n整个 Pipeline 可以结合 image-level 增强方法。\n2.2 Intuitions on TumorCP’s Effectiveness TumorCP 有两个目标：i) 增加数据多样性，ii) 学习高层次和抽象肿瘤的不变表示。数据多样性随着肿瘤的新组合和周围环境的增加而增加。为了了解高层次的信息，如下讨论了 TumorCP 的三个特性来解释其有效性。 通过语境不变的预测消除背景偏差（Eliminated Background Bias by Context-Invariant Prediction）  CNN 不可避免地将周围的视觉上下文与物体本身进行卷积，可能会使得模型偏向于看似合理但实际上与肿瘤无关的线索，从而增加过度拟合的风险。\nTumorCP 则为肿瘤提供了一个更为独立的区域，从而为肿瘤周围上下文环境提供了无限的可能性。提升了模型的泛化能力、消除了背景偏差。\n 通过转换-不变预测提高泛化能力（Improved Generalizability by Transformation-Invariant Prediction）  该模型应同时捕获高级语义信息和低级边界信息，以实现成功的分割。TumorCP 可以生成不同大小、形状、颜色和纹理的肿瘤，增加了类内差异。\n它能够帮助模型从数据中捕获更好的语义信息。换句话说，它使模型的预测在不同的数据转换（可能类似于真实世界的数据）中保持不变，并提高了通用性。\n Oversampling Behavior  数据不平衡是一个普遍存在的问题，典型的解决方案通常是根据类分布重新加权损失函数或重新采样训练数据。在肾脏肿瘤分割任务中，背景、器官、肿瘤极度不平衡，TumorCP 就像一个数据重采样器，以较小的成本显著增加肿瘤的增殖程度。\n3 Experiments and Discussion 在 KiTS19 数据集（肾肿瘤的分割）上评估 TumorCP，使用 Sørensen-Dice系数 (Dice) 评分。\n消融学习\n（1）Ablation on intra-CP with different transformations.\n（2）Ablation on intra-/inter-CP\n（3）Ablation on compatibility\n  TumorCP（Object-level）和 Image-level 的图像增强是兼容的；\n  TumorCP 也改善了器官分割；\n  极低数据量的实验\n4 Conclusion  提出了 TumorCP： a simple but effective object-level data augmentation for tumor segmentation； 在肾脏肿瘤分割任务上，比目前的 SOTA 提升了 2.31% Dice； 实验验证了 TumorCP 在极低数据量情况下的潜力； TumorCP 不直接处理 Inter-CP（不同的病人）之间的分布不匹配问题，但仍然取得了惊人的性能提升。  ","permalink":"http://landodo.github.io/posts/20211119-tumorcp/","summary":"论文简介 论文名称：TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation arXiv：https://arxiv.org/pdf/2107.09843.pdf 单","title":"TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation"},{"content":"🎯Topic: 2.5D methods for Volumetric Medical Image Segmentation论文简介 （1）2.5D 医学图像分割网络综述，发表于 2020 年 10 月。\n Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D Method: An Empirical Study\n （2）2.5D 分割网络实例，发表在 MICCAI 2019\n Learning shape priors for robust cardiac MR segmentation from multi-view images\n Abstract 医学图像数据（CT/MRI）大多是 3D 的，然而 3D CNN 并不一定是好的选择。更多的推理时间、计算代价、参数的增加有过拟合的风险（医学图像标注获取昂贵）。为了解决这一问题，人们提出了许多 2.5D 分割方法，以更低的计算成本利用体积空间信息。\n1. Introduction 3D 医学图像，如 CT 和 MRI 已广泛应用于临床诊断。体积图像的自动分割在生物医学应用中变得越来越重要。目前主要两种策略：（1）将三维体积切割成二维切片，并根据切片内信息训练二维 CNN 进行分割；（2）3D CNN。\n两种方法有其各自的优缺点：\n ✅ 2D CNN 更轻的计算和更高的推理速度。 ❌ 2D CNN 忽略相邻切片之间的信息，阻碍了分割精度的提高。 ✅ 3D CNN 具有对空间信息的感知能力。 ❌ 3D CNN 需要较高的计算成本，更高的推理延迟，参数越多，过拟合风险越大（尤其是小的数据集），这阻碍了进一步临床应用。  为了弥补 2D 和 3D CNN 之间的差距，提出了许多 2.5D 分割方法（也称为伪 3D 方法），通过设计新的体系结构或使用策略将体积信息融合到 2D CNN 中来实现高效的医学图像体积分割。\n本篇文章的主要贡献可以总结如下：\n 本文综述了 2.5D 医学图像分割方法的最新进展，将 2.5D 医学图像分割方法分为多视图融合（multi-view fusion）、融合层间信息（incorporating inter-slice information）和融合二维/三维特征（fusing 2D/3D features）三大类。 对这些 2.5D 方法在 3 各具有代表性的数据集（CT/MRI、心脏/前列腺/腹部）上进行了大规模的评估、比较。  2. Related Work 本节对多视图融合（multi-view fusion）、融合层间信息（incorporating inter-slice information）和融合二维/三维特征（fusing 2D/3D features）三大类 2.5D 分割方法进行综述。\n2.1 多视图融合（multi-view fusion）\n为了将体积空间信息整合到二维 CNN 中，一个简单而直观的解决方案是多视图融合（MVF）。\n多视图包括矢状面（sagittal）、冠状面（coronal）、横断面（axial）。训练 3 个2D CNN 分别从矢状面、冠状面和横断面进行分割，再将 3 个视图的结果融合在一起，可以充分利用 3D 空间信息，从而获得比 2D CNN 更好的分割结果。\n融合的方式有：多数投票、 Volumetric Fusion Net（一个浅层的 3D CNN）。\n2.2. 融合层间信息（Incorporating inter-slice information）\n三维空间的分辨率并不总是相同的，有时 x 和 y 轴的图像分辨率比 z 轴高十倍以上（直肠癌数据就是这样 shape=768x696x48）。这种情况下训练长轴（冠状面：xz 或 矢状面：yz）是不可取的。\n另一种策略将层间信息整合到二维 CNN 中，以探索空间相关性。网络不仅可以利用切片的信息，还可以利用相邻切片的信息。网络的输入是连续的切片，输出是对应中间切片的分割结果。通过整合片间信息，可以利用体积的空间相关性，同时避免了 3D 计算的沉重负担。\n不少方法都是喂入 3 张 slices，但是直接添加相邻的片作为多通道输入可能效率低下。当相邻切片混合成通道维时，输入切片的信息在第一卷积层融合，这一过程网络很难提取用于区分每个片的有用信息。\n计新的层间信息抽取体系结构成为研究的重点。\n bi-directional convolutional long short-term memory (BC-LSTM) ：3D 体的 2D 切片被视为一个时间序列，以提取切片间的上下文和特征； inter-slice attention module：利用相邻切片的信息生成 attention mask，为分割提供先验形状调节；contextual-attention block：使用片与片之间的元素减法，强制模型聚焦于边界区域。  2.3. 融合二维/三维特征（Fusing 2D/3D features）\n一些工作也集中在融合从二维和三维 CNN 提取的特征，虽然这些方法仍然使用三维卷积来提取空间信息，但是与训练纯三维 CNN 相比，总体计算成本降低了。\n#########################################################################\n2.5D 分割网络实例，MICCAI 2019\nAbstract 受经验丰富的临床医生如何通过多个标准视图（即长轴和短轴视图）评估心脏形态和功能的启发，本篇论文提出了一种新的方法，在不同的二维标准视图中学习解剖形状先验，并利用这些先验从短轴（SA） MR 图像中分割左心室 (LV) 心肌。\n提出的分割方法具有二维网络的优点，但同时结合了空间背景。在不同的短轴切片上实现了准确和稳健的心肌分割。\n1. Introduction 基于二维的分割网络，以 slice-by-slice 的方式训练，对于复杂形状的目标，小目标的情况，分割的结果不太理想。这是由于 2D 网络没有结合相邻的 short-axis(SA) 图像或 long-axis(LA) 图像的空间信息。\n对于三维的分割网络，心脏的三维空间背景可能受到潜在的层间运动伪影和低平面空间（SA）分辨率的影响，从而限制了它们的分割性能。（直肠癌使用单纯的 3D 网络，估计无法取得良好的效果）\n x轴和y轴保持的分辨率远高于z轴，3D cnn的性能优势并不明显，有时甚至不如一些2.5D方法。\n 经验丰富的临床医生能够从多个标准视图评估心脏形态和功能，即使用 SA 和 LA 图像来形成对心脏解剖的理解。直觉上，从多个标准视图学习到的表示对 SA 切片的分割任务是有益的。受此启发，论文提出了一种通过四个标准视图学习解剖学先验知识的方法，并利用该方法对二维 SA 图像进行分割。\n贡献总结：\n a) developed a novel autoencoder architecture (Shape MAE)，它从多个标准视图中学习心脏形状的潜在表示； b) developed a segmentation network (multi-view U-Net)，结合多视图图像的解剖形状先验来指导SA图像的分割； c) 与 2D/3D 方法进行评估，表明该方法具有更强的鲁棒性，且对训练数据大小的依赖性较小。  2. Methods 提出的方法包含两个新的架构：\n shape-aware multi-view autoencoder (Shape MAE)：从标准心脏采集平面（包括短轴和长轴视图）学习解剖形状先验； multi-view U-Net：通过将 Shape MAE 学习到的解剖学先验信息整合到改进的 U-Net 体系结构中，实现心脏图像分割。  Shape MAE: Shape-aware multi-view autoencoder\n通过多任务学习从标准心脏视图学习解剖形状先验。对于给定输入的原图 $X_i$，网络学习 $X_i$ 的低维表示 $z_i$，它最能重建所有 $j$ 个目标视图分割 $Y_j$。\n本论文采用 4 source views $X_i \\ (i = 1, 2, 3, 4)$，分别是 3 long-axis(LA) views，和 1 short-axis(SA) views，。\n LA: two-chamber view (LA1), three-chamber view (LA2), the four-chamber view (LA3)\nSA: mid-ventricular slice (Mid-V) from the SA view\n 网络从其中一个视图学习低维表征 $z_i$ 来重建分割图。分割图的视图 $Y_i$ 有 6 个，4 个对应源视图，另外两个为 SA lices：apical 和 basal。\n损失函数：\n 前两项为交叉熵； 最后一项是 latent representations $z_i$ 的正则化项；  该网络的原理是，不同的视图需要独立的函数将它们映射到描述全局形状特征的潜空间；而将这个潜空间转换到另一个视图或平面也需要一个特定的投影函数。根据六个目标视图而不是单一视图来预测心肌的形状，鼓励网络学习和利用不同视图之间的相关性，从而形成一个全局的、视图不变的形状表征，而不是一个特定视图的局部表征。这个框架中的所有编码器和解码器都是以多任务学习的方式联合训练的，这样做的好处是避免过度拟合，鼓励模型的泛化。\nMV U-Net: Multi-view U-Net.\nMV U-Net 相比于原始的 U-Net 卷积核更少一些（取决于当前的任务），并且结合 shape MAE 学习到的解剖形状先验信息。\nFuse Block 模块由两个卷积核 (3 × 3) 和一个残差连接组成，通过可学习权值将不同视图的形状表示结合起来。给定任意短轴图像切片 $I_p$，和它相应的 shape representation $z^{p}_1, z^{p}_2, z^{p}_3, z^{p}4$（通过 Shape MAE 获得），网络可以将先验知识提炼为网络的高级特征，使其能够通过多视图信息有效地细化分割：$S_p = f{MV\\ U-Net}(I^p, z^{p}_1, z^{p}_2, z^{p}_3, z^{p}_4; \\theta)$。\n该网络采用标准的交叉熵损失进行训练。\n3. Experiments and Results 4 Conclusion 提出了一个形状感知的多视图自动编码器，一个能够从多个标准视图学习解剖形状先验信息的多视图 U-Net，该网络是对原始 U-Net 架构的修改，合并了学习的形状先验信息，以提高心脏分割的鲁棒性。\n本论文将长轴 LA 和短轴 SA 结合起来，利用长轴图像的空间背景来指导短轴图像的分割。从 LA 视图中提取的额外解剖信息，对那些具有挑战性的切片的分割特别有利。\nMV U-Net 保持了 2D 网络的计算优势，在有限的训练数据下实现较高的分割性能。\n 什么是 LA？什么是 SA？\n✅答：即长轴和短轴数据。例如在直肠数据中，一个 .nii.gz 的 shape 为 768x696x48，那么 xy 为短轴，xz/yz 为长轴。xy 为短轴，横断面，分辨率较高。长轴分辨率低，数据模糊。\n","permalink":"http://landodo.github.io/posts/20211105-2-5d-network/","summary":"🎯Topic: 2.5D methods for Volumetric Medical Image Segmentation论文简介 （1）2.5D 医学图像分割网络综述，发表于 2020 年 10 月。 Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D Method: An Empirical Study （2）","title":"2.5D 2.5D methods for Volumetric Medical Image Segmentation"},{"content":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。\n1. 聚类问题简介 聚类是针对给定的样本，依据它们的特征的相似度或距离，将其归并到若干“类”或“簇”中。其目的是通过得到的类或簇来发现数据的特点或对数据进行处理。聚类作为经典的无监督学习算法，在数据挖掘/机器学习等领域有着广泛地应用。\n两种最常用的聚类算法：层次聚类和 K 均值聚类。\n 层次聚类：将每个样本各自分到一个类；之后将相聚最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化类别结构。 K-Means 聚类：选择 K 个类别的中心，将样本逐个指派到与其最近的中心的类中，得到一个聚类结果；然后更新每个类的样本的均值，作为类的新的中心；重复以上步骤，直到收敛为止。  K-Means 被选为数据挖掘十大经典算法之一。\n图神经网络已经成为深度学习领域最热门的方向之一，那么，如何利用图神经网络强大的结构捕获能力来提升聚类算法的精度呢？深度聚类是聚类方法的一种，它采用深度神经网络来学习聚类友好表征。\n2. 相关综述 综述文献：[1][2]\n深度聚类算法（Deep Clustering Algorithm）可以分解为三个基本组成部分：\n 深度神经网络 网络损失 $L_n$ 聚类损失 $L_c$  3. 论文精读 论文名称：Attributed Graph Clustering: A Deep Attentional Embedding Approach [3]\n发表期刊：International Joint Conference on Artificial Intelligence (IJCAI-19)\n论文地址：https://arxiv.org/abs/1906.06532\n摘要 最近的研究大多集中在使用深度学习方法来学习一个紧凑的图形嵌入（embedding），在此基础上应用经典的聚类方法（如 K-means）来完成聚类任务。这种两阶段的方法通常无法取得更好的结果，因为其图嵌入不是以目标为导向的，即此深度学习方法并不是为聚类任务而设计的。\n本篇论文提出一种以目标为导向的深度学习方法：Deep Attentional Embedded Graph Clustering (DAEGC)。这种方法包含三个主要核心点：\n（1）注意力机制的图自编码器（Graph Attentional Autoencoder）\n（2）自训练的图聚类（Self-optimizing Embedding）\n（3）自训练过程与图嵌入共同学习和优化（Joint Embedding and Clustering Optimization）\nDAEGC 在 Cora、Citeseer、Pubmed 数据集上都取得了最好的聚类结果。\nIntroduction 对于图聚类问题，其中的关键是如何捕捉结构关系和节点内容信息。很多近期的研究通过深度学习方法学习到节点的 Embedding，再利用简单的聚类算法（如 K-Means）进行聚类。\n很显然，这是一种两个阶段的方法（非目标为导向），其存在着如下的缺点：学习到的 Embedding 可能不是最适合随后的图聚类任务，并且图聚类任务对图的嵌入学习没有帮助。\n传统的以目标为导向的方法大多针对的是分类任务（监督学习，如图卷积实现分类）。\n本论文提出了一种以目标为导向的图注意力自动编码器的图聚类框架。Figure 1 显示了其与两阶段方法的不同，模型学习嵌入并同时在一个统一的框架内进行聚类，从而获得更好的聚类性能。\nModel 如 Figure 2 所示模型主要由两个模块构成：\n（1）Graph attentional autoencoder\n自动编码器将节点属性值和图结构作为输入，并通过最小化重建损失来学习潜在的嵌入。\n（2）Self-training clustering\n自训练模块根据学习到的表征进行聚类，反过来，根据当前的聚类结果来操作潜在的表征。\n在一个统一的框架内学习图的嵌入和聚类，这样每个部分都能使对方受益。接下来对模型的细节进行分析。\n（1）Graph Attentional Autoencoder Graph Attention Encoder\n使用一个图注意力网络（GAT）的变体作为 Graph Encoder，其核心是通过关注其邻居来学习每个节点的隐藏表征，将节点特征与潜在表征中的图结构相结合。注意力机制对邻居的表示给予不同的权重。\n $z^{l+1}_{i}$​：节点 $i$​ 的输出表征（新特征）； $N_i$​：节点 $i$​ 的所有邻居节点； $\\alpha_{ij}$​：注意力系数，衡量节点 $j$​ 对节点 $i$​​ 的重要程度； $\\sigma$​​：非线性激活函数。  注意力系数 $\\alpha_{ij}$​ 可以表示为一个单层前馈神经网络，$x_i$ 和 $x_j$ 表示节点 $i$ 和 $j$ 的特征向量。\n $c_{ij}$​ 是一个标量，衡量节点 $j$​ 对节点 $i$​ 的重要程度。  图注意网络（GAT）只考虑了一阶邻居， 由于图具有复杂的结构关系，本篇论文的编码器中利用高阶邻居，通过考虑图中的 t 阶邻居节点来获得一个接近矩阵：\n $B$ 是一个转移矩阵（非负，各行元素之和等于 1），如果节点 $i$ 和 $j$ 之间存在边，则 $B_{ij} = 1/d_i$，否则 $B_{ij} = 0$。$d_{i}$ 为节点 $i$​​ 的度； $M_{ij}$​ 表示节点 $i$ 和节点 $j$ 之间的拓扑相关性（$t$​ 阶邻居）； $N_i$ 指 $M$ 中 $i$ 的邻居节点，如果 $M_{ij} \u0026gt; 0$，则表示 $j$ 是 $i$​​​ 的邻居节点； $t$：可以针对不同的数据集灵活地选择t，以平衡模型的精度和效率。  注意力系数通常在所有邻域 $j∈N_i$​​ 中用一个 softmax 函数进行归一化，以使它们易于在各节点间进行比较。\n加上拓扑权重 M 和激活函数 δ（LeakyReLU），注意力系数可以表示为：\n$x_i = z^{0}_{i}$​ 作为输入，堆叠两个图注意力层：\n图注意力编码器将结构和节点特征编码成一个隐藏的表示，得到：$z_i = z^{(2)}_{i}$。\nInner Product Decoder\n解码器（Decoder）可以进行如下分类：重建图结构、重建节点特征属性、两种都重建。由于公式（7）获取到的潜伏嵌入（latent embedding）已经包含了内容和结构信息，因此本篇论文选择采用一个简单的内积解码器来预测节点之间的联系：\n重建损失 Reconstruction Loss\n通过衡量 $A$ 和 $\\hat{A}$ 之间的差异性来最小化重构损失：\n（2）Self-optimizing Embedding 图聚类任务是无监督的，因此在训练期间无法获得关于所学嵌入是否得到很好优化的反馈。即 GAE 所学习到的节点表示只是为了更好的重构网络结构，和聚类并没有直接联系。针对此问题，本篇论文提出了 一种自优化的嵌入算法作为解决方案，对 GAE 所学习到的 embedding 进行约束和整合，使其更适合于聚类任务。\n除了优化重建误差，还将隐藏嵌入（hidden embedding）输入到一个自优化的聚类模块中，该模块最小化了以下目标：\n $q_{iu}$ ：衡量节点的 embedding $z_{i}$​ 和聚类中心 embedding $\\mu_u$ 的相似度  使用学生分布（Student\u0026rsquo;s $t$​​-distribution）来衡量，以处理不同规模的集群。（11）式中，聚类中心 embedding 为 $\\mu_u$​​​，则节点 $i$​​​​ 属于某个类别的概率为 $q_{i u}$:\n T 分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值。\n $q_{i u}$ 可以被看作是每个节点的软聚类分配分布。为了引入聚类信息来实现聚类导向的节点表示，我们需要迫使每个节点与相应的聚类中心更近一些，以实现所谓的类内距离最小，类间距离最大（二次方后，分布会变得更加尖锐，也更置信）。定义目标分布 $p_{iu}$​ ：\n 类别 $k$  目标分布 P 将 Q 提高到二次方，以强调这些“自信的分配”的作用。然后，聚类损失迫使当前分布 Q 接近目标分布 P，以便将这些“有信心的分配”设置为软标签（“works as ground-truth labels”），监督 Q 的嵌入学习。\n将聚类损失降到最低，以帮助自动编码器利用嵌入物自身的特性操纵嵌入空间。\n（3）Joint Embedding and Clustering Optimization 共同优化自动编码器嵌入和聚类学习，总目标函数定义为：\n  $L_r$：Reconstruction loss\n  $L_c$：Clustering loss\n  $\\gamma \u0026gt;= 0$：控制两者的平衡\n  可以直接从最后一个优化的 Q 中获得聚类结果，即对节点 $i$ 的所处的簇为：\nDeep Attentional Embedded Graph Clustering Algorithm\nExperiments 三个数据集\n总结  DAEGC: an unsupervised deep attentional embedding algorithm; 在一个统一的框架内，联合进行图聚类和学习图嵌入； 学习到的图嵌入整合了结构和内容信息，并专门用于聚类任务； 针对聚类这个无监督学习任务，提出了一个自训练的聚类组件，从“置信”的分配中生成软标签，以监督嵌入的更新； 聚类损失和自动编码器重建损失被联合优化，以同时获得图嵌入和图聚类的结果； 将实验结果与各种最先进的算法进行比较，验证了 DAEGC 的图聚类性能。  References [1] Aljalbout, E., Golkov, V., Siddiqui, Y., \u0026amp; Cremers, D. (2018). Clustering with Deep Learning: Taxonomy and New Methods. ArXiv, abs/1801.07648.\n[2] Erxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing Cui, and Jun Long. A Survey of Clustering with Deep Learning: From the Perspective of Network Architecture. DOI: 10.1109/ACCESS.2018.2855437, IEEE Access, vol. 6, pp. 39501-39514, 2018.\n[3] Wang, C., Pan, S., Hu, R., Long, G., Jiang, J., \u0026amp; Zhang, C. (2019). Attributed Graph Clustering: A Deep Attentional Embedding Approach. IJCAI.\n[4] https://deepnotes.io/deep-clustering\n Can deep neural networks learn to do clustering? Introduction, survey and discussion of recent works on deep clustering algorithms.\n [5] Papers List (1.2k Stars) https://github.com/zhoushengisnoob/DeepClustering\n Deep Clustering: methods and implements\n [6] 图神经网络时代的深度聚类：https://zhuanlan.zhihu.com/p/114452245\n","permalink":"http://landodo.github.io/posts/deep-clustering-notes/","summary":"Deep Clustering 深度聚类 🎯研究图神经网络的聚类问题，利用图神经网络强大的结构捕获能力来提升聚类算法的效果。 1. 聚类问题简介 聚类是针对给定的样本，依据它们的","title":"深度聚类 Deep Clustering"},{"content":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。\n Hugo 默认是使用 TOML，现在将此更改为更易阅读的 YAML。 Hugo 需要在源目录查找一个 config.toml 的配置文件。如果这个文件不存在，将会查找 config.yaml，然后是 config.json\n 添加站内搜索 Search Page 全局文章内容搜索，从关键字快速定位到文章。\nHogu 文档：https://adityatelange.github.io/hugo-PaperMod/\nPaperMod uses Fuse.js Basic for seach functionality\nAdd the following to site config, config.yml\noutputs:  home:  - HTML  - RSS  - JSON # is necessary Create a page with search.md in content directory with following content\n--- title: \u0026#34;Search\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # is necessary # url: \u0026#34;/archive\u0026#34; # description: \u0026#34;Description for Search\u0026#34; summary: \u0026#34;search\u0026#34; --- To hide a particular page from being searched, add it in post’s fron’t matter\n--- searchHidden: true copy\nex: [search.md]\n添加 Archives 功能 这是自动的，只需要添加一个按钮，其他的就不用管了。\nhttps://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n增加公式支持 此功能会拖慢网站的加载速度，这个以后再说！些许的加载时间能换取良好的阅读体验，目前来看是值得的。我的论文阅读笔记里充斥了大量的 LaTeX 公式。\n公式支持基于 MathJax。\nhttps://mertbakir.gitlab.io/hugo/math-typesetting-in-hugo/\n行内公式：\n给定一组训练样本 $\\mathcal{D}$，TumorCP 有 $(1 - p_{cp})$​​​ 的概率不执行任何操作；有 $p_{cp}$​ 的概率从 $\\mathcal{D}$ 中采样出一个对图像 $(x_{src}, x_{tgt}) \\sim \\mathcal{D}$​​，并执行一次 “Copy-Paste”。\n令 $\\mathcal{O}{src}$ 为图像 $x{src}$ 上肿瘤集合，$\\mathcal{V}{tgt}$ 为 $x{tgt}$ 上的器官的体积坐标集合，$\\mathcal{T}$ 是一组随机数据转换，每个转换都有一个称为 $p_{trans}$​ 的概率参数。\n一次 “Copy-Paste” 流程：\n TumorCP 首先采样一个肿瘤 $o \\sim \\mathcal{O}{src}$​、一组数据转换 $\\tau \\sim \\mathcal{T}$​ 和一个目标位置 $v \\sim \\mathcal{V}{tgt}$； 然后将 $τ(o)$ 以 $v$​ 为中心，取代原始数据和标注。  行间公式：\n$$\\sqrt{x} + \\sqrt{x^{2}+\\sqrt{y}} = \\sqrt[3]{k_{i}} - \\frac{x}{m}$$\n测试 Emoji 🧡💥💢💌💝🕎☪\n测试代码 import sys from PyQt5 import QtCore, QtGui, QtWidgets, uic from PyQt5.QtCore import Qt, QEvent import random  from PyQt5.QtGui import QPixmap from PyQt5.QtWidgets import QAction, QFileDialog   class Canvas(QtWidgets.QLabel):  def __init__(self, parent=None):  super().__init__(parent)  self.background = QPixmap(200, 200)  self.background.fill(Qt.yellow)  # self.clear(Qt.yellow)  self.last_x, self.last_y = None, None  self.pen_color = QtGui.QColor(\u0026#39;#000000\u0026#39;)  self.setPixmap(self.background)   def mouseReleaseEvent(self, e):  self.last_x = None  self.last_y = None 图片的显示 这里需要将图片拷贝的 public 对应的文章目录下，使用原本的相对路径。不需要将文件夹拷贝到 content/posts/ ，这个文件夹下至保存 Markdown 文件。\n使用 hugo 生成文章的 HTML 文件后，每一篇文章都会以 Markdown 文件中的 title 生成一个文件夹。如本篇文章的文件夹名称为 first-post （全小写）。\n在访问时，使用的是如下的链接：\nhttps://landodo.github.io/first-post/0000465981_017.jpg 如果图片实现依旧不正常，使用 F12 进行调试，快速定位问题所在。\n","permalink":"http://landodo.github.io/posts/first-post/","summary":"Hi, Hugo! Hello, Hugo! 这是第一篇内容！将原来的笔记从 Jellky 全部转移至 Hugo。下面是一些关于这个新站点建立时的笔记。 Hugo 默认是使用 TOML，现在将此更改为更易阅","title":"First Post for Hugo"}]