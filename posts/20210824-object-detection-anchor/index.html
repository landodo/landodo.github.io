<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<meta name="robots" content="index, follow">
<title>关于 Object Detection Anchor 的学习笔记 | Notes</title>
<meta name="keywords" content="目标检测, 笔记" />
<meta name="description" content="anchor 什么是 anchor？ anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此">
<meta name="author" content="">
<link rel="canonical" href="http://landodo.github.io/posts/20210824-object-detection-anchor/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<link rel="preload" href="./logo.png" as="image">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://landodo.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://landodo.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://landodo.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://landodo.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://landodo.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="关于 Object Detection Anchor 的学习笔记" />
<meta property="og:description" content="anchor 什么是 anchor？ anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://landodo.github.io/posts/20210824-object-detection-anchor/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-24T10:12:08&#43;08:00" />
<meta property="article:modified_time" content="2021-08-24T10:12:08&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="关于 Object Detection Anchor 的学习笔记"/>
<meta name="twitter:description" content="anchor 什么是 anchor？ anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://landodo.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "关于 Object Detection Anchor 的学习笔记",
      "item": "http://landodo.github.io/posts/20210824-object-detection-anchor/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "关于 Object Detection Anchor 的学习笔记",
  "name": "关于 Object Detection Anchor 的学习笔记",
  "description": "anchor 什么是 anchor？ anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此",
  "keywords": [
    "目标检测", "笔记"
  ],
  "articleBody": "anchor 什么是 anchor？\n anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此使得图像上密集铺满了许多 anchor，从而覆盖到包含物体的所有位置区域。\n 早期的方法：金字塔多尺度 + 滑动窗口，逐尺度逐位置判断“这个尺度的这个位置处有没有认识的目标”，非常的耗时。\n基于 anchor 的方法：预设一组不同尺度、不同位置的固定参考框，覆盖几乎所有位置和尺度，每个参考框检测与其交并比大于阈值的目标。anchor 将问题转换为“这个固定的参考框中有没有认识的目标，目标偏离参考框多远”。\nRPN是一个conv3x3+两个并列的conv1x1，一边预测anchor中是否包含目标，一边预测目标框偏离固定anchor多远。\nfeature map上每个位置设置9个参考anchor，这些大约能覆盖边长70~768的目标。下图是Faster R-CNN论文中各个 anchor 形状训练后学习到的平均proposal大小。\n特征提取部分：\nimport torch import torchvision.models.vgg as vgg import torch.nn as nn  if __name__ == '__main__':   model = vgg.vgg16(pretrained=False)  featrue = list(model.features)[:30]  featrue_extra = nn.Sequential(*featrue)   input = torch.randn((4, 3, 256, 256))  output = featrue_extra(input) input.shape = torch.Size([4, 3, 256, 256])\noutput.shape = torch.Size([4, 512, 16, 16])\nbackbone 提取的特征图（Featuremap）相对于网络输入图像尺寸缩小了 16 倍。因此，featuremap 中的 1 个像素点就相当于输入图像的 16 个像素点。或者说，featuremap 中的 1x1 区域覆盖了输入图像的 16x16 区域。即 featuremap 中的每个像素点都对应的覆盖了输入图像的区域。\n一个点对应 9 个 anchor。（3 种尺度、3 种宽高比的排列组合）\nanchor 坐标有可能出现负数的情况。\n计算 featuremap 左上角(0,0) 对应的 9 个 anchor 的中心点坐标和长宽之后，其余的点坐标/长宽可以通过平移的方式得到。\n分类：\n 3x3 卷积，（融合周围 3x3 的区域，更鲁棒？） 1x1 卷积，（融合通道） ？  回归：\n 1x1 卷积将通道映射到 36=4x9。4 表示？  对分类和回归结进行后处理，生成 RoI（Region of Interest），也称为了 Proposal layer，对 RPN 输出的分类和回归结果进行后处理（如 非极大值抑制 等），得到网络任务包含物体的区域——RoI。\nRoI Pooling 的作用是将不同尺寸的各个 RoI 都映射到相同大小。\nRoI Pooling：如何划分 7x7 个 bin？当无法整除的情况，有人提出了 RoI Align 和 Precise RoI Pooling。\n存在的缺点：每个 bin 中只有一点贡献了梯度，忽略了大部分点的信息。\n将 RoI Pooling 后的结果 flatten 成为 vector，输入全连接层进行分类和回归，对应输出的神经元个数分别为物体类别数（n_classes）和每个类别对应的 bbox（n_classes x 4）。\nRoIHead 的输出不是预测结果的最终形态，还需要进行一些后处理。\n 将网络输出缩放至原图尺寸（不是网络输入的尺寸）； 对回归的结果去归一化（乘 std, 加 mean），结合 RoIs 的位置和大小计算出 bbox 的位置（左上角坐标和右下角坐标），并且裁剪到原始尺寸范围内； 选择置信度大于阈值的矩形框，最后再使用非极大值抑制剔除重叠度高的 bbox 得到最终的结果。  还是搞不太懂，留一段时间吧。先看看 Focal Loss\n — 分割线 —————————–\n😡两个问题：\n （1）正负样本不平衡 （2）难易样本不平衡  二分类问题的标准 Loss 是交叉熵：\n$$L_{ce} = -ylog \\check{y} - (1-y)log(1 - \\check{y})$$\n 当 y = 1 时，$L = -log\\check{y}$。（如果网络的输出预测值 $\\check{y}$ 越接近于 1，则 loss 越小） 当 y = 0 时，$L = -log(1-\\check{y})$（如果网络的预测值 $\\check{y}$ 越接近于 0，则 loss 越小）  -log 的函数图像：\n“硬截断”的 Loss：$L^* = \\lambda(y, \\check{y}) \\cdot L_{ce}$，其中：\n (y == 1) 且 $\\check{y}  0.5$ 或 (y == 0) 且 $\\check{y} 其他情况，为 1  正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，就都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。\n上面的 Loss 存在问题，即它只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5。\n进一步的优化是将“硬截断”的 Loss “光滑化”，可导。\nGHM(gradient harmonizing mechanism) 是基于 Focal Loss 的改进。\n 公式(2)解决了正负样本的不平衡：\n  公式(3)解决了难易样本的不平衡：\n  结合 (2)(3) 得到 Focal Loss：\n 😍GHM：不应该过多关注易分样本，但是特别难分的样本（噪声、离群点）也不应该过多的关注。\n梯度密度 $GD(g)$：单位梯度模长g部分的样本个数。\n🤑关于目标检测任务中正负样本不平衡、难易样本不平衡问题的学习笔记\n 5分钟理解Focal Loss与GHM——解决样本不平衡利器\nhttps://zhuanlan.zhihu.com/p/80594704\nAAAI 2019：把Cross Entropy梯度分布拉‘平’，就能轻松超越Focal Loss\nhttps://zhuanlan.zhihu.com/p/55017036\n Focal Loss （RetinaNet）\none-stage 为什么会差于 tow-stage：\n   one-stage 网络最终学习的anchor有很多，但是只有少数anchor对最终网络的学习是有利的，而大部分anchor对最终网络的学习都是不利的，这部分的anchor很大程度上影响了整个网络的学习，拉低了整体的准确率；\n  two-stage 网络最终学习的anchor虽然不多，但是背景anchor也就是对网络学习不利的anchor也不会特别多，它虽然也能影响整体的准确率，但是肯定没有one-stage影响得那么严重，所以它的准确率比one-stage肯定要高。\n     MxN 大小的图像经过 Conv layers 得到 (M/16)x(N/16) 的 Feature map 上面分支 1x1 卷积，得到的输出为 WxHx18（W=M/16，H=N/16）   FPN：Feature pyramid networks for object detection.\n对于每张图像，detectors 评估 10^4 - 10^5 个候选位置，但只有少数位置包含对象。这种不平衡会造成两个问题：\n （1）训练是低效的，因为大多数位置都是简单的负样本，没有提供有用的学习信号。 （2）简单的负样本可以压倒训练，并导致退化模型。  R-CNN - Fast R-CNN - Faster R-CNN\nR-CNN: Regions with CNN features\n😁RCNN 算法流程可以分为 4 个步骤：\n 候选区域的生成：Selective Search 算法 对每个候选区域，调整为相同的大小（warped region），使用深度网络提取特征 特征送入每一类的 SVM 分类器，判定类别 使用回归器精细修正候选框位置  R-CNN 框架（4 部分）\n  Region Proposal(Selective Search)\n  Feature extraction(CNN)\n  Classification(SVM)\n  Bounding-box regression(regression)\n  R-CNN 存在的问题：\n 测试速度慢：候选框之间存在大量重叠，提取特征冗余 训练速度慢：过程繁琐 训练所需空间大  Fast R-CNN 与 R-CNN 相比，训练时间快 9 倍，测试推理时间快 213 倍，准确率从 62% 提升至 66%。\n😁Fast R-CNN 算法流程的 3 个步骤：\n 一张图像生成 1K~2K 个候选区域（Selective Search） 将图像输入网络得到相应的特征图，将 SS 算法生成的候选框投影到特征图上获得相应的特征矩阵（1k-2k 的候选框各自的 feature map） 将每个特征矩阵通过 ROI pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。  Fast-RCNN 将整张图像送入网络，紧接着从特征图像上提取相应的候选区域。这些候选区域的特征不需要再重复计算。\nROI Pooling 不限制输入图像的尺寸。\nMulti-task loss\nL = 分类损失 + 边界框回归损失\nFast CNN 框架（2 部分）\n  Region proposal(Selective Search)\n  Feature extraction/Classification/Bounding-box regression(CNN)\n  Faster RCNN：将 Region proposal 也融合进网络中，成为端到端的架构。\nFaster R-CNN 推理速度 5fps（包括候选区域的生成）\n😁Faster R-CNN 算法流程的 3 个步骤\n 将图像输入网络得到相应的特征图 使用 RPN 结构生成候选框，将 RPN 生成的候选框投影到特征图上获得相应的特征矩阵， 将每个特征矩阵通过 ROI pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。  RPN + Fast R-CNN\nRegion Proposal Network 替代 SS 算法。\n",
  "wordCount" : "2847",
  "inLanguage": "en",
  "datePublished": "2021-08-24T10:12:08+08:00",
  "dateModified": "2021-08-24T10:12:08+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://landodo.github.io/posts/20210824-object-detection-anchor/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://landodo.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://landodo.github.io/" accesskey="h" title="Notes (Alt + H)">
                <img src="http://landodo.github.io/logo.png" alt="logo" aria-label="logo"
                    height="30">Notes</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://landodo.github.io/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/tags" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/cs-zoo" title="CS ZOO">
                    <span>CS ZOO</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://landodo.github.io/">Home</a>&nbsp;»&nbsp;<a href="http://landodo.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      关于 Object Detection Anchor 的学习笔记
    </h1>
    <div class="post-meta"><span title='2021-08-24 10:12:08 +0800 CST'>August 24, 2021</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;2847 words

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#anchor" aria-label="anchor">anchor</a><ul>
                        
                <li>
                    <a href="#fast-r-cnn" aria-label="Fast R-CNN">Fast R-CNN</a></li>
                <li>
                    <a href="#faster-r-cnn" aria-label="Faster R-CNN">Faster R-CNN</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="anchor">anchor<a hidden class="anchor" aria-hidden="true" href="#anchor">#</a></h1>
<p>什么是 anchor？</p>
<blockquote>
<p>anchor 是可能包含目标物体的矩形框。在目标检测任务中，通常会为每个像素点预设一个或多个大小和宽高比例不同的 anchor，以此使得图像上密集铺满了许多 anchor，从而覆盖到包含物体的所有位置区域。</p>
</blockquote>
<p>早期的方法：金字塔多尺度 + 滑动窗口，逐尺度逐位置判断“这个尺度的这个位置处有没有认识的目标”，非常的耗时。</p>
<p>基于 anchor 的方法：预设一组不同尺度、不同位置的固定参考框，覆盖几乎所有位置和尺度，每个参考框检测与其交并比大于阈值的目标。anchor 将问题转换为“这个固定的参考框中有没有认识的目标，目标偏离参考框多远”。</p>
<p>RPN是一个conv3x3+两个并列的conv1x1，一边预测anchor中是否包含目标，一边预测目标框偏离固定anchor多远。</p>
<p>feature map上每个位置设置9个参考anchor，这些大约能覆盖边长70~768的目标。下图是Faster R-CNN论文中各个 anchor 形状训练后学习到的平均proposal大小。</p>
<p><img loading="lazy" src="./20210824/1.png" alt=""  />
</p>
<p>特征提取部分：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torchvision.models.vgg <span style="color:#66d9ef">as</span> vgg
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> vgg<span style="color:#f92672">.</span>vgg16(pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    featrue <span style="color:#f92672">=</span> list(model<span style="color:#f92672">.</span>features)[:<span style="color:#ae81ff">30</span>]
</span></span><span style="display:flex;"><span>    featrue_extra <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>featrue)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>))
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> featrue_extra(input)
</span></span></code></pre></div><p>input.shape = <code>torch.Size([4, 3, 256, 256])</code></p>
<p>output.shape = <code>torch.Size([4, 512, 16, 16])</code></p>
<p>backbone 提取的特征图（Featuremap）相对于网络输入图像尺寸缩小了 16 倍。因此，featuremap 中的 1 个像素点就相当于输入图像的 16 个像素点。或者说，featuremap 中的 1x1 区域覆盖了输入图像的 16x16 区域。即 featuremap 中的每个像素点都对应的覆盖了输入图像的区域。</p>
<p>一个点对应 9 个 anchor。（3 种尺度、3 种宽高比的排列组合）</p>
<p><img loading="lazy" src="./20210824/2.png" alt=""  />
</p>
<p>anchor 坐标有可能出现负数的情况。</p>
<p>计算 featuremap 左上角(0,0) 对应的 9 个 anchor 的中心点坐标和长宽之后，其余的点坐标/长宽可以通过<strong>平移</strong>的方式得到。</p>
<p>分类：</p>
<ul>
<li>3x3 卷积，（融合周围 3x3 的区域，更鲁棒？）</li>
<li>1x1 卷积，（融合通道）</li>
<li>？</li>
</ul>
<p>回归：</p>
<ul>
<li>1x1 卷积将通道映射到 36=4x9。4 表示？</li>
</ul>
<p>对分类和回归结进行后处理，生成 RoI（Region of Interest），也称为了 Proposal layer，对 RPN 输出的分类和回归结果进行后处理（如 非极大值抑制 等），得到网络任务包含物体的区域——RoI。</p>
<p>RoI Pooling 的作用是将不同尺寸的各个 RoI 都映射到相同大小。</p>
<p>RoI Pooling：如何划分 7x7 个 bin？当无法整除的情况，有人提出了 RoI Align 和 Precise RoI Pooling。</p>
<p>存在的缺点：每个 bin 中只有一点贡献了梯度，忽略了大部分点的信息。</p>
<p>将 RoI Pooling 后的结果 flatten 成为 vector，输入全连接层进行分类和回归，对应输出的神经元个数分别为物体类别数（n_classes）和每个类别对应的 bbox（n_classes x 4）。</p>
<p>RoIHead 的输出不是预测结果的最终形态，还需要进行一些后处理。</p>
<ul>
<li>将网络输出缩放至原图尺寸（不是网络输入的尺寸）；</li>
<li>对回归的结果去归一化（乘 std, 加 mean），结合 RoIs 的位置和大小计算出 bbox 的位置（左上角坐标和右下角坐标），并且裁剪到原始尺寸范围内；</li>
<li>选择置信度大于阈值的矩形框，最后再使用非极大值抑制剔除重叠度高的 bbox 得到最终的结果。</li>
</ul>
<p>还是搞不太懂，留一段时间吧。先看看 Focal Loss</p>
<hr>
<p>&mdash; 分割线 &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;</p>
<p>😡两个问题：</p>
<ul>
<li>（1）正负样本不平衡</li>
<li>（2）难易样本不平衡</li>
</ul>
<p>二分类问题的标准 Loss 是交叉熵：</p>
<p>$$L_{ce} = -ylog \check{y} - (1-y)log(1 - \check{y})$$</p>
<ul>
<li>当 y = 1 时，$L = -log\check{y}$。（如果网络的输出预测值 $\check{y}$ 越接近于 1，则 loss 越小）</li>
<li>当 y = 0 时，$L = -log(1-\check{y})$（如果网络的预测值 $\check{y}$ 越接近于 0，则 loss 越小）</li>
</ul>
<p>-log 的函数图像：</p>
<p><img loading="lazy" src="./20210824/3.png" alt=""  />
</p>
<p>“硬截断”的 Loss：$L^* = \lambda(y, \check{y}) \cdot L_{ce}$，其中：</p>
<ul>
<li>(y == 1) 且 $\check{y} &gt; 0.5$ 或 (y == 0) 且 $\check{y} &lt; 0.5$ 时，$\lambda(y, \check{y}) = 0$</li>
<li>其他情况，为 1</li>
</ul>
<p><strong>正样本的预测值大于 0.5 的，或者负样本的预测值小于 0.5 的，就都不更新了，把注意力集中在预测不准的那些样本，当然这个阈值可以调整。</strong></p>
<p>上面的 Loss 存在问题，<strong>即它只告诉模型正样本的预测值大于 0.5 就不更新了，却没有告诉它要“保持”大于 0.5</strong>。</p>
<p>进一步的优化是将“硬截断”的 Loss “光滑化”，可导。</p>
<p>GHM(gradient harmonizing mechanism) 是基于 Focal Loss 的改进。</p>
<blockquote>
<p>公式(2)解决了正负样本的不平衡：</p>
</blockquote>
<p><img loading="lazy" src="./20210824/4.png" alt=""  />
</p>
<blockquote>
<p>公式(3)解决了难易样本的不平衡：</p>
</blockquote>
<p><img loading="lazy" src="./20210824/5.png" alt=""  />
</p>
<blockquote>
<p>结合 (2)(3) 得到 Focal Loss：</p>
</blockquote>
<p><img loading="lazy" src="./20210824/6.png" alt=""  />
</p>
<p>😍GHM：不应该过多关注易分样本，但是特别难分的样本（噪声、离群点）也不应该过多的关注。</p>
<p>梯度密度 $GD(g)$：<strong>单位梯度模长g部分的样本个数。</strong></p>
<p>🤑关于目标检测任务中<strong>正负样本不平衡</strong>、<strong>难易样本不平衡</strong>问题的学习笔记</p>
<blockquote>
<p>5分钟理解Focal Loss与GHM——解决样本不平衡利器</p>
<p><a href="https://zhuanlan.zhihu.com/p/80594704">https://zhuanlan.zhihu.com/p/80594704</a></p>
<p>AAAI 2019：把Cross Entropy梯度分布拉‘平’，就能轻松超越Focal Loss</p>
<p><a href="https://zhuanlan.zhihu.com/p/55017036">https://zhuanlan.zhihu.com/p/55017036</a></p>
</blockquote>
<p>Focal Loss （RetinaNet）</p>
<p>one-stage 为什么会差于 tow-stage：</p>
<blockquote>
<ul>
<li>
<p>one-stage 网络最终学习的anchor有很多，但是只有<strong>少数</strong>anchor对最终网络的学习是有利的，而大部分anchor对最终网络的学习都是不利的，这部分的anchor很大程度上影响了整个网络的学习，拉低了整体的准确率；</p>
</li>
<li>
<p>two-stage 网络最终学习的anchor虽然不多，但是背景anchor也就是对网络学习不利的anchor也不会特别多，它虽然也能影响整体的准确率，但是肯定没有one-stage影响得那么严重，所以它的准确率比one-stage肯定要高。</p>
</li>
</ul>
</blockquote>
<hr>
<p><img loading="lazy" src="./20210824/7.jpg" alt=""  />
</p>
<ul>
<li>MxN 大小的图像经过 Conv layers 得到 (M/16)x(N/16) 的 Feature map</li>
<li>上面分支 1x1 卷积，得到的输出为 WxHx18（W=M/16，H=N/16）</li>
</ul>
<hr>
<p>FPN：Feature pyramid networks for object detection.</p>
<p>对于每张图像，detectors 评估 10^4 - 10^5 个候选位置，但只有少数位置包含对象。这种不平衡会造成两个问题：</p>
<ul>
<li>（1）训练是低效的，因为大多数位置都是简单的负样本，没有提供有用的学习信号。</li>
<li>（2）简单的负样本可以压倒训练，并导致退化模型。</li>
</ul>
<p>R-CNN -&gt; Fast R-CNN -&gt; Faster R-CNN</p>
<p>R-CNN: Regions with CNN features</p>
<p>😁RCNN 算法流程可以分为 4 个步骤：</p>
<ol>
<li>候选区域的生成：Selective Search 算法</li>
<li>对每个候选区域，调整为相同的大小（warped region），使用深度网络提取特征</li>
<li>特征送入每一类的 SVM 分类器，判定类别</li>
<li>使用回归器精细修正候选框位置</li>
</ol>
<p>R-CNN 框架（4 部分）</p>
<ul>
<li>
<p>Region Proposal(Selective Search)</p>
</li>
<li>
<p>Feature extraction(CNN)</p>
</li>
<li>
<p>Classification(SVM)</p>
</li>
<li>
<p>Bounding-box regression(regression)</p>
</li>
</ul>
<p>R-CNN 存在的问题：</p>
<ol>
<li>测试速度慢：候选框之间存在大量重叠，提取特征冗余</li>
<li>训练速度慢：过程繁琐</li>
<li>训练所需空间大</li>
</ol>
<h2 id="fast-r-cnn">Fast R-CNN<a hidden class="anchor" aria-hidden="true" href="#fast-r-cnn">#</a></h2>
<p>与 R-CNN 相比，训练时间快 9 倍，测试推理时间快 213 倍，准确率从 62% 提升至 66%。</p>
<p>😁Fast R-CNN 算法流程的 3 个步骤：</p>
<ol>
<li>一张图像生成 1K~2K 个候选区域（Selective Search）</li>
<li>将图像输入网络得到相应的特征图，将 SS 算法生成的候选框<strong>投影</strong>到特征图上获得相应的特征矩阵（1k-2k 的候选框各自的 feature map）</li>
<li>将每个特征矩阵通过 ROI pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。</li>
</ol>
<p>Fast-RCNN 将整张图像送入网络，紧接着从特征图像上提取相应的候选区域。这些候选区域的特征不需要再重复计算。</p>
<p>ROI Pooling 不限制输入图像的尺寸。</p>
<p>Multi-task loss</p>
<p>L = 分类损失 + 边界框回归损失</p>
<p>Fast CNN 框架（2 部分）</p>
<ul>
<li>
<p>Region proposal(Selective Search)</p>
</li>
<li>
<p>Feature extraction/Classification/Bounding-box regression(CNN)</p>
</li>
</ul>
<p>Faster RCNN：将 Region proposal 也融合进网络中，成为端到端的架构。</p>
<h2 id="faster-r-cnn">Faster R-CNN<a hidden class="anchor" aria-hidden="true" href="#faster-r-cnn">#</a></h2>
<p>推理速度 5fps（包括候选区域的生成）</p>
<p>😁Faster R-CNN 算法流程的 3 个步骤</p>
<ol>
<li>将图像输入网络得到相应的特征图</li>
<li>使用 RPN 结构生成候选框，将 RPN 生成的候选框投影到特征图上获得相应的特征矩阵，</li>
<li>将每个特征矩阵通过 ROI pooling 层缩放到 7x7 大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。</li>
</ol>
<p>RPN + Fast R-CNN</p>
<p>Region Proposal Network 替代 SS 算法。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://landodo.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li>
      <li><a href="http://landodo.github.io/tags/%E7%AC%94%E8%AE%B0/">笔记</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://landodo.github.io/posts/20210827-focal-loss/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Focal Loss for Dense Object Detection</span>
  </a>
  <a class="next" href="http://landodo.github.io/posts/20210806-medical-image-basic/">
    <span class="title">Next Page »</span>
    <br>
    <span>医学图像基础</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Landon</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
