<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<meta name="robots" content="index, follow">
<title>CIFAR-10、PyTorch 和 AlexNet | Notes</title>
<meta name="keywords" content="论文阅读, AlexNet, PyTorch" />
<meta name="description" content="CIFAR-10、PyTorch 和 AlexNet 先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。 这些经典的神经网络结构，要熟悉">
<meta name="author" content="">
<link rel="canonical" href="http://landodo.github.io/posts/20201109-alexnet-code/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<link rel="preload" href="./logo.png" as="image">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://landodo.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://landodo.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://landodo.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://landodo.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://landodo.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="CIFAR-10、PyTorch 和 AlexNet" />
<meta property="og:description" content="CIFAR-10、PyTorch 和 AlexNet 先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。 这些经典的神经网络结构，要熟悉" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://landodo.github.io/posts/20201109-alexnet-code/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-09T10:17:29&#43;08:00" />
<meta property="article:modified_time" content="2020-11-09T10:17:29&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CIFAR-10、PyTorch 和 AlexNet"/>
<meta name="twitter:description" content="CIFAR-10、PyTorch 和 AlexNet 先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。 这些经典的神经网络结构，要熟悉"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://landodo.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "CIFAR-10、PyTorch 和 AlexNet",
      "item": "http://landodo.github.io/posts/20201109-alexnet-code/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CIFAR-10、PyTorch 和 AlexNet",
  "name": "CIFAR-10、PyTorch 和 AlexNet",
  "description": "CIFAR-10、PyTorch 和 AlexNet 先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。 这些经典的神经网络结构，要熟悉",
  "keywords": [
    "论文阅读", "AlexNet", "PyTorch"
  ],
  "articleBody": "CIFAR-10、PyTorch 和 AlexNet 先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。\n这些经典的神经网络结构，要熟悉到能白板编程的技能。\n最近看到有关机器学习的一个推特，觉得很有启发。\n What a lot of machine learning courses \u0026 books teach:\n Load clean data Train an MNIST-* classifier with 99% accuracy  What they should actually be teaching:\n Process, clean, load your own data Fail many experiments and research/find/understand ways to improve your ML model   很多机器学习课程与书籍所教授的内容：\n 加载干净的数据 训练一个MNIST-* 分类器，准确率达 99%。  他们其实应该教什么。\n 处理、清理、加载自己的数据 多次实验失败，研究/发现/了解改进你的 ML 模型的方法。   环境准备 第一步：启动虚拟环境\n➜ source activate ldl-env 第二步：安装 PyTorch\n(ldl-env) ➜ workspace conda install pytorch torchvision torchaudio -c pytorch  要多看 PyTorch 的官方文档。\n 数据集 torchvision.datasets torchvision.datasets 中包含了较多的数据集\n  MNIST（0-9 手写数字）\n  COCO（用于图像标注和目标检测）\n  LSUN（包含数百万个场景和对象的彩色图像）\n  CIFAR-10\n  CIFAR-10 dataset (Canadian Institute For Advanced Research) ：https://www.cs.toronto.edu/~kriz/cifar.html\nCIFAR-10 和 CIFAR-100 是一个包含 8000 万张微小图像的数据集，由 Alex Krizhevsky、Vinod Nair 和 Geoffrey Hinton 收集。\nCIFAR-10 数据集包含 10 个类别，总共 60000 张 $$32\\times32$$ 彩色图像。\n每个类别 6000 张图像。\n有 50000 张训练图像（train set）和 10000 张测试图像（test set）。\n数据集分为 5 个训练 Batch 和 1 个测试 Batch。每个Train Batch 有 10000 张图像，Test Batch 正好包含 1000 张从每个类中随机选取的图像。 Train Batch 按随机顺序包含剩余的图像，但一些 Train Batch 可能包含来自一个类的图像比另一个类更多。在它们之间，Train batch 正好包含来自每个类的 5000 张图像。\n Fashion-MNIST  GitHub: https://github.com/zalandoresearch/fashion-mnist\n每个训练和测试样本都按照以下类别进行了标注：\n   标注编号 描述     0 T-shirt/top（T恤）   1 Trouser（裤子）   2 Pullover（套衫）   3 Dress（裙子）   4 Coat（外套）   5 Sandal（凉鞋）   6 Shirt（汗衫）   7 Sneaker（运动鞋）   8 Bag（包）   9 Ankle boot（踝靴）    找到了一个可供参考的小项目，数据集是 CIFAR-10。\nAlexNet GitHub：pytorch-cifar10 输入图片的大小为 $$32\\times32\\times3$$\nclass AlexNet(nn.Module):  def __init__(self, num_classes=NUM_CLASSES):  super(AlexNet, self).__init__()  self.features = nn.Sequential(  nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=2),  nn.Conv2d(64, 192, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=2),  nn.Conv2d(192, 384, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.Conv2d(384, 256, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.Conv2d(256, 256, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=2),  )  self.classifier = nn.Sequential(  nn.Dropout(),  nn.Linear(256 * 2 * 2, 4096),  nn.ReLU(inplace=True),  nn.Dropout(),  nn.Linear(4096, 4096),  nn.ReLU(inplace=True),  nn.Linear(4096, num_classes),  )   def forward(self, x):  x = self.features(x)  x = x.view(x.size(0), 256 * 2 * 2)  x = self.classifier(x)  return x 我按照吴恩达老师的视频教程，绘制了一个图，如下。\nPapers with Code 这里的图片输入应该是 $$227\\times227\\times3$$，所以网络的内部 kernel_size 和 stride 会有不同。\nclass AlexNet(nn.Module):  \"\"\" Neural network model consisting of layers propsed by AlexNet paper. \"\"\"  def __init__(self, num_classes=1000):  \"\"\" Define and allocate layers for this neural net. Args: num_classes (int): number of classes to predict with this model \"\"\"  super().__init__()  # input size should be : (b x 3 x 227 x 227)  # The image in the original paper states that width and height are 224 pixels, but  # the dimensions after first convolution layer do not lead to 55 x 55.  self.net = nn.Sequential(  nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4), # (b x 96 x 55 x 55)  nn.ReLU(),  nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), # section 3.3  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 96 x 27 x 27)  nn.Conv2d(96, 256, 5, padding=2), # (b x 256 x 27 x 27)  nn.ReLU(),  nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 13 x 13)  nn.Conv2d(256, 384, 3, padding=1), # (b x 384 x 13 x 13)  nn.ReLU(),  nn.Conv2d(384, 384, 3, padding=1), # (b x 384 x 13 x 13)  nn.ReLU(),  nn.Conv2d(384, 256, 3, padding=1), # (b x 256 x 13 x 13)  nn.ReLU(),  nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 6 x 6)  )  # classifier is just a name for linear layers  self.classifier = nn.Sequential(  nn.Dropout(p=0.5, inplace=True),  nn.Linear(in_features=(256 * 6 * 6), out_features=4096),  nn.ReLU(),  nn.Dropout(p=0.5, inplace=True),  nn.Linear(in_features=4096, out_features=4096),  nn.ReLU(),  nn.Linear(in_features=4096, out_features=num_classes),  )  self.init_bias() # initialize bias   def init_bias(self):  for layer in self.net:  if isinstance(layer, nn.Conv2d):  nn.init.normal_(layer.weight, mean=0, std=0.01)  nn.init.constant_(layer.bias, 0)  # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers  nn.init.constant_(self.net[4].bias, 1)  nn.init.constant_(self.net[10].bias, 1)  nn.init.constant_(self.net[12].bias, 1)   def forward(self, x):  \"\"\" Pass the input through the net. Args: x (Tensor): input tensor Returns: output (Tensor): output tensor \"\"\"  x = self.net(x)  x = x.view(-1, 256 * 6 * 6) # reduce the dimensions for linear layer input  return self.classifier(x) Zhihu 知乎 https://zhuanlan.zhihu.com/p/29786939\n给出了局部响应归一化的实现。\nimport torch.nn as nn from torch.nn import functional as F from torch.autograd import Variable  class LRN(nn.Module):  def __init__(self, local_size=1, alpha=1.0, beta=0.75, ACROSS_CHANNELS=False):  super(LRN, self).__init__()  self.ACROSS_CHANNELS = ACROSS_CHANNELS  if self.ACROSS_CHANNELS:  self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1), #0.2.0_4会报错，需要在最新的分支上AvgPool3d才有padding参数  stride=1,  padding=(int((local_size-1.0)/2), 0, 0))  else:  self.average=nn.AvgPool2d(kernel_size=local_size,  stride=1,  padding=int((local_size-1.0)/2))  self.alpha = alpha  self.beta = beta    def forward(self, x):  if self.ACROSS_CHANNELS:  div = x.pow(2).unsqueeze(1)  div = self.average(div).squeeze(1)  div = div.mul(self.alpha).add(1.0).pow(self.beta)#这里的1.0即为bias  else:  div = x.pow(2)  div = self.average(div)  div = div.mul(self.alpha).add(1.0).pow(self.beta)  x = x.div(div)  return x 这个专门分了 layer1， layer2， layer3 ，更加的好理解了。\nfrom torch import nn from torch.nn import functional as F from torch.autograd import Variable  import torch class AlexNet(nn.Module):  def __init__(self, num_classes = 1000):#imagenet数量  super().__init__()  self.layer1 = nn.Sequential(  nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=3, stride=2),  LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)  )   self.layer2 = nn.Sequential(  nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, groups=2, padding=2),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=3, stride=2),  LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)  )   self.layer3 = nn.Sequential(  nn.Conv2d(in_channels=256, out_channels=384, padding=1, kernel_size=3),  nn.ReLU(inplace=True)  )  self.layer4 = nn.Sequential(  nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),  nn.ReLU(inplace=True)  )   self.layer5 = nn.Sequential(  nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),  nn.ReLU(inplace=True),  nn.MaxPool2d(kernel_size=3, stride=2)  )   #需要针对上一层改变view  self.layer6 = nn.Sequential(  nn.Linear(in_features=6*6*256, out_features=4096),  nn.ReLU(inplace=True),  nn.Dropout()  )  self.layer7 = nn.Sequential(  nn.Linear(in_features=4096, out_features=4096),  nn.ReLU(inplace=True),  nn.Dropout()  )   self.layer8 = nn.Linear(in_features=4096, out_features=num_classes)   def forward(self, x):  x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))  x = x.view(-1, 6*6*256)  x = self.layer8(self.layer7(self.layer6(x)))   return x 思考 我有几个疑问？\n  AlexNet 论文里面的数据集使用的是 $$256\\times256\\times3$$，采用数据增强后真正输入到网络的图像 size 为 $$224\\times224\\times3$$，但是编程实现的时候，输入的大多是 $$227\\times227\\times3$$ 这个 227 是怎么算出来的？\n   2021.02.05：224 应该是作者的一个失误，其实没关系的。将 224 代入网络，就会发现 224 存在的问题，227 是一个更好的取值。\n     我感觉有人用 224，有人用 227，有什么区别吗？\n   2021.02.05：用 227，不要用 224。代入网络推一遍就知道为什么了！\n     上文我记录的第一个程序 pytorch-cifar10，每张图像的输入是 $$32\\times32\\times3$$。这种情况下一种处理方式是 resize 成 224；另一种是更改网络的 kernel_size、padding、stride，修改过之后的网络还能叫做是 AlexNet 吗？\n   2021.02.05：我认为修改后的网络就不能叫做 AlexNet 了。\nAlexNet 中的卷积核大小、步长、填充都是作者为 ImageNet 数据集精心设计的。\n     ",
  "wordCount" : "2091",
  "inLanguage": "en",
  "datePublished": "2020-11-09T10:17:29+08:00",
  "dateModified": "2020-11-09T10:17:29+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://landodo.github.io/posts/20201109-alexnet-code/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://landodo.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://landodo.github.io/" accesskey="h" title="Notes (Alt + H)">
                <img src="http://landodo.github.io/logo.png" alt="logo" aria-label="logo"
                    height="30">Notes</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://landodo.github.io/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/tags" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/cs-zoo" title="CS ZOO">
                    <span>CS ZOO</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://landodo.github.io/">Home</a>&nbsp;»&nbsp;<a href="http://landodo.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      CIFAR-10、PyTorch 和 AlexNet
    </h1>
    <div class="post-meta"><span title='2020-11-09 10:17:29 +0800 CST'>November 9, 2020</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;2091 words

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#cifar-10pytorch-%e5%92%8c-alexnet" aria-label="CIFAR-10、PyTorch 和 AlexNet">CIFAR-10、PyTorch 和 AlexNet</a></li>
                <li>
                    <a href="#%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87" aria-label="环境准备">环境准备</a></li>
                <li>
                    <a href="#%e6%95%b0%e6%8d%ae%e9%9b%86" aria-label="数据集">数据集</a><ul>
                        
                <li>
                    <a href="#torchvisiondatasets" aria-label="torchvision.datasets">torchvision.datasets</a></li></ul>
                </li>
                <li>
                    <a href="#alexnet" aria-label="AlexNet">AlexNet</a><ul>
                        
                <li>
                    <a href="#githubpytorch-cifar10" aria-label="GitHub：pytorch-cifar10">GitHub：pytorch-cifar10</a></li>
                <li>
                    <a href="#papers-with-code" aria-label="Papers with Code">Papers with Code</a></li>
                <li>
                    <a href="#zhihu-%e7%9f%a5%e4%b9%8e" aria-label="Zhihu 知乎">Zhihu 知乎</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%80%9d%e8%80%83" aria-label="思考">思考</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="cifar-10pytorch-和-alexnet">CIFAR-10、PyTorch 和 AlexNet<a hidden class="anchor" aria-hidden="true" href="#cifar-10pytorch-和-alexnet">#</a></h2>
<p>先在自己的电脑上走一遍，保证语法和逻辑不会错。再放到远程配置高的电脑上跑。</p>
<p>这些经典的神经网络结构，要熟悉到能白板编程的技能。</p>
<p>最近看到有关机器学习的一个推特，觉得很有启发。</p>
<blockquote>
<p>What a lot of machine learning courses &amp; books teach:</p>
<ul>
<li>Load clean data</li>
<li>Train an MNIST-* classifier with 99% accuracy</li>
</ul>
<p>What they should actually be teaching:</p>
<ul>
<li>Process, clean, load your own data</li>
<li>Fail many experiments and research/find/understand ways to improve your ML model</li>
</ul>
<hr>
<p>很多机器学习课程与书籍所教授的内容：</p>
<ul>
<li>加载干净的数据</li>
<li>训练一个MNIST-* 分类器，准确率达 99%。</li>
</ul>
<p>他们其实应该教什么。</p>
<ul>
<li>处理、清理、加载自己的数据</li>
<li>多次实验失败，研究/发现/了解改进你的 ML 模型的方法。</li>
</ul>
</blockquote>
<p><img loading="lazy" src="./20201109/tips.png" alt=""  />
</p>
<h2 id="环境准备">环境准备<a hidden class="anchor" aria-hidden="true" href="#环境准备">#</a></h2>
<p>第一步：启动虚拟环境</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>➜ source activate ldl-env
</span></span></code></pre></div><p>第二步：安装 PyTorch</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">(</span>ldl-env<span style="color:#f92672">)</span> ➜  workspace conda install pytorch torchvision torchaudio -c pytorch
</span></span></code></pre></div><p><img loading="lazy" src="./20201109/1.PNG" alt=""  />
</p>
<blockquote>
<p>要多看 PyTorch 的官方文档。</p>
</blockquote>
<h2 id="数据集">数据集<a hidden class="anchor" aria-hidden="true" href="#数据集">#</a></h2>
<h3 id="torchvisiondatasets">torchvision.datasets<a hidden class="anchor" aria-hidden="true" href="#torchvisiondatasets">#</a></h3>
<p><code>torchvision.datasets </code>中包含了较多的<a href="https://pytorch.org/docs/stable/torchvision/datasets.html">数据集</a></p>
<ul>
<li>
<p>MNIST（0-9 手写数字）</p>
</li>
<li>
<p>COCO（用于图像标注和目标检测）</p>
</li>
<li>
<p>LSUN（包含数百万个场景和对象的彩色图像）</p>
</li>
<li>
<p><strong>CIFAR-10</strong></p>
</li>
</ul>
<p><img loading="lazy" src="./20201109/3.png" alt=""  />
</p>
<p>CIFAR-10 dataset (Canadian Institute For Advanced Research) ：<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p>
<p>CIFAR-10 和 CIFAR-100 是一个包含 8000 万张微小图像的数据集，由 Alex Krizhevsky、Vinod Nair 和 Geoffrey Hinton 收集。</p>
<p><strong>CIFAR-10</strong> 数据集包含 10 个类别，总共 60000 张 $$32\times32$$ 彩色图像。</p>
<p>每个类别 6000 张图像。</p>
<p>有 50000 张训练图像（train set）和 10000 张测试图像（test set）。</p>
<p>数据集分为 5 个训练 Batch 和 1 个测试 Batch。每个Train Batch 有 10000 张图像，Test Batch 正好包含 1000 张从每个类中随机选取的图像。 Train Batch 按随机顺序包含剩余的图像，但一些 Train Batch 可能包含来自一个类的图像比另一个类更多。在它们之间，Train batch 正好包含来自每个类的 5000 张图像。</p>
<p><img loading="lazy" src="./20201109/4.png" alt=""  />
</p>
<ul>
<li>Fashion-MNIST</li>
</ul>
<p>GitHub: <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a></p>
<p><img loading="lazy" src="./20201109/fashion-mnist-sprite.png" alt=""  />
</p>
<p>每个训练和测试样本都按照以下类别进行了标注：</p>
<table>
<thead>
<tr>
<th>标注编号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>T-shirt/top（T恤）</td>
</tr>
<tr>
<td>1</td>
<td>Trouser（裤子）</td>
</tr>
<tr>
<td>2</td>
<td>Pullover（套衫）</td>
</tr>
<tr>
<td>3</td>
<td>Dress（裙子）</td>
</tr>
<tr>
<td>4</td>
<td>Coat（外套）</td>
</tr>
<tr>
<td>5</td>
<td>Sandal（凉鞋）</td>
</tr>
<tr>
<td>6</td>
<td>Shirt（汗衫）</td>
</tr>
<tr>
<td>7</td>
<td>Sneaker（运动鞋）</td>
</tr>
<tr>
<td>8</td>
<td>Bag（包）</td>
</tr>
<tr>
<td>9</td>
<td>Ankle boot（踝靴）</td>
</tr>
</tbody>
</table>
<p>找到了一个可供参考的小项目，数据集是 CIFAR-10。</p>
<p><img loading="lazy" src="./20201109/6.png" alt=""  />
</p>
<h2 id="alexnet">AlexNet<a hidden class="anchor" aria-hidden="true" href="#alexnet">#</a></h2>
<h3 id="githubpytorch-cifar10">GitHub：pytorch-cifar10<a hidden class="anchor" aria-hidden="true" href="#githubpytorch-cifar10">#</a></h3>
<p>输入图片的大小为 $$32\times32\times3$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AlexNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_classes<span style="color:#f92672">=</span>NUM_CLASSES):
</span></span><span style="display:flex;"><span>        super(AlexNet, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>features <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">192</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">192</span>, <span style="color:#ae81ff">384</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">256</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4096</span>, <span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4096</span>, num_classes),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>features(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">256</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>classifier(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>我按照吴恩达老师的视频教程，绘制了一个图，如下。</p>
<p><img loading="lazy" src="./20201109/5.jpg" alt=""  />
</p>
<h3 id="papers-with-code">Papers with Code<a hidden class="anchor" aria-hidden="true" href="#papers-with-code">#</a></h3>
<p>这里的图片输入应该是 $$227\times227\times3$$，所以网络的内部 <code>kernel_size</code> 和 <code>stride</code> 会有不同。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AlexNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Neural network model consisting of layers propsed by AlexNet paper.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Define and allocate layers for this neural net.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            num_classes (int): number of classes to predict with this model
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># input size should be : (b x 3 x 227 x 227)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># The image in the original paper states that width and height are 224 pixels, but</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># the dimensions after first convolution layer do not lead to 55 x 55.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>net <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">96</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>),  <span style="color:#75715e"># (b x 96 x 55 x 55)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>LocalResponseNorm(size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># section 3.3</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 96 x 27 x 27)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">96</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">5</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 256 x 27 x 27)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>LocalResponseNorm(size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 256 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  <span style="color:#75715e"># (b x 384 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  <span style="color:#75715e"># (b x 384 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  <span style="color:#75715e"># (b x 256 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 256 x 6 x 6)</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># classifier is just a name for linear layers</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span>(<span style="color:#ae81ff">256</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span>), out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>, out_features<span style="color:#f92672">=</span>num_classes),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>init_bias()  <span style="color:#75715e"># initialize bias</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_bias</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>net:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(layer, nn<span style="color:#f92672">.</span>Conv2d):
</span></span><span style="display:flex;"><span>                nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>normal_(layer<span style="color:#f92672">.</span>weight, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>                nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(layer<span style="color:#f92672">.</span>bias, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers</span>
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(self<span style="color:#f92672">.</span>net[<span style="color:#ae81ff">4</span>]<span style="color:#f92672">.</span>bias, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(self<span style="color:#f92672">.</span>net[<span style="color:#ae81ff">10</span>]<span style="color:#f92672">.</span>bias, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(self<span style="color:#f92672">.</span>net[<span style="color:#ae81ff">12</span>]<span style="color:#f92672">.</span>bias, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Pass the input through the net.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x (Tensor): input tensor
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            output (Tensor): output tensor
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>net(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">256</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span>)  <span style="color:#75715e"># reduce the dimensions for linear layer input</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>classifier(x)
</span></span></code></pre></div><h3 id="zhihu-知乎">Zhihu 知乎<a hidden class="anchor" aria-hidden="true" href="#zhihu-知乎">#</a></h3>
<p><a href="https://zhuanlan.zhihu.com/p/29786939">https://zhuanlan.zhihu.com/p/29786939</a></p>
<p>给出了局部响应归一化的实现。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.nn <span style="color:#f92672">import</span> functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LRN</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, local_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, ACROSS_CHANNELS<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        super(LRN, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ACROSS_CHANNELS <span style="color:#f92672">=</span> ACROSS_CHANNELS
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>ACROSS_CHANNELS:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>average<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>AvgPool3d(kernel_size<span style="color:#f92672">=</span>(local_size, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), <span style="color:#75715e">#0.2.0_4会报错，需要在最新的分支上AvgPool3d才有padding参数</span>
</span></span><span style="display:flex;"><span>                    stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>                    padding<span style="color:#f92672">=</span>(int((local_size<span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>), <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>)) 
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>average<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>AvgPool2d(kernel_size<span style="color:#f92672">=</span>local_size,
</span></span><span style="display:flex;"><span>                    stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>                    padding<span style="color:#f92672">=</span>int((local_size<span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>beta <span style="color:#f92672">=</span> beta
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>ACROSS_CHANNELS:
</span></span><span style="display:flex;"><span>            div <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>pow(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            div <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>average(div)<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            div <span style="color:#f92672">=</span> div<span style="color:#f92672">.</span>mul(self<span style="color:#f92672">.</span>alpha)<span style="color:#f92672">.</span>add(<span style="color:#ae81ff">1.0</span>)<span style="color:#f92672">.</span>pow(self<span style="color:#f92672">.</span>beta)<span style="color:#75715e">#这里的1.0即为bias</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            div <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>pow(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>            div <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>average(div)
</span></span><span style="display:flex;"><span>            div <span style="color:#f92672">=</span> div<span style="color:#f92672">.</span>mul(self<span style="color:#f92672">.</span>alpha)<span style="color:#f92672">.</span>add(<span style="color:#ae81ff">1.0</span>)<span style="color:#f92672">.</span>pow(self<span style="color:#f92672">.</span>beta)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>div(div)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>这个专门分了 <code>layer1</code>， <code>layer2</code>， <code>layer3</code> ，更加的好理解了。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.nn <span style="color:#f92672">import</span> functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AlexNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_classes <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>):<span style="color:#75715e">#imagenet数量</span>
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">96</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            LRN(local_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-4</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, ACROSS_CHANNELS<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">96</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, groups<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            LRN(local_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-4</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, ACROSS_CHANNELS<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer4 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer5 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>         <span style="color:#75715e">#需要针对上一层改变view</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer6 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span><span style="color:#f92672">*</span><span style="color:#ae81ff">6</span><span style="color:#f92672">*</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout()
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer7 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout()
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer8 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>, out_features<span style="color:#f92672">=</span>num_classes)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layer5(self<span style="color:#f92672">.</span>layer4(self<span style="color:#f92672">.</span>layer3(self<span style="color:#f92672">.</span>layer2(self<span style="color:#f92672">.</span>layer1(x)))))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span><span style="color:#f92672">*</span><span style="color:#ae81ff">6</span><span style="color:#f92672">*</span><span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layer8(self<span style="color:#f92672">.</span>layer7(self<span style="color:#f92672">.</span>layer6(x)))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><h2 id="思考">思考<a hidden class="anchor" aria-hidden="true" href="#思考">#</a></h2>
<p>我有几个疑问？</p>
<ul>
<li>
<p>AlexNet 论文里面的数据集使用的是 $$256\times256\times3$$，采用数据增强后真正输入到网络的图像 size 为 $$224\times224\times3$$，但是编程实现的时候，输入的大多是 $$227\times227\times3$$ 这个 227 是怎么算出来的？</p>
<ul>
<li>
<blockquote>
<p>2021.02.05：224 应该是作者的一个失误，其实没关系的。将 224 代入网络，就会发现 224 存在的问题，227 是一个更好的取值。</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>我感觉有人用 224，有人用 227，有什么区别吗？</p>
<ul>
<li>
<blockquote>
<p>2021.02.05：用 227，不要用 224。代入网络推一遍就知道为什么了！</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>上文我记录的第一个程序 <code>pytorch-cifar10</code>，每张图像的输入是 $$32\times32\times3$$。这种情况下一种处理方式是 <code>resize</code> 成 224；另一种是更改网络的 kernel_size、padding、stride，修改过之后的网络还能叫做是 AlexNet 吗？</p>
<ul>
<li>
<blockquote>
<p>2021.02.05：我认为修改后的网络就不能叫做 AlexNet 了。</p>
<p>AlexNet 中的卷积核大小、步长、填充都是作者为 ImageNet 数据集精心设计的。</p>
</blockquote>
</li>
</ul>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://landodo.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></li>
      <li><a href="http://landodo.github.io/tags/alexnet/">AlexNet</a></li>
      <li><a href="http://landodo.github.io/tags/pytorch/">PyTorch</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://landodo.github.io/posts/20201113-http/">
    <span class="title">« Prev Page</span>
    <br>
    <span>使用 C 语言实现一个 HTTP 服务器（2）</span>
  </a>
  <a class="next" href="http://landodo.github.io/posts/20201215-network-in-network/">
    <span class="title">Next Page »</span>
    <br>
    <span>Network In Network</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Landon</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
