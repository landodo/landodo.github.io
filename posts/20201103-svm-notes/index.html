<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<meta name="robots" content="index, follow">
<title>SVM 支持向量机推导 | </title>
<meta name="keywords" content="机器学" />
<meta name="description" content="SVM 笔记 1: $\min_{w,b}\frac{1}{2}\left\Vert w \right\Vert^2_2$ 的推导 学习率应该怎样去选择？ 鲁莽司机（Bold Driver） 每轮之后在一个 dataset 上进行评价 $$\alpha_t=\frac{\al">
<meta name="author" content="">
<link rel="canonical" href="http://landodo.github.io/posts/20201103-svm-notes/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<link rel="preload" href="./logo.png" as="image">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://landodo.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://landodo.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://landodo.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://landodo.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://landodo.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="SVM 支持向量机推导" />
<meta property="og:description" content="SVM 笔记 1: $\min_{w,b}\frac{1}{2}\left\Vert w \right\Vert^2_2$ 的推导 学习率应该怎样去选择？ 鲁莽司机（Bold Driver） 每轮之后在一个 dataset 上进行评价 $$\alpha_t=\frac{\al" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://landodo.github.io/posts/20201103-svm-notes/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-03T10:17:29&#43;08:00" />
<meta property="article:modified_time" content="2020-11-03T10:17:29&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="SVM 支持向量机推导"/>
<meta name="twitter:description" content="SVM 笔记 1: $\min_{w,b}\frac{1}{2}\left\Vert w \right\Vert^2_2$ 的推导 学习率应该怎样去选择？ 鲁莽司机（Bold Driver） 每轮之后在一个 dataset 上进行评价 $$\alpha_t=\frac{\al"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://landodo.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "SVM 支持向量机推导",
      "item": "http://landodo.github.io/posts/20201103-svm-notes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SVM 支持向量机推导",
  "name": "SVM 支持向量机推导",
  "description": "SVM 笔记 1: $\\min_{w,b}\\frac{1}{2}\\left\\Vert w \\right\\Vert^2_2$ 的推导 学习率应该怎样去选择？ 鲁莽司机（Bold Driver） 每轮之后在一个 dataset 上进行评价 $$\\alpha_t=\\frac{\\al",
  "keywords": [
    "机器学"
  ],
  "articleBody": "SVM 笔记 1: $\\min_{w,b}\\frac{1}{2}\\left\\Vert w \\right\\Vert^2_2$ 的推导 学习率应该怎样去选择？\n 鲁莽司机（Bold Driver） 每轮之后在一个 dataset 上进行评价 $$\\alpha_t=\\frac{\\alpha_0}{1+t/T}$$，学习率随着 t 不断下降  分类任务\n  Linear SVM：最简单的 SVM 的形式\n  Kernelized SVM：核化的一种 SVM 方法\n  回归任务\n SVR（Support Vector Regression）  SVM 这个方法用得并没有那么广泛，这个算法最大的一个好处就是它比较稳定。从 ML 上来看，它是一个很漂亮的方法，它背后有很好的思想。\n泛化能力的分析（Generalization Analysis），有两本书，一本厚的、一本薄的，其中 SVM 就是其中一个核心的算法。现有的理论，后来才有的算法。\n Generalization Analysis: Vapnik, the nature of Statistical Learning Theory, Spinger, 1995. Generalization Analysis: Vapnik, Statistical Learning Theory, Wiley\u0026Sons, 1998.  SMO：下一节课会讲\n工具包：\n SVM Light LIBSVM Multi-Class SVM and StructSVM：多分类、结构化数据  什么是 SVM？它究竟在做些什么事情？ 从最简单的情况出发，假设数据线性可分。\n如下的直线都可以完美的将数据分开，分类面不唯一，所有分类面的训练误差都等于 0，那么该如何选择分类面呢。\n应该选择泛化误差最小的那一个。那么怎么去考量泛化误差最小的呢？正确和错误之间有一个较大的容忍度，才能容忍更多的测试数据。\n从肉眼来看，这条黑色的线是最好的，因为它之间的间隔充分的大，容忍区域宽。这是从几何的角度看的。\n怎么样将这种思想用一种可计算的方式表达出来呢？从而有一个算法能够自动的找到那条黑色的线。这是 SVM 算法的一个出发点。\n从以前的算法，看看能得到什么启示。\n逻辑回归（Logistic Regression） 逻辑回归是一个概率模型，概率模型的好处是：每次算给定的 x 的标签的时候，事实上算的是一个概率，有了概率之后，就有了一个叫做置信度的概念。\n有了概率就可以做极大似然估计。置信度告诉我们分类结果有多大的把握。\n$$P(y = 1|x)=\\frac{1}{1+e^{(-w^Tx)}}$$\n概率模型都会有一个置信度。\n置信度和前面的那条黑色的分割线有什么关系？可以理解为：黑色的线有较大的置信度。\n在没有概率模型的情况下，该怎么定义置信度呢？这就是 SVM 最初要解决的东西：如何定义置信度。\nSVM 通过一个叫做 Margin 的东西来定义置信度。SVM 方法也叫做 Max Margin。\n注意，最大化Margin是一种思想，SVM 只是其中一个典型的代表。\nMargin 刻画置信度这一概念。\nMargin 在数学上的表达  假设在做一个二分类问题，特征是 x，标签是 y。 用一个线性的模型来做分类  $$f_{w,b}(x) = g(w^Tx+b).$$ 当 $$w^Tx+b=0$$ ，则为类 1；否则为类 -1.   分类面：$$w^Tx+b = 0$$  接下来来定义Margin Margin，衡量一个点到一个分类面的一个远近程度。\n 函数Margin（Functional Margin）  对于每一个训练数据$$(x^{(i)},y^{(i)})$$，得到一个值，如下就是函数 Margin 的。\n$$\\check{\\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)}+b)$$\n$$\\check{\\gamma}^{(i)}$$是一个大于 0 的值，它不是距离，但是和距离有一定关系。\n 因为如果标签 $$y^{(i)}=1$$，则需要 $$w^Tx+b$$ 是一个正的值 因为如果标签 $$y^{(i)}=-1$$，则需要 $$w^Tx+b$$ 是一个负的值  函数 Margin 只是想象中有一个量去尽可能的刻画置信度的作用，什么时候可以把它真正定义成一个距离呢？这就是几何 Margin 所定义的内容。\n 几何 Margin（Geometric Margin）  假设 A 点的坐标是 $$x^{(i)}$$，它与 $$\\vec{OA}$$ 等价，一个样本可以理解为空间中的一点，不同维度对应它的不同特征。\n则 B 点的坐标可以通过 A 点推导出来。\n$$B 点的坐标 = x^{(i)}-\\gamma^{(i)}w/\\left\\Vert w \\right\\Vert_{2}$$\n 这个式子的推导如下：\n$$\\frac{w}{\\left\\vert w \\right\\vert}$$ 为 $w$ 方向上的单位向量。\n$$\\gamma^{(i)}w/\\left\\vert w \\right\\vert$$ 得到的是 $\\vec{BA}$\n我们知道 $$\\vec{OB} = \\vec{OA} + \\vec{AB}$$，$$\\vec{OB}$$ 等价于 B 点的坐标。\n因此 $$\\vec{OB} = \\vec{OA} - \\vec{BA}$$\n得到式子：$$\\vec{OB} = x^{(i)}-\\gamma^{(i)}w/\\left\\Vert w \\right\\Vert_2$$. 推导完成。\n注：$$\\left\\Vert \\right\\Vert_2$$ 相当于 $$\\left\\vert \\right\\vert$$.\n B 在判别超平面上，将 B 点的坐标代入得：\n$$w^T(x^{(i)}-\\gamma^{(i)}\\frac{w}{\\left\\Vert w \\right\\Vert_2}) + b = 0$$\n求得 $$\\gamma^{(i)}$$\n$$\\gamma^{(i)} = \\frac{w^Tx^{(i)}+b} {\\left\\Vert w\\right\\Vert_2}=(\\frac{w}{\\left\\Vert w \\right\\Vert_2})^T x^{(i)}+\\frac{b}{\\left\\Vert w \\right\\Vert_2}$$\n观察这个式子可以看到就是在 $$wx+b$$ 上系数除以一个范数。\n$$\\gamma$$ 和 $$\\check{\\gamma}$$ 成一个比例的关系。\n$$\\gamma^{(i)}= \\frac{\\check{\\gamma}^{(i)}}{\\left\\Vert w \\right\\Vert_2}$$\n$$\\gamma= \\frac{\\check{\\gamma}}{\\left\\Vert w \\right\\Vert_2}$$\n这样的话，就可以将训练数据的几何 Margin 给计算出来。\n如果 $$\\left\\Vert w\\right\\Vert_2 = 1$$，则函数 Margin 和几何 Margin 是相同的。\n几何 Margin 会用得更多，一方面，几何的概念反映了一个距离；另一方面，几何 Margin 有一种不变性，它除了范数，所以随便 w 怎么变，它不变。\n函数 Margin 是一个相对距离，几何 Margin 是一个绝对距离。\n置信度就是由几何 Margin 来决定的。\n算法就可以表示出来了。\n$$max\\quad\\gamma$$\n$$s.t. \\quad y^{(i)}\\frac{w^T}{\\left\\Vert w \\right\\Vert_2} x^{(i)}+\\frac{b}{\\left\\Vert w \\right\\Vert_2}=\\gamma,\\quad i=1,…,n$$\n这就是在最简单的情况下（线性可分）的一个 SVM 原始问题。\n将 SVM 写成一个更容易求解的形式 有两个步骤。\n 利用几何 Margin 和函数 Margin 之间的关系得：  $$\\max_{\\check{\\gamma}, w,b} \\quad \\frac{\\check{\\gamma}}{\\left\\Vert w \\right\\Vert_2}$$\n$$s.t. y^{(i)}(w^Tx^{(i)}+b)\\geqslant\\check{\\gamma}\\quad i=1,…,n$$\n取一个合适的 w，使得 $$\\check{\\gamma}=1$$， $$\\left\\Vert \\right\\Vert^2$$ 是为了更好的优化（平方），1/2 是为了计算的简便。得到简化的式子：  $$\\min_{w,b}\\frac{1}{2}\\left\\Vert w \\right\\Vert^2_2$$\n$$s.t. y^{(i)}(w^Tx^{(i)}+b)\\geqslant 1 \\quad i=1,…,n$$\n这就是平常看到的最熟悉的 SVM 算法。要优化的参数是 $w,b$，目标是 Margin 最大。条件是每一个训练数据都分对了，$$\\geqslant 1$$ 的意思是支持向量内没有任何样本，也就是说这是一个 Hard Margin。\n关于 SVM 的推导（Hard Margin/Soft Margin）我还会再写一篇通俗易懂的文章。\n老师的 PPT：\n",
  "wordCount" : "2025",
  "inLanguage": "en",
  "datePublished": "2020-11-03T10:17:29+08:00",
  "dateModified": "2020-11-03T10:17:29+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://landodo.github.io/posts/20201103-svm-notes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "http://landodo.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://landodo.github.io/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/tags" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/cs-zoo" title="CS ZOO">
                    <span>CS ZOO</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://landodo.github.io/">Home</a>&nbsp;»&nbsp;<a href="http://landodo.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      SVM 支持向量机推导
    </h1>
    <div class="post-meta"><span title='2020-11-03 10:17:29 +0800 CST'>November 3, 2020</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;2025 words

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#svm-%e7%ac%94%e8%ae%b0-1-min_wbfrac12leftvert-w-rightvert2_2-%e7%9a%84%e6%8e%a8%e5%af%bc" aria-label="SVM 笔记 1: $\min_{w,b}\frac{1}{2}\left\Vert w \right\Vert^2_2$ 的推导">SVM 笔记 1: $\min_{w,b}\frac{1}{2}\left\Vert w \right\Vert^2_2$ 的推导</a><ul>
                        
                <li>
                    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-svm%e5%ae%83%e7%a9%b6%e7%ab%9f%e5%9c%a8%e5%81%9a%e4%ba%9b%e4%bb%80%e4%b9%88%e4%ba%8b%e6%83%85" aria-label="什么是 SVM？它究竟在做些什么事情？">什么是 SVM？它究竟在做些什么事情？</a></li>
                <li>
                    <a href="#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92logistic-regression" aria-label="逻辑回归（Logistic Regression）">逻辑回归（Logistic Regression）</a></li>
                <li>
                    <a href="#margin-%e5%9c%a8%e6%95%b0%e5%ad%a6%e4%b8%8a%e7%9a%84%e8%a1%a8%e8%be%be" aria-label="Margin 在数学上的表达">Margin 在数学上的表达</a></li>
                <li>
                    <a href="#%e5%b0%86-svm-%e5%86%99%e6%88%90%e4%b8%80%e4%b8%aa%e6%9b%b4%e5%ae%b9%e6%98%93%e6%b1%82%e8%a7%a3%e7%9a%84%e5%bd%a2%e5%bc%8f" aria-label="将 SVM 写成一个更容易求解的形式">将 SVM 写成一个更容易求解的形式</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="svm-笔记-1-min_wbfrac12leftvert-w-rightvert2_2-的推导">SVM 笔记 1: $\min_{w,b}\frac{1}{2}\left\Vert w \right\Vert^2_2$ 的推导<a hidden class="anchor" aria-hidden="true" href="#svm-笔记-1-min_wbfrac12leftvert-w-rightvert2_2-的推导">#</a></h1>
<p>学习率应该怎样去选择？</p>
<ul>
<li>鲁莽司机（Bold Driver）</li>
<li>每轮之后在一个 dataset 上进行评价</li>
<li>$$\alpha_t=\frac{\alpha_0}{1+t/T}$$，学习率随着 t 不断下降</li>
</ul>
<p>分类任务</p>
<ul>
<li>
<p>Linear SVM：最简单的 SVM 的形式</p>
</li>
<li>
<p>Kernelized SVM：核化的一种 SVM 方法</p>
</li>
</ul>
<p>回归任务</p>
<ul>
<li>SVR（Support Vector Regression）</li>
</ul>
<p>SVM 这个方法用得并没有那么广泛，这个算法最大的一个好处就是它比较稳定。从 ML 上来看，它是一个很漂亮的方法，它背后有很好的思想。</p>
<p>泛化能力的分析（Generalization Analysis），有两本书，一本厚的、一本薄的，其中 SVM 就是其中一个核心的算法。现有的理论，后来才有的算法。</p>
<ul>
<li>Generalization Analysis: Vapnik, the nature of Statistical Learning Theory, Spinger, 1995.</li>
<li>Generalization Analysis: Vapnik, Statistical Learning Theory, Wiley&amp;Sons, 1998.</li>
</ul>
<p>SMO：下一节课会讲</p>
<p>工具包：</p>
<ul>
<li>SVM Light</li>
<li>LIBSVM</li>
<li>Multi-Class SVM and StructSVM：多分类、结构化数据</li>
</ul>
<h2 id="什么是-svm它究竟在做些什么事情">什么是 SVM？它究竟在做些什么事情？<a hidden class="anchor" aria-hidden="true" href="#什么是-svm它究竟在做些什么事情">#</a></h2>
<p>从最简单的情况出发，假设数据线性可分。</p>
<p><img loading="lazy" src="./20201103/1.png" alt=""  />
</p>
<p>如下的直线都可以完美的将数据分开，分类面不唯一，所有分类面的训练误差都等于 0，那么该如何选择分类面呢。</p>
<p><img loading="lazy" src="./20201103/2.png" alt=""  />
</p>
<p>应该选择泛化误差最小的那一个。那么怎么去考量泛化误差最小的呢？正确和错误之间有一个较大的容忍度，才能容忍更多的测试数据。</p>
<p>从肉眼来看，这条黑色的线是最好的，因为它之间的间隔充分的大，容忍区域宽。这是从几何的角度看的。</p>
<p><img loading="lazy" src="./20201103/3.png" alt=""  />
</p>
<p>怎么样将这种思想用一种可计算的方式表达出来呢？从而有一个算法能够自动的找到那条黑色的线。这是 SVM 算法的一个出发点。</p>
<p>从以前的算法，看看能得到什么启示。</p>
<h2 id="逻辑回归logistic-regression">逻辑回归（Logistic Regression）<a hidden class="anchor" aria-hidden="true" href="#逻辑回归logistic-regression">#</a></h2>
<p>逻辑回归是一个概率模型，概率模型的好处是：每次算给定的 x 的标签的时候，事实上算的是一个概率，有了概率之后，就有了一个叫做置信度的概念。</p>
<p>有了概率就可以做极大似然估计。置信度告诉我们分类结果有多大的把握。</p>
<p>$$P(y = 1|x)=\frac{1}{1+e^{(-w^Tx)}}$$</p>
<p>概率模型都会有一个置信度。</p>
<p>置信度和前面的那条黑色的分割线有什么关系？可以理解为：黑色的线有较大的置信度。</p>
<p>在没有概率模型的情况下，该怎么定义置信度呢？这就是 SVM 最初要解决的东西：如何定义置信度。</p>
<p>SVM 通过一个叫做 Margin 的东西来定义置信度。SVM 方法也叫做 Max Margin。</p>
<p>注意，最大化Margin是一种思想，SVM 只是其中一个典型的代表。</p>
<p>Margin 刻画置信度这一概念。</p>
<h2 id="margin-在数学上的表达">Margin 在数学上的表达<a hidden class="anchor" aria-hidden="true" href="#margin-在数学上的表达">#</a></h2>
<ul>
<li>假设在做一个二分类问题，特征是 x，标签是 y。</li>
<li>用一个线性的模型来做分类
<ul>
<li>$$f_{w,b}(x) = g(w^Tx+b).$$</li>
<li>当 $$w^Tx+b&gt;=0$$ ，则为类 1；否则为类 -1.</li>
</ul>
</li>
<li>分类面：$$w^Tx+b = 0$$</li>
</ul>
<p>接下来来定义Margin Margin，衡量一个点到一个分类面的一个远近程度。</p>
<ul>
<li>函数Margin（Functional Margin）</li>
</ul>
<p>对于每一个训练数据$$(x^{(i)},y^{(i)})$$，得到一个值，如下就是函数 Margin 的。</p>
<p>$$\check{\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)}+b)$$</p>
<p>$$\check{\gamma}^{(i)}$$是一个大于 0 的值，它不是距离，但是和距离有一定关系。</p>
<ul>
<li>因为如果标签 $$y^{(i)}=1$$，则需要 $$w^Tx+b$$ 是一个正的值</li>
<li>因为如果标签 $$y^{(i)}=-1$$，则需要 $$w^Tx+b$$ 是一个负的值</li>
</ul>
<p>函数 Margin 只是想象中有一个量去尽可能的刻画置信度的作用，什么时候可以把它真正定义成一个距离呢？这就是几何 Margin 所定义的内容。</p>
<ul>
<li>几何 Margin（Geometric Margin）</li>
</ul>
<p><img loading="lazy" src="./20201103/4.png" alt=""  />
</p>
<p>假设 A 点的坐标是 $$x^{(i)}$$，它与 $$\vec{OA}$$ 等价，一个样本可以理解为空间中的一点，不同维度对应它的不同特征。</p>
<p>则 B 点的坐标可以通过 A 点推导出来。</p>
<p>$$B 点的坐标 = x^{(i)}-\gamma^{(i)}w/\left\Vert w \right\Vert_{2}$$</p>
<blockquote>
<p>这个式子的推导如下：</p>
<p>$$\frac{w}{\left\vert w \right\vert}$$ 为 $w$ 方向上的单位向量。</p>
<p>$$\gamma^{(i)}w/\left\vert w \right\vert$$ 得到的是 $\vec{BA}$</p>
<p>我们知道 $$\vec{OB} = \vec{OA} + \vec{AB}$$，$$\vec{OB}$$ 等价于 B 点的坐标。</p>
<p>因此 $$\vec{OB} = \vec{OA} - \vec{BA}$$</p>
<p>得到式子：$$\vec{OB} = x^{(i)}-\gamma^{(i)}w/\left\Vert w \right\Vert_2$$. 推导完成。</p>
<p>注：$$\left\Vert \right\Vert_2$$ 相当于 $$\left\vert \right\vert$$.</p>
</blockquote>
<p>B 在判别超平面上，将 B 点的坐标代入得：</p>
<p>$$w^T(x^{(i)}-\gamma^{(i)}\frac{w}{\left\Vert w \right\Vert_2}) + b = 0$$</p>
<p>求得 $$\gamma^{(i)}$$</p>
<p>$$\gamma^{(i)} = \frac{w^Tx^{(i)}+b} {\left\Vert w\right\Vert_2}=(\frac{w}{\left\Vert w \right\Vert_2})^T x^{(i)}+\frac{b}{\left\Vert w \right\Vert_2}$$</p>
<p>观察这个式子可以看到就是在 $$wx+b$$ 上系数除以一个范数。</p>
<p>$$\gamma$$ 和 $$\check{\gamma}$$ 成一个比例的关系。</p>
<p>$$\gamma^{(i)}= \frac{\check{\gamma}^{(i)}}{\left\Vert w \right\Vert_2}$$</p>
<p>$$\gamma= \frac{\check{\gamma}}{\left\Vert w \right\Vert_2}$$</p>
<p>这样的话，就可以将训练数据的几何 Margin 给计算出来。</p>
<p>如果 $$\left\Vert w\right\Vert_2 = 1$$，则函数 Margin 和几何 Margin 是相同的。</p>
<p>几何 Margin 会用得更多，一方面，几何的概念反映了一个距离；另一方面，几何 Margin 有一种不变性，它除了范数，所以随便 w 怎么变，它不变。</p>
<p>函数 Margin 是一个相对距离，几何 Margin 是一个绝对距离。</p>
<p>置信度就是由几何 Margin 来决定的。</p>
<p>算法就可以表示出来了。</p>
<p>$$max\quad\gamma$$</p>
<p>$$s.t. \quad y^{(i)}\frac{w^T}{\left\Vert w \right\Vert_2} x^{(i)}+\frac{b}{\left\Vert w \right\Vert_2}&gt;=\gamma,\quad i=1,&hellip;,n$$</p>
<p>这就是在最简单的情况下（线性可分）的一个 SVM 原始问题。</p>
<h2 id="将-svm-写成一个更容易求解的形式">将 SVM 写成一个更容易求解的形式<a hidden class="anchor" aria-hidden="true" href="#将-svm-写成一个更容易求解的形式">#</a></h2>
<p>有两个步骤。</p>
<ol>
<li>利用几何 Margin 和函数 Margin 之间的关系得：</li>
</ol>
<p>$$\max_{\check{\gamma}, w,b} \quad \frac{\check{\gamma}}{\left\Vert w \right\Vert_2}$$</p>
<p>$$s.t. y^{(i)}(w^Tx^{(i)}+b)\geqslant\check{\gamma}\quad i=1,&hellip;,n$$</p>
<ol start="2">
<li>取一个合适的 w，使得 $$\check{\gamma}=1$$， $$\left\Vert \right\Vert^2$$ 是为了更好的优化（平方），1/2 是为了计算的简便。得到简化的式子：</li>
</ol>
<p>$$\min_{w,b}\frac{1}{2}\left\Vert w \right\Vert^2_2$$</p>
<p>$$s.t. y^{(i)}(w^Tx^{(i)}+b)\geqslant 1 \quad i=1,&hellip;,n$$</p>
<p>这就是平常看到的最熟悉的 SVM 算法。要优化的参数是 $w,b$，目标是 Margin 最大。条件是每一个训练数据都分对了，$$\geqslant 1$$ 的意思是支持向量内没有任何样本，也就是说这是一个 Hard Margin。</p>
<p>关于 SVM 的推导（Hard Margin/Soft Margin）我还会再写一篇通俗易懂的文章。</p>
<p>老师的 PPT：</p>
<p><img loading="lazy" src="./20201103/5.png" alt=""  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://landodo.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6/">机器学</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://landodo.github.io/posts/20201104-paper-numpy/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Array Programming with NumPy</span>
  </a>
  <a class="next" href="http://landodo.github.io/posts/20201024-paper-alexnet/">
    <span class="title">Next Page »</span>
    <br>
    <span>ImageNet Classification with Deep Convolutional Neural Networks</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Landon</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
