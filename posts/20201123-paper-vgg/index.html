<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<meta name="robots" content="index, follow">
<title>VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION | Landodo&#39;s NoteBook</title>
<meta name="keywords" content="CNN, VGG, 论文阅读" />
<meta name="description" content="VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ✅ 论文地址：https://arxiv.org/pdf/1409.1556.pdf ✅ 发表时间：2015 年（Published">
<meta name="author" content="">
<link rel="canonical" href="http://landodo.github.io/posts/20201123-paper-vgg/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://landodo.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://landodo.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://landodo.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://landodo.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://landodo.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION" />
<meta property="og:description" content="VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ✅ 论文地址：https://arxiv.org/pdf/1409.1556.pdf ✅ 发表时间：2015 年（Published" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://landodo.github.io/posts/20201123-paper-vgg/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-23T10:17:29&#43;08:00" />
<meta property="article:modified_time" content="2020-11-23T10:17:29&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION"/>
<meta name="twitter:description" content="VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ✅ 论文地址：https://arxiv.org/pdf/1409.1556.pdf ✅ 发表时间：2015 年（Published"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://landodo.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION",
      "item": "http://landodo.github.io/posts/20201123-paper-vgg/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION",
  "name": "VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION",
  "description": "VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ✅ 论文地址：https://arxiv.org/pdf/1409.1556.pdf ✅ 发表时间：2015 年（Published",
  "keywords": [
    "CNN", "VGG", "论文阅读"
  ],
  "articleBody": "VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ✅ 论文地址：https://arxiv.org/pdf/1409.1556.pdf\n✅ 发表时间：2015 年（Published as a conference paper at ICLR 2015）\nVGG 名称的来源：Visual Geometry Group, Department of Engineering Science, University of Oxford.\nABSTRACT 论文研究了在大规模图像识别中卷积网络的深度对其精度的影响。\n使用 $3\\times3$ 的卷积核。\n网络深度提升到了 16-19层。\n获得了 2014 ImageNet Challenge 的 localistion 和 classification 的第一名和第二名。（分类任务的第一名是 GoogLeNet）\n1 INTRODUCTION 卷积网络（ConvNets）最近在大规模图像图像和视频识别方面取得了巨大的成功。\nImageNet 大规模视觉识别挑战赛（ILSVRC）在推进深度视觉识别架构方面发挥了重要作用。\nConvNets 目前大多数人已经尝试的改进方式：\n 利用了较小的接收窗口尺寸和较小的第一卷积层步幅（ smaller receptive window size and smaller stride of the first convolutional layer.） 在整个图像和多个尺度上密集地训练和测试网络（training and testing the networks densely over the whole image and over multiple scales.）  本篇论文的工作是固定网络的结构的其他参数，使用 $3\\times3$ 的卷积核，增加更多的卷积层来增加网络的深度。\n we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers.\n 提出了精度更高的 ConvNets 架构，不仅在 ILSVRC 的 Classification 和 Localisation 任务上达到了最高的准确度，而且还适用于其他图像识别的数据集。\n2 CONVNET CONFIGURATIONS 网络结构  网络输入的图片尺寸是 $224\\times224$ 预处理：每个像素中减去在训练集上计算的平均 RGB 值 卷积核大小为 $3\\times3$，还有 $1\\times1$ 可以看作是输入通道的线性变换 卷积的步幅（Stride）固定为 1 像素（pixel） 有 5 个最大池化层，并非每个卷积层后面都接着一个最大池化层。最大池化的窗口为 $2\\times2$，步幅为 2 不同的网络架构，卷积层的深度不相同 卷积层最后接着 3 个全连接层，前两个有 4096 个通道，最后是一个 1000 的 ILSVRC 分类（Softmax）。 所有的隐藏层都使用 ReLU 激活函数 摒弃了 LRN， Section 4 实验证实了 LRN 不会提高 ILSVRC 数据集的性能  6 个网络 A、A-LRN、B、C、D、E 这个 6 网络只在深度上有所不同，其他的都采用通用的设计。\n网络深度越深，参数越多。尽管深度很大，但是网络的参数并没有大很多。\n In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in [Sermanet et al., 2014]).\n 网络更细致讨论 整个网络中使用 $3\\times3$ 的感受野（receptive fields）。\n两个 $3\\times3$ 的 conv. 层的堆叠（中间没有池化）的有效感受野为 $5\\times5$。（画一画就理解了）\n三个这样的层堆叠的有效感受野为 $7\\times7$。\n通过使用三个 $3\\times3$ conv. 层的堆叠而不是只使用一个 $7\\times7$ 层，有什么好处呢？\n more discriminative.（怎么翻译？） 具有更少的参数（怎么算的，C 为什么要平方？）  表 1 的 C 网络，使用了 $1\\times1$ 的卷积。$1\\times1$ 卷积本质上是对相同维度空间的线性投影，但引入了额外的非线性。\nGoodfellow 等人将深度 ConvNets （11 个权重层）应用于街号识别的任务中，得出结论：深度的增加带来了更好的性能。\nGoogLeNet 是 ILSVRC-2014 分类任务的第一名。\n本篇论文的模型在单网分类精度方面优于 GoogLeNet。（Section 4.5，啥是单网分类？）\n our model is outperforming that of [Szegedy et al.(2014)] in terms of the single-network classification accuracy.\n 分类评估细节 训练 TRAINING  Momentum Mini-batch 梯度下降 batch size=256 momentum 参数为 0.9 权重衰减（L2 正则化 $5\\times10^{-4}$）和 Dropout 正则化（丢弃率为 0.5） 学习率初始为 0.01。学习率衰减：精度停止提升时，学习率减少 10 倍，总共减少 3 次。 74 个 epochs  虽然比 AlexNet 更深，但是由于：\n 更小一些的卷积核 conv. 带来的正则化效果 某些层的预初始化  网络将收敛的更快。\n权重的初始化非常重要，初始化不好使得深层网络中梯度的不稳定而导致学习停滞。论文中 A 结构比较浅，因此可以使用均值为 0、方差为 0.01 的高斯分布进行随机初始化，偏置则可以初始化为零。\n数据增强与 AlexNet 相同，随机裁剪 $224\\times224$， random horizontal flipping and random RGB colour shift。\nS 被称为训练规模（we also refer to S as the training scale）。如果 S = 224，那么裁剪到的是整张图像；如果 $S\\ll 224$ ，裁剪到的将是图像的一小部分。\n S = 256 广泛使用，如（AlexNet，GoogLeNet）。首先训练网络时使用 S=256，在训练 S=384 时，用 S = 256 预先训练的权重进行初始化。 从 $[S_{min}, S_{max}]$ 随机抽取（$S_{min} = 256, S_{max}=512$）。识别目标可能这图像的任何位置，因此这是非常有益的。规模抖动的训练集增强。  测试 TESTING 一个训练好的 ConvNet 和输入图片。\n 输入图片缩放到一个 Q（Q 测试尺度 scale，不一定等于 S。对每个 S 使用不同的 Q 可以提升性能） 全连接层转换为卷积层（第一个 FC 层转换为 $7\\times7$ conv. 层，最后两个 FC 层转换为 $1\\times1$ conv. 层） 将得到的网络应用于未裁剪的图像。 结果是一个通道数量等于类数量的类分数映射，空间分辨率可变，取决于输入图像的大小。 可以通过通过水平翻转图像来对测试集进行数据增强。  全卷积网络是应用在整个图像上的，因此不需要在测试时对多个 crops 进行采样。\n使用大量的 crops，可以提升精度，与完全卷积网络相比，它可以对输入图像进行更精细的采样。当将 ConvNet 应用于一个 crop 时，卷积的特征图会被填充为零。\n注：crop 即裁剪的意思。\nPS：这一小节我没看懂。\n实现细节 基于开源的 C++ Caffe toolbox 实现，但是进行的修改，使得能够使用多 GPU 并行 ，以及在多个 scales 的全尺寸（未裁剪）图像上进行训练和评估。\n每 batch 的训练图像分割成多个 GPU batch，在每个 GPU 上并行处理来进行。在计算完 GPU batch 梯度后，对它们进行平均，得到所有 batch 的梯度。梯度计算是在各个 GPU 上同步进行的，所以结果和在单个 GPU 上训练时的结果是完全一样的。\n在配备 4 个 NVIDIA Titan Black GPU 的系统上，根据架构的不同，训练一个网络需要 2-3 周的时间。\n分类实验 上述的 ConvNet 架构在 ILSVRC-2012 数据集上实现的图像分类结果。\n这个数据集包括 1000 个类的图像。\n 训练集（130 万张) 验证集（5 万张) 测试集（10 万张带有保留类标签的图像）。  分类性能的评估采用两个衡量标准：top-1 和 top-5 误差。（这两个指标我在 AlexNet 论文笔记中有说明）\nILSVRC-2014 比赛的 “VGG” 团队。\n4.1 SINGLE SCALE EVALUATION 对于固定的 S，Q = S 和抖动的 $S ∈ [S_{min}，S_{max}]$，Q = 0.5($S_{min}+S_{max}$)。结果如下表：\n A 和 A-LRN 的结果说明，LRN 没有多大的用处。 分类误差随着层数的加深而降低，A（11 层）到 E（19 层） B 和 C 的结果说明，加入非线性层对精度提升有一定的帮助 C 和 D 的层数相同，C 包含一些 $1\\times1$ 的卷积核，D 全部都是 $3\\times3$。$3\\times3$ 能捕捉更多的上下文信息。$3\\times3$ 论文中称为 filters with non-trivial receptive fields 深度达到 19 层时，错误率会达到饱和，更深的模型可能需要大的数据集  两个 $3\\times3$ 与一个 $5\\times5$ 具有相同的感受野。将 B 网络中的每对 $3\\times3$ 替换为单个 $5\\times5$，替换后的网络的 top-1误差比 B 的 top-1 误差高 7%。\n使用 $S ∈ [S_{min}，S_{max}]$ 相比固定 S 来说更优，证实了通过尺度抖动（scale jittering ）的训练集增强确实有助于捕捉多尺度的图像统计数据。\n4.2 MULTI-SCALE EVALUATION 评估测试时尺度抖动的对模型的影响。\n $Q={S-32, S, S+32}$ 训练时的尺度抖动使得网络在测试时可以应用于更大范围的尺度， 用可变 $S∈[S_{min},S_{max}]$ 训练的模型在更大范围的尺寸上进行评估 $Q={S_{min}, 0.5(S_{min}+S_{max}), S_{max}}$  测试时的规模抖动（scale jittering）会带来更好的性能（和 Table 3 一起看）。测试集上，网络 E 实现了 7.3% 的 top-5 错误了。\n4.3 MULTI-CROP EVALUATION dense 和 multi-crop 两种方法是互补的。单独来看，后者相对较优，两者的组合比单独要更优。\n4.4 多模型结合 结合各网络的输出，对 Softmax 的值进行平均，能提升模型的性能。\nTable 5 中，表现最好的单一模型实现了 7.1% 的 top-5 误差（模型E）。结合 D 和 E 这个两个网络，测试的 top-5 误差降到了 7.0%。\n4.5 与现有的技术进行比较 在 ILSVRC-2014 挑战赛的分类任务中，本篇论文 VGG 团队使用 7 个网络模型的集合，以 7.3% 的测试误差获得了第 2 名。过后，使用 D 和 E 模型的结合将错误率降低到 6.8%。\n与分类任务的第一名（GoogLeNet，错误率为 6.7%）相比也有竞争力，并且大大超过了 ILSVRC-2013 的冠军 Clarifai。\nVGG 的最佳结果只结合了两个模型来实现，比大多数 ILSVRC 提交的作品中使用的模型要少得多。\n在单网性能方面，VGG 做到了最好（7.0% 的测试误差），GoogLeNet 是 7.9%。（In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. ）\n总结 VGG 使用传统的 ConvNet 架构（LeNet, AlexNet），在大幅增加深度的情况下，在 ImageNet 挑战数据集上取得了最好的成绩。\n模型可以很好地应用到数据集中。\n再次证实了深度在视觉表征中的重要性。\n LOCALISATION（定位） VGG 在 2014 年的 ILSVRC 挑战赛的定位任务，误差为 25.3%。\n定位网络 LOCALISATION CONVNET 训练：\n  使用 D 网络结构（Table 1），VGG-16。\n  最后一个全连接层预测边界框的位置。边界框由一个 4-D 向量表示，存储着中心坐标、宽度和高度。\n  损失函数采用 Euclidean loss\n  S = 256 和 S = 384\n  使用上面的分类模型进行初始化，最后的全连接层进行随机初始化\n  学习率初始化为 0.001\n  测试：\n 只考虑对 ground truth class 对边界框预测（Ground truth 是正确标注的数据） 仅应用于图像的中心裁剪，获得边界框  实验 如果 IOU（intersection over union）大于 0.5，则认为预测的边界框是正确的。\nper-class regression（PCR）\nsingle-class regression（SCR）\n PCR 的表现优于 SCR 全部 fine-tuning 比只 fine-tuning 第一第二个全连接层要好  与只进行中心裁剪相比，将网络应用与整张图像能提升精确率。\n25.3% 的测试误差，VGG 团队赢得了 ILSVRC-2014 挑战赛的 localisation 任务的冠军。比 ILSVRC-2013 的冠军 Overfeat 的结果要好得多，而且 VGG 还有没采用分辨率增强等技术，VGG 还有一定的提升空间。\nGENERALISATION OF VERY DEEP FEATURES 在 ILSVRC 数据集上预先训练一个 ConvNets，然后应用到其他数据集上。\n 去掉最后一个全连接层（1000 类的 Softmax）； 使用倒数第二层的 4096 维的激活值作为图像的特征； 经过 L2 归一化，并与线性 SVM 分类器相结合，在目标数据集上进行训练。  VGG 与其他方法在 VOC-2007、VOC-2012、Caltech-101和 Caltech-256 上的图像分类结果如下表。\nVOC-2007 图像数据集包含 10K 张图像，VOC-2012 包含 22.5K 张。每张图像都被标注了一个或几个标签，对应 20 个对象类别。识别性能采用各类平均精度（mAP）来衡量。\n(Wei et al., 2014) 的方法在 VOC-2012 上的 mAP 相比 VGG 高 1%。这是通过在 2000 类 ILSVRC 数据集上进行预训练，其中包括额外的 1000 个类别，语义上与 VOC 数据集中的类别接近，并且采用了对象检测辅助分类流水线（bject detection-assisted classification pipeline.），因此取得了更好的结果。\nCaltech-101 数据集包含 9K 图像，有 102 个类（101 个对象类别和一个背景类）。Caltech-256 有 31K 图像，257 个类别。\nVGG 在 Caltech-101 数据集上相比何恺明等人的方法稍差一些，但是 VGG 在 VOC-2007 上明显优于何恺明等人。\n在 PASCAL VOC-2012 动作分类任务上，VGG 使用 D\u0026E 结合，与的比较如下表。\nVGG 仅仅依靠非常深的卷积特征的表示能力，就取得了第一名的成绩。\nVGG 广泛的应用于其他的图像识别任务，并且始终优于更浅层的表示。\nPS：对于这些数据集训练和测试的细节我没有做笔记，需要了解直接看论文\n",
  "wordCount" : "4348",
  "inLanguage": "en",
  "datePublished": "2020-11-23T10:17:29+08:00",
  "dateModified": "2020-11-23T10:17:29+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://landodo.github.io/posts/20201123-paper-vgg/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Landodo's NoteBook",
    "logo": {
      "@type": "ImageObject",
      "url": "http://landodo.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://landodo.github.io/" accesskey="h" title="Landodo&#39;s NoteBook (Alt + H)">Landodo&#39;s NoteBook</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://landodo.github.io/search" title="🔍Search (Alt &#43; /)" accesskey=/>
                    <span>🔍Search</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/tags" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/cs-zoo" title="CS ZOO">
                    <span>CS ZOO</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://landodo.github.io/">Home</a>&nbsp;»&nbsp;<a href="http://landodo.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION
    </h1>
    <div class="post-meta"><span title='2020-11-23 10:17:29 +0800 CST'>November 23, 2020</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;4348 words

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#very-deep-convolutional-networks-for-large-scale-image-recognition" aria-label="VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</a><ul>
                        
                <li>
                    <a href="#abstract" aria-label="ABSTRACT">ABSTRACT</a></li>
                <li>
                    <a href="#1-introduction" aria-label="1 INTRODUCTION">1 INTRODUCTION</a></li>
                <li>
                    <a href="#2-convnet-configurations" aria-label="2 CONVNET CONFIGURATIONS">2 CONVNET CONFIGURATIONS</a><ul>
                        
                <li>
                    <a href="#%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84" aria-label="网络结构">网络结构</a></li></ul>
                </li>
                <li>
                    <a href="#6-%e4%b8%aa%e7%bd%91%e7%bb%9c" aria-label="6 个网络">6 个网络</a><ul>
                        
                <li>
                    <a href="#%e7%bd%91%e7%bb%9c%e6%9b%b4%e7%bb%86%e8%87%b4%e8%ae%a8%e8%ae%ba" aria-label="网络更细致讨论">网络更细致讨论</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%88%86%e7%b1%bb%e8%af%84%e4%bc%b0%e7%bb%86%e8%8a%82" aria-label="分类评估细节">分类评估细节</a><ul>
                        
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83-training" aria-label="训练 TRAINING">训练 TRAINING</a></li>
                <li>
                    <a href="#%e6%b5%8b%e8%af%95-testing" aria-label="测试 TESTING">测试 TESTING</a></li>
                <li>
                    <a href="#%e5%ae%9e%e7%8e%b0%e7%bb%86%e8%8a%82" aria-label="实现细节">实现细节</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%88%86%e7%b1%bb%e5%ae%9e%e9%aa%8c" aria-label="分类实验">分类实验</a><ul>
                        
                <li>
                    <a href="#41-single-scale-evaluation" aria-label="4.1 SINGLE SCALE EVALUATION">4.1 SINGLE SCALE EVALUATION</a></li>
                <li>
                    <a href="#42-multi-scale-evaluation" aria-label="4.2 MULTI-SCALE EVALUATION">4.2 MULTI-SCALE EVALUATION</a></li>
                <li>
                    <a href="#43-multi-crop-evaluation" aria-label="4.3 MULTI-CROP EVALUATION">4.3 MULTI-CROP EVALUATION</a></li>
                <li>
                    <a href="#44-%e5%a4%9a%e6%a8%a1%e5%9e%8b%e7%bb%93%e5%90%88" aria-label="4.4 多模型结合">4.4 多模型结合</a></li>
                <li>
                    <a href="#45-%e4%b8%8e%e7%8e%b0%e6%9c%89%e7%9a%84%e6%8a%80%e6%9c%af%e8%bf%9b%e8%a1%8c%e6%af%94%e8%be%83" aria-label="4.5 与现有的技术进行比较">4.5 与现有的技术进行比较</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%80%bb%e7%bb%93" aria-label="总结">总结</a></li>
                <li>
                    <a href="#localisation%e5%ae%9a%e4%bd%8d" aria-label="LOCALISATION（定位）">LOCALISATION（定位）</a><ul>
                        
                <li>
                    <a href="#%e5%ae%9a%e4%bd%8d%e7%bd%91%e7%bb%9c-localisation-convnet" aria-label="定位网络 LOCALISATION CONVNET">定位网络 LOCALISATION CONVNET</a></li>
                <li>
                    <a href="#%e5%ae%9e%e9%aa%8c" aria-label="实验">实验</a></li></ul>
                </li>
                <li>
                    <a href="#generalisation-of-very-deep-features" aria-label="GENERALISATION OF VERY DEEP FEATURES">GENERALISATION OF VERY DEEP FEATURES</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="very-deep-convolutional-networks-for-large-scale-image-recognition">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION<a hidden class="anchor" aria-hidden="true" href="#very-deep-convolutional-networks-for-large-scale-image-recognition">#</a></h1>
<p>✅ 论文地址：<a href="https://arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a></p>
<p>✅ 发表时间：2015 年（Published as a conference paper at ICLR 2015）</p>
<p>VGG 名称的来源：<strong>V</strong>isual <strong>G</strong>eometry <strong>G</strong>roup, Department of Engineering Science, University of Oxford.</p>
<h2 id="abstract">ABSTRACT<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<p>论文研究了在大规模图像识别中卷积网络的<strong>深度</strong>对其精度的影响。</p>
<p>使用 $3\times3$ 的卷积核。</p>
<p>网络深度提升到了 16-19层。</p>
<p>获得了 2014 ImageNet Challenge 的 localistion 和 classification 的第一名和第二名。（分类任务的第一名是 GoogLeNet）</p>
<h2 id="1-introduction">1 INTRODUCTION<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h2>
<p>卷积网络（ConvNets）最近在大规模图像图像和视频识别方面取得了巨大的成功。</p>
<p>ImageNet 大规模视觉识别挑战赛（ILSVRC）在推进深度视觉识别架构方面发挥了重要作用。</p>
<p>ConvNets 目前大多数人已经尝试的改进方式：</p>
<ul>
<li>利用了较小的接收窗口尺寸和较小的第一卷积层步幅（ smaller receptive window size and
smaller stride of the first convolutional layer.）</li>
<li>在整个图像和多个尺度上密集地训练和测试网络（training and testing the networks densely over the whole image and over multiple scales.）</li>
</ul>
<p>本篇论文的工作是固定网络的结构的其他参数，使用 $3\times3$ 的卷积核，增加更多的卷积层来增加网络的深度。</p>
<blockquote>
<p>we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers.</p>
</blockquote>
<p>提出了精度更高的 ConvNets 架构，不仅在 ILSVRC 的 Classification 和 Localisation 任务上达到了最高的准确度，而且还适用于其他图像识别的数据集。</p>
<h2 id="2-convnet-configurations">2 CONVNET CONFIGURATIONS<a hidden class="anchor" aria-hidden="true" href="#2-convnet-configurations">#</a></h2>
<h3 id="网络结构">网络结构<a hidden class="anchor" aria-hidden="true" href="#网络结构">#</a></h3>
<ul>
<li>网络输入的图片尺寸是 $224\times224$</li>
<li>预处理：每个像素中减去在训练集上计算的平均 RGB 值</li>
<li>卷积核大小为 $3\times3$，还有 $1\times1$ 可以看作是输入通道的线性变换</li>
<li>卷积的步幅（Stride）固定为 1 像素（pixel）</li>
<li>有 5 个最大池化层，并非每个卷积层后面都接着一个最大池化层。最大池化的窗口为 $2\times2$，步幅为 2</li>
<li>不同的网络架构，卷积层的深度不相同</li>
<li>卷积层最后接着 3 个全连接层，前两个有 4096 个通道，最后是一个 1000 的 ILSVRC 分类（Softmax）。</li>
<li>所有的隐藏层都使用 ReLU 激活函数</li>
<li>摒弃了 LRN， Section 4 实验证实了 LRN 不会提高 ILSVRC 数据集的性能</li>
</ul>
<h2 id="6-个网络">6 个网络<a hidden class="anchor" aria-hidden="true" href="#6-个网络">#</a></h2>
<p>A、A-LRN、B、C、D、E 这个 6 网络只在深度上有所不同，其他的都采用通用的设计。</p>
<p><img loading="lazy" src="./20201123-VGG/1.png" alt=""  />
</p>
<p>网络深度越深，参数越多。尽管深度很大，但是网络的参数并没有大很多。</p>
<blockquote>
<p>In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in [Sermanet et al., 2014]).</p>
</blockquote>
<p><img loading="lazy" src="./20201123-VGG/2.png" alt=""  />
</p>
<h3 id="网络更细致讨论">网络更细致讨论<a hidden class="anchor" aria-hidden="true" href="#网络更细致讨论">#</a></h3>
<p>整个网络中使用 $3\times3$ 的感受野（receptive fields）。</p>
<p>两个 $3\times3$ 的 conv. 层的堆叠（中间没有池化）的有效感受野为 $5\times5$。（画一画就理解了）</p>
<p>三个这样的层堆叠的有效感受野为 $7\times7$。</p>
<p>通过使用三个 $3\times3$ conv. 层的堆叠而不是只使用一个 $7\times7$ 层，有什么好处呢？</p>
<ul>
<li><code>more discriminative</code>.（怎么翻译？）</li>
<li>具有更少的参数（怎么算的，C 为什么要平方？）</li>
</ul>
<p>表 1 的 C 网络，使用了 $1\times1$ 的卷积。$1\times1$ 卷积本质上是对相同维度空间的线性投影，但引入了额外的非线性。</p>
<p>Goodfellow 等人将深度 ConvNets （11 个权重层）应用于街号识别的任务中，得出结论：深度的增加带来了更好的性能。</p>
<p>GoogLeNet 是 ILSVRC-2014 分类任务的第一名。</p>
<p>本篇论文的模型在单网分类精度方面优于 GoogLeNet。（Section 4.5，啥是单网分类？）</p>
<blockquote>
<p>our model is outperforming that of [Szegedy et al.(2014)] in terms of the single-network classification accuracy.</p>
</blockquote>
<h2 id="分类评估细节">分类评估细节<a hidden class="anchor" aria-hidden="true" href="#分类评估细节">#</a></h2>
<h3 id="训练-training">训练 TRAINING<a hidden class="anchor" aria-hidden="true" href="#训练-training">#</a></h3>
<ul>
<li>Momentum Mini-batch 梯度下降</li>
<li>batch size=256</li>
<li>momentum 参数为 0.9</li>
<li>权重衰减（L2 正则化 $5\times10^{-4}$）和 Dropout 正则化（丢弃率为 0.5）</li>
<li>学习率初始为 0.01。学习率衰减：精度停止提升时，学习率减少 10 倍，总共减少 3 次。</li>
<li>74 个 epochs</li>
</ul>
<p>虽然比 AlexNet 更深，但是由于：</p>
<ul>
<li>更小一些的卷积核 conv. 带来的正则化效果</li>
<li>某些层的预初始化</li>
</ul>
<p>网络将收敛的更快。</p>
<p>权重的初始化非常重要，初始化不好使得深层网络中梯度的不稳定而导致学习停滞。论文中 A 结构比较浅，因此可以使用均值为 0、方差为 0.01 的高斯分布进行随机初始化，偏置则可以初始化为零。</p>
<p>数据增强与 AlexNet 相同，随机裁剪 $224\times224$， random horizontal flipping and random RGB colour shift。</p>
<p>S 被称为训练规模（we also refer to S as the training scale）。如果 S = 224，那么裁剪到的是整张图像；如果 $S\ll 224$ ，裁剪到的将是图像的一小部分。</p>
<ul>
<li>S = 256 广泛使用，如（AlexNet，GoogLeNet）。首先训练网络时使用 S=256，在训练 S=384 时，用 S = 256 预先训练的权重进行初始化。</li>
<li>从 $[S_{min}, S_{max}]$ 随机抽取（$S_{min} = 256, S_{max}=512$）。识别目标可能这图像的任何位置，因此这是非常有益的。规模抖动的训练集增强。</li>
</ul>
<h3 id="测试-testing">测试 TESTING<a hidden class="anchor" aria-hidden="true" href="#测试-testing">#</a></h3>
<p>一个训练好的 ConvNet 和输入图片。</p>
<ul>
<li>输入图片缩放到一个 Q（Q 测试尺度 scale，不一定等于 S。对每个 S 使用不同的 Q 可以提升性能）</li>
<li>全连接层转换为卷积层（第一个 FC 层转换为 $7\times7$ conv. 层，最后两个 FC 层转换为 $1\times1$ conv. 层）</li>
<li>将得到的网络应用于未裁剪的图像。</li>
<li>结果是一个通道数量等于类数量的类分数映射，空间分辨率可变，取决于输入图像的大小。</li>
<li>可以通过通过水平翻转图像来对测试集进行数据增强。</li>
</ul>
<p>全卷积网络是应用在整个图像上的，因此不需要在测试时对多个 crops 进行采样。</p>
<p>使用大量的 crops，可以提升精度，与完全卷积网络相比，它可以对输入图像进行更精细的采样。当将 ConvNet 应用于一个 crop 时，卷积的特征图会被填充为零。</p>
<p>注：crop 即裁剪的意思。</p>
<p><strong>PS：这一小节我没看懂。</strong></p>
<h3 id="实现细节">实现细节<a hidden class="anchor" aria-hidden="true" href="#实现细节">#</a></h3>
<p>基于开源的 C++ Caffe toolbox 实现，但是进行的修改，使得能够使用多 GPU 并行 ，以及在多个 scales 的全尺寸（未裁剪）图像上进行训练和评估。</p>
<p>每 batch 的训练图像分割成多个 GPU batch，在每个 GPU 上并行处理来进行。在计算完 GPU batch 梯度后，对它们进行平均，得到所有 batch 的梯度。梯度计算是在各个 GPU 上同步进行的，所以结果和在单个 GPU 上训练时的结果是完全一样的。</p>
<p>在配备 4 个 NVIDIA Titan Black GPU 的系统上，根据架构的不同，训练一个网络需要 2-3 周的时间。</p>
<h2 id="分类实验">分类实验<a hidden class="anchor" aria-hidden="true" href="#分类实验">#</a></h2>
<p>上述的 ConvNet 架构在 ILSVRC-2012 数据集上实现的图像分类结果。</p>
<p>这个数据集包括 1000 个类的图像。</p>
<ul>
<li>训练集（130 万张)</li>
<li>验证集（5 万张)</li>
<li>测试集（10 万张带有保留类标签的图像）。</li>
</ul>
<p>分类性能的评估采用两个衡量标准：top-1 和 top-5 误差。（这两个指标我在 AlexNet 论文笔记中有说明）</p>
<p>ILSVRC-2014 比赛的 &ldquo;VGG&rdquo; 团队。</p>
<h3 id="41-single-scale-evaluation">4.1 SINGLE SCALE EVALUATION<a hidden class="anchor" aria-hidden="true" href="#41-single-scale-evaluation">#</a></h3>
<p>对于固定的 S，Q = S 和抖动的 $S ∈ [S_{min}，S_{max}]$，Q = 0.5($S_{min}+S_{max}$)。结果如下表：</p>
<p><img loading="lazy" src="./20201123-VGG/3.png" alt=""  />
</p>
<ul>
<li>A 和 A-LRN 的结果说明，LRN 没有多大的用处。</li>
<li>分类误差随着层数的加深而降低，A（11 层）到 E（19 层）</li>
<li>B 和 C 的结果说明，加入非线性层对精度提升有一定的帮助</li>
<li>C 和 D 的层数相同，C 包含一些 $1\times1$ 的卷积核，D 全部都是 $3\times3$。$3\times3$ 能捕捉更多的上下文信息。$3\times3$ 论文中称为 filters with non-trivial receptive fields</li>
<li>深度达到 19 层时，错误率会达到饱和，更深的模型可能需要大的数据集</li>
</ul>
<p>两个 $3\times3$ 与一个 $5\times5$ 具有相同的感受野。将 B 网络中的每对 $3\times3$ 替换为单个 $5\times5$，替换后的网络的 top-1误差比 B 的 top-1 误差高 7%。</p>
<p>使用  $S ∈ [S_{min}，S_{max}]$ 相比固定 S 来说更优，证实了通过尺度抖动（scale jittering ）的训练集增强确实有助于捕捉多尺度的图像统计数据。</p>
<h3 id="42-multi-scale-evaluation">4.2 MULTI-SCALE EVALUATION<a hidden class="anchor" aria-hidden="true" href="#42-multi-scale-evaluation">#</a></h3>
<p>评估测试时尺度抖动的对模型的影响。</p>
<ul>
<li>$Q={S-32, S, S+32}$</li>
<li>训练时的尺度抖动使得网络在测试时可以应用于更大范围的尺度，</li>
<li>用可变 $S∈[S_{min},S_{max}]$ 训练的模型在更大范围的尺寸上进行评估 $Q={S_{min}, 0.5(S_{min}+S_{max}), S_{max}}$</li>
</ul>
<p>测试时的规模抖动（scale jittering）会带来更好的性能（和 Table 3 一起看）。测试集上，网络 E 实现了  7.3% 的 top-5 错误了。</p>
<p><img loading="lazy" src="./20201123-VGG/4.png" alt=""  />
</p>
<h3 id="43-multi-crop-evaluation">4.3 MULTI-CROP EVALUATION<a hidden class="anchor" aria-hidden="true" href="#43-multi-crop-evaluation">#</a></h3>
<p>dense 和 multi-crop 两种方法是互补的。单独来看，后者相对较优，两者的组合比单独要更优。</p>
<p><img loading="lazy" src="./20201123-VGG/5.png" alt=""  />
</p>
<h3 id="44-多模型结合">4.4 多模型结合<a hidden class="anchor" aria-hidden="true" href="#44-多模型结合">#</a></h3>
<p>结合各网络的输出，对 Softmax 的值进行平均，能提升模型的性能。</p>
<p>Table 5 中，表现最好的单一模型实现了 7.1% 的 top-5 误差（模型E）。结合 D 和 E 这个两个网络，测试的 top-5 误差降到了 7.0%。</p>
<p><img loading="lazy" src="./20201123-VGG/6.png" alt=""  />
</p>
<h3 id="45-与现有的技术进行比较">4.5 与现有的技术进行比较<a hidden class="anchor" aria-hidden="true" href="#45-与现有的技术进行比较">#</a></h3>
<p>在 ILSVRC-2014 挑战赛的分类任务中，本篇论文 VGG 团队使用 7 个网络模型的集合，以 7.3% 的测试误差获得了第 2 名。过后，使用 D 和 E 模型的结合将错误率降低到 6.8%。</p>
<p>与分类任务的第一名（GoogLeNet，错误率为 6.7%）相比也有竞争力，并且大大超过了 ILSVRC-2013 的冠军 Clarifai。</p>
<p>VGG 的最佳结果只结合了两个模型来实现，比大多数 ILSVRC 提交的作品中使用的模型要少得多。</p>
<p>在单网性能方面，VGG 做到了最好（7.0% 的测试误差），GoogLeNet 是 7.9%。（In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. ）</p>
<p><img loading="lazy" src="./20201123-VGG/7.png" alt=""  />
</p>
<h2 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h2>
<p>VGG 使用传统的 ConvNet 架构（LeNet, AlexNet），在大幅增加深度的情况下，在 ImageNet 挑战数据集上取得了最好的成绩。</p>
<p>模型可以很好地应用到数据集中。</p>
<p>再次证实了深度在视觉表征中的重要性。</p>
<hr>
<h2 id="localisation定位">LOCALISATION（定位）<a hidden class="anchor" aria-hidden="true" href="#localisation定位">#</a></h2>
<p>VGG 在 2014 年的 ILSVRC 挑战赛的定位任务，误差为 25.3%。</p>
<h3 id="定位网络-localisation-convnet">定位网络 LOCALISATION CONVNET<a hidden class="anchor" aria-hidden="true" href="#定位网络-localisation-convnet">#</a></h3>
<p>训练：</p>
<ul>
<li>
<p>使用 D 网络结构（Table 1），VGG-16。</p>
</li>
<li>
<p>最后一个全连接层预测边界框的位置。边界框由一个 4-D 向量表示，存储着中心坐标、宽度和高度。</p>
</li>
<li>
<p>损失函数采用 Euclidean loss</p>
</li>
<li>
<p>S = 256 和 S = 384</p>
</li>
<li>
<p>使用上面的分类模型进行初始化，最后的全连接层进行随机初始化</p>
</li>
<li>
<p>学习率初始化为 0.001</p>
</li>
</ul>
<p>测试：</p>
<ul>
<li>只考虑对 ground truth class 对边界框预测（Ground truth 是正确标注的数据）</li>
<li>仅应用于图像的中心裁剪，获得边界框</li>
</ul>
<p><img loading="lazy" src="./20201123-VGG/8.png" alt=""  />
</p>
<h3 id="实验">实验<a hidden class="anchor" aria-hidden="true" href="#实验">#</a></h3>
<p>如果 IOU（intersection over union）大于 0.5，则认为预测的边界框是正确的。</p>
<p>per-class regression（PCR）</p>
<p>single-class regression（SCR）</p>
<ul>
<li>PCR 的表现优于 SCR</li>
<li>全部 fine-tuning 比只 fine-tuning 第一第二个全连接层要好</li>
</ul>
<p><img loading="lazy" src="./20201123-VGG/9.png" alt=""  />
</p>
<p>与只进行中心裁剪相比，将网络应用与整张图像能提升精确率。</p>
<p><img loading="lazy" src="./20201123-VGG/10.png" alt=""  />
</p>
<p>25.3% 的测试误差，VGG 团队赢得了 ILSVRC-2014 挑战赛的 localisation 任务的冠军。比 ILSVRC-2013 的冠军 Overfeat 的结果要好得多，而且 VGG 还有没采用分辨率增强等技术，VGG 还有一定的提升空间。</p>
<p><img loading="lazy" src="./20201123-VGG/11.png" alt=""  />
</p>
<h2 id="generalisation-of-very-deep-features">GENERALISATION OF VERY DEEP FEATURES<a hidden class="anchor" aria-hidden="true" href="#generalisation-of-very-deep-features">#</a></h2>
<p>在 ILSVRC 数据集上预先训练一个 ConvNets，然后应用到其他数据集上。</p>
<ul>
<li>去掉最后一个全连接层（1000 类的 Softmax）；</li>
<li>使用倒数第二层的 4096 维的激活值作为图像的特征；</li>
<li>经过 L2 归一化，并与线性 SVM 分类器相结合，在目标数据集上进行训练。</li>
</ul>
<p>VGG 与其他方法在 VOC-2007、VOC-2012、Caltech-101和 Caltech-256 上的图像分类结果如下表。</p>
<p><img loading="lazy" src="./20201123-VGG/12.png" alt=""  />
</p>
<p>VOC-2007 图像数据集包含 10K 张图像，VOC-2012 包含 22.5K 张。每张图像都被标注了一个或几个标签，对应 20 个对象类别。识别性能采用各类平均精度（mAP）来衡量。</p>
<p>(Wei et al., 2014) 的方法在 VOC-2012 上的 mAP 相比 VGG 高 1%。这是通过在 2000 类 ILSVRC 数据集上进行预训练，其中包括额外的 1000 个类别，语义上与 VOC 数据集中的类别接近，并且采用了对象检测辅助分类流水线（bject detection-assisted classification pipeline.），因此取得了更好的结果。</p>
<p>Caltech-101 数据集包含 9K 图像，有 102 个类（101 个对象类别和一个背景类）。Caltech-256 有 31K 图像，257 个类别。</p>
<p>VGG 在 Caltech-101 数据集上相比何恺明等人的方法稍差一些，但是 VGG 在 VOC-2007 上明显优于何恺明等人。</p>
<p>在 PASCAL VOC-2012 动作分类任务上，VGG 使用 D&amp;E 结合，与的比较如下表。</p>
<p><img loading="lazy" src="./20201123-VGG/13.png" alt=""  />
</p>
<p>VGG 仅仅依靠非常深的卷积特征的表示能力，就取得了第一名的成绩。</p>
<p>VGG 广泛的应用于其他的图像识别任务，并且始终优于更浅层的表示。</p>
<p><strong>PS：对于这些数据集训练和测试的细节我没有做笔记，需要了解直接看论文</strong></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://landodo.github.io/tags/cnn/">CNN</a></li>
      <li><a href="http://landodo.github.io/tags/vgg/">VGG</a></li>
      <li><a href="http://landodo.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://landodo.github.io/posts/20201124-deeplearning-review/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Deep learning Review</span>
  </a>
  <a class="next" href="http://landodo.github.io/posts/20201113-http/">
    <span class="title">Next Page »</span>
    <br>
    <span>使用 C 语言实现一个 HTTP 服务器（2）</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Landon</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
