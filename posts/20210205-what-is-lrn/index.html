<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<meta name="robots" content="index, follow">
<title>AlexNet 中的 LRN（Local Response Normalization） 是什么 | Notes</title>
<meta name="keywords" content="深度学习, CNN, 论文阅读" />
<meta name="description" content="AlexNet 中的 LRN（Local Response Normalization） 是什么 对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。 LRN 操作在哪一步">
<meta name="author" content="">
<link rel="canonical" href="http://landodo.github.io/posts/20210205-what-is-lrn/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<link rel="preload" href="./logo.png" as="image">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni&#43;Srwf3KK8HyZB9V4809q9TWE="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://landodo.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://landodo.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://landodo.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://landodo.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://landodo.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="AlexNet 中的 LRN（Local Response Normalization） 是什么" />
<meta property="og:description" content="AlexNet 中的 LRN（Local Response Normalization） 是什么 对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。 LRN 操作在哪一步" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://landodo.github.io/posts/20210205-what-is-lrn/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-15T10:17:29&#43;08:00" />
<meta property="article:modified_time" content="2021-02-15T10:17:29&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="AlexNet 中的 LRN（Local Response Normalization） 是什么"/>
<meta name="twitter:description" content="AlexNet 中的 LRN（Local Response Normalization） 是什么 对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。 LRN 操作在哪一步"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://landodo.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "AlexNet 中的 LRN（Local Response Normalization） 是什么",
      "item": "http://landodo.github.io/posts/20210205-what-is-lrn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "AlexNet 中的 LRN（Local Response Normalization） 是什么",
  "name": "AlexNet 中的 LRN（Local Response Normalization） 是什么",
  "description": "AlexNet 中的 LRN（Local Response Normalization） 是什么 对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。 LRN 操作在哪一步",
  "keywords": [
    "深度学习", "CNN", "论文阅读"
  ],
  "articleBody": "AlexNet 中的 LRN（Local Response Normalization） 是什么 对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。\nLRN 操作在哪一步？ 答：ReLU 之后。 AlexNet 的 PyTorch 官方实现 （1）PyTorch\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\nPyTorch 把 LRN 给移除了。\n（2）Paper with Code\n下面的一个有 LRN 的版本，来自 Paper with Code。我觉得是写得最清晰的。\nhttps://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40\nclass AlexNet(nn.Module): \"\"\" Neural network model consisting of layers propsed by AlexNet paper. \"\"\" def __init__(self, num_classes=1000): \"\"\" Define and allocate layers for this neural net. Args: num_classes (int): number of classes to predict with this model \"\"\" super().__init__() # input size should be : (b x 3 x 227 x 227) # The image in the original paper states that width and height are 224 pixels, but # the dimensions after first convolution layer do not lead to 55 x 55. self.net = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4), # (b x 96 x 55 x 55) nn.ReLU(), nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), # section 3.3 nn.MaxPool2d(kernel_size=3, stride=2), # (b x 96 x 27 x 27) nn.Conv2d(96, 256, 5, padding=2), # (b x 256 x 27 x 27) nn.ReLU(), nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 13 x 13) nn.Conv2d(256, 384, 3, padding=1), # (b x 384 x 13 x 13) nn.ReLU(), nn.Conv2d(384, 384, 3, padding=1), # (b x 384 x 13 x 13) nn.ReLU(), nn.Conv2d(384, 256, 3, padding=1), # (b x 256 x 13 x 13) nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # (b x 256 x 6 x 6) ) # classifier is just a name for linear layers self.classifier = nn.Sequential( nn.Dropout(p=0.5, inplace=True), nn.Linear(in_features=(256 * 6 * 6), out_features=4096), nn.ReLU(), nn.Dropout(p=0.5, inplace=True), nn.Linear(in_features=4096, out_features=4096), nn.ReLU(), nn.Linear(in_features=4096, out_features=num_classes), ) def forward(self, x): \"\"\" Pass the input through the net. Args: x (Tensor): input tensor Returns: output (Tensor): output tensor \"\"\" x = self.net(x) x = x.view(-1, 256 * 6 * 6) # reduce the dimensions for linear layer input return self.classifier(x) 需要注意的是，LRN 发生在 ReLU 激活函数之后。\n接下来看看论文是如何描述 LRN 的。\nLocal Response Normalization ReLU 不需要输入归一化来防止饱和（Saturation），这是 ReLU 的一个理想性质。如果至少有一些训练例子对 ReLU 产生正向输入，学习就会在该神经元中发生。\n图片来源：https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7\n作者发现以下局部归一化方案有助于泛化。响应归一化 $b_{x,y}^{i}$ 由如下表达式得到。\n$a_{x,y}^{i}$ 表示在位置 $(x,y)$ 处应用核 $i$ 卷积计算后，再运用激活函数 ReLU 后的输出。（即 ReLU 后进行 LRN） 如下是 LRN 是整体示意图。\n响应归一化实现了一种受真实神经元类型启发的横向抑制形式，在使用不同内核计算的神经元输出中创造了大活动的竞争。常量 k，n，α 和 β 是超参数，其值是使用验证集确定的，使用 k = 2，n = 5，α = 10e-4，β = 0.75。在某些层中应用 ReLU 非线性后应用了这种归一化。\n这个方案更正确的说法是”亮度归一化“，因为没有减去平均活性。响应归一化使 top-1 和 top-5 错误率分别降低了 1.4% 和 1.2%。\n在 CIFAR-10 数据集上：一个四层 CNN 在没有归一化的情况下实现了 13% 的测试错误率，而在归一化的情况下实现了 11% 的错误率。\nLRN 细节 接下来深入到 LRN 的细节，看看 LRN 究竟实现了什么样的效果。\n（1）公式的解释\na 表示卷积层（包括卷积操作和激活操作）后的输出结果。这个输出的结果是一个四维数组 [batch,height,width,channel]。这个输出结构中的一个位置 [a,b,c,d]，可以理解成在某一张特征图中的某一个通道下的某个高度和某个宽度位置的点，即第 a 张特征图的第 d 个通道下的高度为 b 宽度为 c 的点。 $a_{x,y}^{i}$ 表示第 i 片特征图在位置（x,y）运用激活函数 ReLU 后的输出。n 是同一位置上临近的 feature map 的数目，N 是特征图的总数。 参数 $k, n, \\alpha，\\beta$ 都是超参数。k=2，n=5，α=10-4，β=0.75。 举一个例子：\ni = 10, N = 96 时，第 i=10 个卷积核在位置（x,y）处的取值为 $a_{x,y}^{i}$，它的局部响应归一化过程如下：用 $a_{x,y}^{i}$ 除以第 8、9、10、11、12 片特征图位置（x,y）处的取值求和。\n也就是跨通道的一个 Normalization 操作。\ntorch.nn.LocalResponseNorm() $$b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n} ​ \\sum_{c’=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c’}^2\\right)^{-\\beta}$$\nInit signature: nn.LocalResponseNorm( size:int, alpha:float=0.0001, beta:float=0.75, k:float=1.0, ) -\u003e None Docstring: Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels. Args: size: amount of neighbouring channels used for normalization alpha: multiplicative factor. Default: 0.0001 beta: exponent. Default: 0.75 k: additive factor. Default: 1 Shape: - Input: :math:`(N, C, *)` - Output: :math:`(N, C, *)` (same shape as input) Examples:: \u003e\u003e\u003e lrn = nn.LocalResponseNorm(2) \u003e\u003e\u003e signal_2d = torch.randn(32, 5, 24, 24) \u003e\u003e\u003e signal_4d = torch.randn(16, 5, 7, 7, 7, 7) \u003e\u003e\u003e output_2d = lrn(signal_2d) \u003e\u003e\u003e output_4d = lrn(signal_4d) 使用：\nimport torch import torch.nn as nn torch.__version__ # '1.7.0' lrn = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2) signal_2d = torch.randn(32, 96, 55, 55) # batch_size=32, feature_map=96×55×55 output_2d = lrn(signal_2d) signal_2d.shape # torch.Size([32, 96, 55, 55]) output_2d.shape # torch.Size([32, 96, 55, 55]) 手算检验 import torch torch.__version__ '1.7.0' import torch.nn as nn lrn = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2) torch.manual_seed(666) signal_2d = torch.randn(2, 3, 2, 2) # batch_size=2, feature_map=3×2×2 output_2d = lrn(signal_2d) output_2d.shape torch.Size([2, 3, 2, 2]) signal_2d.shape torch.Size([2, 3, 2, 2]) signal_2d tensor([[[[-0.7747, 0.7926], [-0.0062, -0.4377]], [[ 0.4657, -0.1880], [-0.8975, 0.4169]], [[-0.3479, -0.4007], [ 0.8059, -0.1021]]], [[[-0.3055, -1.7611], [-0.6461, 0.3470]], [[ 0.9144, 1.6259], [-0.6535, -0.0865]], [[ 0.2100, 0.4811], [ 0.4506, 0.0600]]]]) output_2d tensor([[[[-0.4607, 0.4713], [-0.0037, -0.2603]], [[ 0.2769, -0.1118], [-0.5336, 0.2479]], [[-0.2069, -0.2383], [ 0.4792, -0.0607]]], [[[-0.1816, -1.0471], [-0.3842, 0.2063]], [[ 0.5437, 0.9667], [-0.3886, -0.0514]], [[ 0.1249, 0.2860], [ 0.2680, 0.0357]]]]) 分析：\n这个 batch 里面的第一张特征图、第一个通道、(0,0) 位置的取值为 -0.7747。接下来分析其 LRN 归一化后的值。\nLRN 的超参数：size=5, alpha=0.0001, beta=0.75, k=2。\n$$b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n} ​ \\sum_{c’=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c’}^2\\right)^{-\\beta}$$\n$$\\frac{-0.7747}{(2 + \\frac{0.0001}{5} (0.7747^2 + 0.4657^2 + 0.3479^2))^{0.75}} = -0.4607$$\n跨通道求和的下限：max(0, 0 - 5/2) = 0\n上限：min(2, 0 + 5/2) = 2 LRN 归一化公式内部的细节理解就先这样了，至于它更深层的作用，以及它为什么会被舍弃，留到后面。\n",
  "wordCount" : "1768",
  "inLanguage": "en",
  "datePublished": "2021-02-15T10:17:29+08:00",
  "dateModified": "2021-02-15T10:17:29+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://landodo.github.io/posts/20210205-what-is-lrn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://landodo.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://landodo.github.io/" accesskey="h" title="Notes (Alt + H)">
                <img src="http://landodo.github.io/logo.png" alt="logo" aria-label="logo"
                    height="30">Notes</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://landodo.github.io/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/tags" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/cs-zoo" title="CS ZOO">
                    <span>CS ZOO</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://landodo.github.io/">Home</a>&nbsp;»&nbsp;<a href="http://landodo.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      AlexNet 中的 LRN（Local Response Normalization） 是什么
    </h1>
    <div class="post-meta"><span title='2021-02-15 10:17:29 +0800 CST'>February 15, 2021</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;1768 words

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#alexnet-%e4%b8%ad%e7%9a%84-lrnlocal-response-normalization-%e6%98%af%e4%bb%80%e4%b9%88" aria-label="AlexNet 中的 LRN（Local Response Normalization） 是什么">AlexNet 中的 LRN（Local Response Normalization） 是什么</a><ul>
                        
                <li>
                    <a href="#alexnet-%e7%9a%84-pytorch-%e5%ae%98%e6%96%b9%e5%ae%9e%e7%8e%b0" aria-label="AlexNet 的 PyTorch 官方实现">AlexNet 的 PyTorch 官方实现</a></li>
                <li>
                    <a href="#local-response-normalization" aria-label="Local Response Normalization">Local Response Normalization</a></li>
                <li>
                    <a href="#lrn-%e7%bb%86%e8%8a%82" aria-label="LRN 细节">LRN 细节</a></li>
                <li>
                    <a href="#torchnnlocalresponsenorm" aria-label="torch.nn.LocalResponseNorm()">torch.nn.LocalResponseNorm()</a></li>
                <li>
                    <a href="#%e6%89%8b%e7%ae%97%e6%a3%80%e9%aa%8c" aria-label="手算检验">手算检验</a></li>
                <li>
                    <a href="#%e4%b8%8a%e9%99%90min2-0--52--2" aria-label="上限：min(2, 0 &#43; 5/2) = 2">上限：min(2, 0 + 5/2) = 2</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="alexnet-中的-lrnlocal-response-normalization-是什么">AlexNet 中的 LRN（Local Response Normalization） 是什么<a hidden class="anchor" aria-hidden="true" href="#alexnet-中的-lrnlocal-response-normalization-是什么">#</a></h1>
<p>对我而言，LRN 是 AleNet 论文中的一个难点，今天就来更加细致的理解一下。</p>
<ul>
<li>LRN 操作在哪一步？
<ul>
<li>答：ReLU 之后。</li>
</ul>
</li>
</ul>
<h2 id="alexnet-的-pytorch-官方实现">AlexNet 的 PyTorch 官方实现<a hidden class="anchor" aria-hidden="true" href="#alexnet-的-pytorch-官方实现">#</a></h2>
<p>（1）PyTorch</p>
<p><a href="https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py">https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py</a></p>
<p>PyTorch 把 LRN 给移除了。</p>
<p>（2）Paper with Code</p>
<p>下面的一个有 LRN 的版本，来自 Paper with Code。我觉得是写得最清晰的。</p>
<p><a href="https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40">https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AlexNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Neural network model consisting of layers propsed by AlexNet paper.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Define and allocate layers for this neural net.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            num_classes (int): number of classes to predict with this model
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># input size should be : (b x 3 x 227 x 227)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># The image in the original paper states that width and height are 224 pixels, but</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># the dimensions after first convolution layer do not lead to 55 x 55.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>net <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">96</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>),  <span style="color:#75715e"># (b x 96 x 55 x 55)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>LocalResponseNorm(size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># section 3.3</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 96 x 27 x 27)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">96</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">5</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 256 x 27 x 27)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>LocalResponseNorm(size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 256 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  <span style="color:#75715e"># (b x 384 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  <span style="color:#75715e"># (b x 384 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">384</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  <span style="color:#75715e"># (b x 256 x 13 x 13)</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),  <span style="color:#75715e"># (b x 256 x 6 x 6)</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># classifier is just a name for linear layers</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span>(<span style="color:#ae81ff">256</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span>), out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span>, out_features<span style="color:#f92672">=</span>num_classes),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Pass the input through the net.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x (Tensor): input tensor
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            output (Tensor): output tensor
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>net(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">256</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span>)  <span style="color:#75715e"># reduce the dimensions for linear layer input</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>classifier(x)
</span></span></code></pre></div><p>需要注意的是，<strong>LRN 发生在 ReLU 激活函数之后。</strong></p>
<p>接下来看看论文是如何描述 LRN 的。</p>
<h2 id="local-response-normalization">Local Response Normalization<a hidden class="anchor" aria-hidden="true" href="#local-response-normalization">#</a></h2>
<p>ReLU 不需要输入归一化来防止饱和（Saturation），这是 ReLU 的一个理想性质。如果至少有一些训练例子对 ReLU 产生正向输入，学习就会在该神经元中发生。</p>
<p>图片来源：<a href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7">https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7</a></p>
<p><img loading="lazy" src="./20210205/1.png" alt=""  />
</p>
<p>作者发现以下局部归一化方案有助于泛化。响应归一化 $b_{x,y}^{i}$ 由如下表达式得到。</p>
<ul>
<li>$a_{x,y}^{i}$ 表示在位置 $(x,y)$ 处应用核 $i$ 卷积计算后，再运用激活函数 ReLU 后的输出。（即 ReLU 后进行 LRN）</li>
</ul>
<p><img loading="lazy" src="./20210205/2.png" alt=""  />
</p>
<blockquote>
<p>如下是 LRN 是整体示意图。</p>
</blockquote>
<blockquote>
<p><img loading="lazy" src="./20210205/3.png" alt=""  />
</p>
</blockquote>
<p>响应归一化实现了一种受真实神经元类型启发的横向抑制形式，在使用不同内核计算的神经元输出中创造了大活动的竞争。常量 k，n，α 和 β 是超参数，其值是使用验证集确定的，使用 k = 2，n = 5，α = 10e-4，β = 0.75。在某些层中应用 ReLU 非线性后应用了这种归一化。</p>
<p>这个方案更正确的说法是”亮度归一化“，因为没有减去平均活性。响应归一化使 top-1 和 top-5 错误率分别降低了 1.4% 和 1.2%。</p>
<p>在 CIFAR-10 数据集上：一个四层 CNN 在没有归一化的情况下实现了 13% 的测试错误率，而在归一化的情况下实现了 11% 的错误率。</p>
<h2 id="lrn-细节">LRN 细节<a hidden class="anchor" aria-hidden="true" href="#lrn-细节">#</a></h2>
<p>接下来深入到 LRN 的细节，看看 LRN 究竟实现了什么样的效果。</p>
<p>（1）公式的解释</p>
<p><img loading="lazy" src="./20210205/2.png" alt=""  />
</p>
<ul>
<li>a 表示<strong>卷积层（包括卷积操作和激活操作）后的输出结果</strong>。这个输出的结果是一个四维数组 [batch,height,width,channel]。这个输出结构中的一个位置 [a,b,c,d]，可以理解成在某一张特征图中的某一个通道下的某个高度和某个宽度位置的点，即<strong>第 a 张特征图的第 d 个通道下的高度为 b 宽度为 c 的点。</strong></li>
<li>$a_{x,y}^{i}$ 表示第 i 片特征图在位置（x,y）运用激活函数 ReLU 后的输出。n 是同一位置上临近的 feature map 的数目，N 是特征图的总数。</li>
</ul>
<p><img loading="lazy" src="./20210205/4.png" alt=""  />
</p>
<ul>
<li>参数 $k, n, \alpha，\beta$ 都是超参数。k=2，n=5，α=10-4，β=0.75。</li>
</ul>
<p>举一个例子：</p>
<p>i = 10, N = 96 时，第 i=10 个卷积核在位置（x,y）处的取值为 $a_{x,y}^{i}$，它的局部响应归一化过程如下：用 $a_{x,y}^{i}$ 除以第 8、9、10、11、12 片特征图位置（x,y）处的取值求和。</p>
<p>也就是跨通道的一个 Normalization 操作。</p>
<h2 id="torchnnlocalresponsenorm">torch.nn.LocalResponseNorm()<a hidden class="anchor" aria-hidden="true" href="#torchnnlocalresponsenorm">#</a></h2>
<p>$$b_{c} = a_{c}\left(k + \frac{\alpha}{n}
​    \sum_{c&rsquo;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&rsquo;}^2\right)^{-\beta}$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>Init signature:
</span></span><span style="display:flex;"><span>nn.LocalResponseNorm<span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    size:int,
</span></span><span style="display:flex;"><span>    alpha:float<span style="color:#f92672">=</span>0.0001,
</span></span><span style="display:flex;"><span>    beta:float<span style="color:#f92672">=</span>0.75,
</span></span><span style="display:flex;"><span>    k:float<span style="color:#f92672">=</span>1.0,
</span></span><span style="display:flex;"><span><span style="color:#f92672">)</span> -&gt; None
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Docstring:     
</span></span><span style="display:flex;"><span>Applies local response normalization over an input signal composed
</span></span><span style="display:flex;"><span>of several input planes, where channels occupy the second dimension.
</span></span><span style="display:flex;"><span>Applies normalization across channels.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Args:
</span></span><span style="display:flex;"><span>    size: amount of neighbouring channels used <span style="color:#66d9ef">for</span> normalization
</span></span><span style="display:flex;"><span>    alpha: multiplicative factor. Default: 0.0001
</span></span><span style="display:flex;"><span>    beta: exponent. Default: 0.75
</span></span><span style="display:flex;"><span>    k: additive factor. Default: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Shape:
</span></span><span style="display:flex;"><span>    - Input: :math:<span style="color:#e6db74">`</span><span style="color:#f92672">(</span>N, C, *<span style="color:#f92672">)</span><span style="color:#e6db74">`</span>
</span></span><span style="display:flex;"><span>    - Output: :math:<span style="color:#e6db74">`</span><span style="color:#f92672">(</span>N, C, *<span style="color:#f92672">)</span><span style="color:#e6db74">`</span> <span style="color:#f92672">(</span>same shape as input<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Examples::
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    &gt;&gt;&gt; lrn <span style="color:#f92672">=</span> nn.LocalResponseNorm<span style="color:#f92672">(</span>2<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    &gt;&gt;&gt; signal_2d <span style="color:#f92672">=</span> torch.randn<span style="color:#f92672">(</span>32, 5, 24, 24<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    &gt;&gt;&gt; signal_4d <span style="color:#f92672">=</span> torch.randn<span style="color:#f92672">(</span>16, 5, 7, 7, 7, 7<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    &gt;&gt;&gt; output_2d <span style="color:#f92672">=</span> lrn<span style="color:#f92672">(</span>signal_2d<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    &gt;&gt;&gt; output_4d <span style="color:#f92672">=</span> lrn<span style="color:#f92672">(</span>signal_4d<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>使用：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>__version__
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &#39;1.7.0&#39;</span>
</span></span><span style="display:flex;"><span>lrn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LocalResponseNorm(size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>signal_2d <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">96</span>, <span style="color:#ae81ff">55</span>, <span style="color:#ae81ff">55</span>) <span style="color:#75715e"># batch_size=32, feature_map=96×55×55</span>
</span></span><span style="display:flex;"><span>output_2d <span style="color:#f92672">=</span> lrn(signal_2d)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>signal_2d<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([32, 96, 55, 55])</span>
</span></span><span style="display:flex;"><span>output_2d<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.Size([32, 96, 55, 55])</span>
</span></span></code></pre></div><h2 id="手算检验">手算检验<a hidden class="anchor" aria-hidden="true" href="#手算检验">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>__version__
</span></span></code></pre></div><pre tabindex="0"><code>&#39;1.7.0&#39;
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lrn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LocalResponseNorm(size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">666</span>)
</span></span></code></pre></div><pre tabindex="0"><code>&lt;torch._C.Generator at 0x7ffd61652d38&gt;
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>signal_2d <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># batch_size=2, feature_map=3×2×2</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>output_2d <span style="color:#f92672">=</span> lrn(signal_2d)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>output_2d<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([2, 3, 2, 2])
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>signal_2d<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([2, 3, 2, 2])
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>signal_2d
</span></span></code></pre></div><pre tabindex="0"><code>tensor([[[[-0.7747,  0.7926],
          [-0.0062, -0.4377]],

         [[ 0.4657, -0.1880],
          [-0.8975,  0.4169]],

         [[-0.3479, -0.4007],
          [ 0.8059, -0.1021]]],
         
        [[[-0.3055, -1.7611],
          [-0.6461,  0.3470]],

         [[ 0.9144,  1.6259],
          [-0.6535, -0.0865]],

         [[ 0.2100,  0.4811],
          [ 0.4506,  0.0600]]]])
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>output_2d
</span></span></code></pre></div><pre tabindex="0"><code>tensor([[[[-0.4607,  0.4713],
          [-0.0037, -0.2603]],

         [[ 0.2769, -0.1118],
          [-0.5336,  0.2479]],

         [[-0.2069, -0.2383],
          [ 0.4792, -0.0607]]],
         
         
        [[[-0.1816, -1.0471],
          [-0.3842,  0.2063]],

         [[ 0.5437,  0.9667],
          [-0.3886, -0.0514]],

         [[ 0.1249,  0.2860],
          [ 0.2680,  0.0357]]]])
</code></pre><p>分析：</p>
<p>这个 batch 里面的第一张特征图、第一个通道、(0,0) 位置的取值为 -0.7747。接下来分析其 LRN 归一化后的值。</p>
<p>LRN 的超参数：<code>size=5, alpha=0.0001, beta=0.75, k=2</code>。</p>
<p>$$b_{c} = a_{c}\left(k + \frac{\alpha}{n}
​    \sum_{c&rsquo;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&rsquo;}^2\right)^{-\beta}$$</p>
<p>$$\frac{-0.7747}{(2 + \frac{0.0001}{5} (0.7747^2 + 0.4657^2 + 0.3479^2))^{0.75}} = -0.4607$$</p>
<ul>
<li>
<p>跨通道求和的下限：max(0, 0 - 5/2) = 0</p>
</li>
<li>
<h2 id="上限min2-0--52--2">上限：min(2, 0 + 5/2) = 2<a hidden class="anchor" aria-hidden="true" href="#上限min2-0--52--2">#</a></h2>
</li>
</ul>
<p><img loading="lazy" src="./20210205/5.png" alt=""  />
</p>
<hr>
<p>LRN 归一化公式内部的细节理解就先这样了，至于它更深层的作用，以及它为什么会被舍弃，留到后面。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://landodo.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li>
      <li><a href="http://landodo.github.io/tags/cnn/">CNN</a></li>
      <li><a href="http://landodo.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://landodo.github.io/posts/20210301-attention-mechanism/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Attention mechanism: SENet &amp; SKNet</span>
  </a>
  <a class="next" href="http://landodo.github.io/posts/20210130-pytorch-paper-reading-1/">
    <span class="title">Next Page »</span>
    <br>
    <span>PyTorch: An Imperative Style, High-Performance Deep Learning Library</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Landon</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
