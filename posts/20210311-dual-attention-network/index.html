<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<meta name="robots" content="index, follow">
<title>Dual Attention Network for Scene Segmentation | Landodo&#39;s NoteBook</title>
<meta name="keywords" content="CNN, è®ºæ–‡é˜…è¯»" />
<meta name="description" content="Dual Attention Network for Scene Segmentation arXiv: 1809.02983 â€œç”¨äºåœºæ™¯åˆ†å‰²çš„åŒæ³¨æ„åŠ›ç½‘ç»œâ€œ Abstract é’ˆå¯¹åœºæ™¯åˆ†å‰²ä»»åŠ¡ï¼ŒåŸºäº self-Attention æœºåˆ¶æ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚ ä¸ä»¥å¾€é€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆæ¥æ•æ‰ä¸Šä¸‹æ–‡çš„">
<meta name="author" content="">
<link rel="canonical" href="http://landodo.github.io/posts/20210311-dual-attention-network/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://landodo.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://landodo.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://landodo.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://landodo.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://landodo.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Dual Attention Network for Scene Segmentation" />
<meta property="og:description" content="Dual Attention Network for Scene Segmentation arXiv: 1809.02983 â€œç”¨äºåœºæ™¯åˆ†å‰²çš„åŒæ³¨æ„åŠ›ç½‘ç»œâ€œ Abstract é’ˆå¯¹åœºæ™¯åˆ†å‰²ä»»åŠ¡ï¼ŒåŸºäº self-Attention æœºåˆ¶æ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚ ä¸ä»¥å¾€é€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆæ¥æ•æ‰ä¸Šä¸‹æ–‡çš„" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://landodo.github.io/posts/20210311-dual-attention-network/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-03-11T10:17:29&#43;08:00" />
<meta property="article:modified_time" content="2021-03-11T10:17:29&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Dual Attention Network for Scene Segmentation"/>
<meta name="twitter:description" content="Dual Attention Network for Scene Segmentation arXiv: 1809.02983 â€œç”¨äºåœºæ™¯åˆ†å‰²çš„åŒæ³¨æ„åŠ›ç½‘ç»œâ€œ Abstract é’ˆå¯¹åœºæ™¯åˆ†å‰²ä»»åŠ¡ï¼ŒåŸºäº self-Attention æœºåˆ¶æ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚ ä¸ä»¥å¾€é€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆæ¥æ•æ‰ä¸Šä¸‹æ–‡çš„"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://landodo.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Dual Attention Network for Scene Segmentation",
      "item": "http://landodo.github.io/posts/20210311-dual-attention-network/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Dual Attention Network for Scene Segmentation",
  "name": "Dual Attention Network for Scene Segmentation",
  "description": "Dual Attention Network for Scene Segmentation arXiv: 1809.02983 â€œç”¨äºåœºæ™¯åˆ†å‰²çš„åŒæ³¨æ„åŠ›ç½‘ç»œâ€œ Abstract é’ˆå¯¹åœºæ™¯åˆ†å‰²ä»»åŠ¡ï¼ŒåŸºäº self-Attention æœºåˆ¶æ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚ ä¸ä»¥å¾€é€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆæ¥æ•æ‰ä¸Šä¸‹æ–‡çš„",
  "keywords": [
    "CNN", "è®ºæ–‡é˜…è¯»"
  ],
  "articleBody": "Dual Attention Network for Scene Segmentation arXiv: 1809.02983\nâ€œç”¨äºåœºæ™¯åˆ†å‰²çš„åŒæ³¨æ„åŠ›ç½‘ç»œâ€œ\nAbstract é’ˆå¯¹åœºæ™¯åˆ†å‰²ä»»åŠ¡ï¼ŒåŸºäº self-Attention æœºåˆ¶æ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚\nä¸ä»¥å¾€é€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆæ¥æ•æ‰ä¸Šä¸‹æ–‡çš„å·¥ä½œä¸åŒï¼Œæœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŒæ³¨æ„åŠ›ç½‘ç»œï¼ˆDual Attention Network, DANetï¼‰æ¥è‡ªé€‚åº”åœ°æ•´åˆå±€éƒ¨ç‰¹å¾ä¸å…¶å…¨å±€ä¾èµ–æ€§ã€‚\ntwo types of attention modulesï¼šåˆ†åˆ«åœ¨ç©ºé—´ç»´åº¦å’Œé€šé“ç»´åº¦ä¸Šå»ºç«‹è¯­ä¹‰ç›¸äº’ä¾èµ–çš„æ¨¡å‹ã€‚\nï¼ˆ1ï¼‰position attention module\n å¯¹æ‰€æœ‰ä½ç½®çš„ç‰¹å¾è¿›è¡ŒåŠ æƒå’Œï¼Œé€‰æ‹©æ€§åœ°èšåˆæ¯ä¸ªä½ç½®çš„ç‰¹å¾ã€‚  ï¼ˆ2ï¼‰channel attention module\n é€šé“å…³æ³¨æ¨¡å—æœ‰é€‰æ‹©åœ°å¼ºè°ƒç›¸äº’ä¾èµ–çš„é€šé“å›¾ï¼Œé€šè¿‡æ•´åˆæ‰€æœ‰é€šé“å›¾ä¹‹é—´çš„ç›¸å…³ç‰¹å¾ã€‚  å°†ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºç›¸åŠ ï¼Œä»¥è¿›ä¸€æ­¥å®ç°æ”¹å–„ç‰¹å¾è¡¨ç¤ºï¼Œè¿™æœ‰åŠ©äºæ›´ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚\nåœ¨ Cityscapesã€PASCAL Context å’Œ COCO Stuff æ•°æ®é›†ä¸Šå–å¾—äº† SOTAã€‚\n1. Introduction æœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶å·¥ä½œï¼Œç§°ä¸ºä½œä¸ºåŒæ³¨æ„åŠ›ç½‘ç»œï¼ˆDual Attention Network, DANetï¼‰ï¼Œç”¨äºè‡ªç„¶åœºæ™¯å›¾åƒåˆ†å‰²ï¼Œå¦‚ Figure 2ã€‚\nå®ƒå¼•å…¥äº†ä¸€ç§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«æ•æ‰ç©ºé—´å’Œé€šé“ç»´åº¦çš„ç‰¹å¾ä¾èµ–æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ‰©å¼ çš„ FCN ä¹‹ä¸Šé™„åŠ äº†ä¸¤ä¸ªå¹³è¡Œçš„æ³¨æ„åŠ›æ¨¡å—ã€‚ä¸€ä¸ªæ˜¯ä½ç½®æ³¨æ„åŠ›æ¨¡å—ï¼ˆposition attention moduleï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯é€šé“æ³¨æ„åŠ›æ¨¡å—ï¼ˆchannel attention moduleï¼‰ã€‚\nå¯¹äºä½ç½®æ³¨æ„åŠ›æ¨¡å—ï¼Œå¼•å…¥è‡ªå…³æ³¨æœºåˆ¶æ¥æ•æ‰ç‰¹å¾å›¾ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„ç©ºé—´ä¾èµ–æ€§ã€‚\nå¯¹äºé€šé“æ³¨æ„åŠ›æ¨¡å—ï¼Œä½¿ç”¨ç±»ä¼¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰ä»»æ„ä¸¤ä¸ªé€šé“å›¾ä¹‹é—´çš„é€šé“ä¾èµ–æ€§ã€‚\næœ€åï¼Œå°†è¿™ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºè¿›è¡Œèåˆï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚\n å›¾ 1ï¼šåœºæ™¯åˆ†å‰²çš„ç›®æ ‡æ˜¯è¯†åˆ«æ¯ä¸ªåƒç´ ï¼ŒåŒ…æ‹¬ä¸œè¥¿ã€ä¸åŒçš„ç‰©ä½“ã€‚ç‰©ä½“/ä¸œè¥¿çš„å„ç§å°ºåº¦ã€é®æŒ¡å’Œå…‰ç…§å˜åŒ–ï¼Œä½¿å¾—è§£ææ¯ä¸ªåƒç´ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚\n 2. Related Work â€œAttention is all your needâ€ ç‡å…ˆæå‡ºäº†ç»˜åˆ¶è¾“å…¥çš„å…¨å±€ä¾èµ–æ€§çš„è‡ªæ³¨æ„æœºåˆ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœºå™¨ç¿»è¯‘ä¸­ã€‚\n3. Dual Attention Network åœ¨æœ¬èŠ‚ä¸­ï¼Œé¦–å…ˆä»‹ç»ç½‘ç»œçš„æ€»ä½“æ¡†æ¶ï¼Œç„¶åä»‹ç»äº†ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒä»¬åˆ†åˆ«åœ¨ç©ºé—´å’Œé€šé“ç»´åº¦ä¸Šæ•æ‰é•¿ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ€åä»‹ç»å¦‚ä½•å°†å®ƒä»¬èšåˆåœ¨ä¸€èµ·è¿›è¡Œè¿›ä¸€æ­¥çš„å®Œå–„ã€‚\n3.1. Overview å»æ‰äº†å‘ä¸‹é‡‡æ ·æ“ä½œï¼Œå¹¶åœ¨æœ€åä¸¤ä¸ª ResNet å—ä¸­é‡‡ç”¨äº†ç©ºæ´å·ç§¯ï¼Œä»è€Œå°†æœ€åçš„ç‰¹å¾å›¾çš„å¤§å°æ”¾å¤§åˆ°è¾“å…¥å›¾åƒçš„ 1/8ã€‚å®ƒä¿ç•™äº†æ›´å¤šçš„ç»†èŠ‚ï¼Œè€Œæ²¡æœ‰å¢åŠ é¢å¤–çš„å‚æ•°ã€‚ç„¶åï¼Œæ®‹å·®ç½‘ç»œçš„ç‰¹å¾å°†è¢«è¾“å…¥åˆ°ä¸¤ä¸ªå¹³è¡Œçš„æ³¨æ„åŠ›æ¨¡å—ä¸­ã€‚\nFigure 2 åœ¨ä¸Šéƒ¨åˆ†æ˜¯ spatial attention modulesã€‚é¦–å…ˆåº”ç”¨å·ç§¯å±‚æ¥è·å¾—é™ç»´çš„ç‰¹å¾ã€‚ç„¶åå°†è¿™äº›ç‰¹å¾åé¦ˆåˆ°ä½ç½®æ³¨æ„åŠ›æ¨¡å—ä¸­ï¼Œé€šè¿‡ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ç”Ÿæˆæ–°çš„ç©ºé—´é•¿ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ç‰¹å¾ï¼ˆspatial long-range contextual informationï¼‰ã€‚\nï¼ˆ1ï¼‰ç¬¬ä¸€æ­¥æ˜¯ç”Ÿæˆä¸€ä¸ªç©ºé—´æ³¨æ„åŠ›çŸ©é˜µï¼Œè¯¥çŸ©é˜µå¯¹ç‰¹å¾çš„ä»»æ„ä¸¤ä¸ªåƒç´ ä¹‹é—´çš„ç©ºé—´å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚\nï¼ˆ2ï¼‰åœ¨æ³¨æ„åŠ›çŸ©é˜µå’ŒåŸå§‹ç‰¹å¾ä¹‹é—´è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚\nï¼ˆ3ï¼‰å¯¹ä¸Šè¿°ä¹˜æ³•ç»“æœçŸ©é˜µå’ŒåŸå§‹ç‰¹å¾è¿›è¡Œå…ƒç´ æ±‚å’Œæ“ä½œï¼Œä»¥è·å¾—åæ˜ è¿œè·ç¦»ä¸Šä¸‹æ–‡çš„æœ€ç»ˆè¡¨ç¤ºã€‚\nåŒæ—¶ï¼Œé€šé“ç»´åº¦çš„è¿œè·ç¦»ä¸Šä¸‹æ–‡ä¿¡æ¯ç”±é€šé“æ³¨æ„åŠ›æ¨¡å—æ•è·ã€‚æ•æ‰é€šé“å…³ç³»çš„è¿‡ç¨‹ä¸ä½ç½®å…³æ³¨æ¨¡å—ç±»ä¼¼ï¼Œåªæ˜¯ç¬¬ä¸€æ­¥ï¼Œåœ¨é€šé“ç»´åº¦ä¸Šè®¡ç®—é€šé“æ³¨æ„åŠ›çŸ©é˜µã€‚\næœ€åï¼Œå°†ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºæ±‡æ€»ï¼Œä»¥è·å¾—æ›´å¥½çš„ç‰¹å¾è¡¨ç¤ºï¼Œç”¨äºåƒç´ çº§é¢„æµ‹ã€‚\n3.2. Position Attention Module ç»™å®šä¸€ä¸ªå±€éƒ¨ç‰¹å¾å›¾ $A \\in \\mathbb{R}^{CÃ—HÃ— W}$ï¼Œé¦–å…ˆå°†å…¶é€å…¥å·ç§¯å±‚ï¼Œåˆ†åˆ«ç”Ÿæˆä¸¤ä¸ªæ–°çš„ç‰¹å¾å›¾ B å’Œ Cï¼Œ$(B, C) \\in \\mathbb{R}^{C \\times H \\times W}$ã€‚\nç„¶åå°† Bã€C reshape ä¸º $(B, C) \\in \\mathbb{R}^{C \\times N}$ï¼Œå…¶ä¸­ $N = H \\times W$ï¼Œè¡¨ç¤ºä¸€ä¸ªé€šé“çš„ç‰¹å¾å›¾çš„åƒç´ æ€»æ•°ã€‚\nä¹‹åï¼ŒçŸ©é˜µä¹˜æ³• $C^T \\cdot B$ï¼Œå¹¶åº”ç”¨ softmax å±‚è®¡ç®—ç©ºé—´æ³¨æ„åŠ›å›¾ $S \\in \\mathbb{R}^{NÃ—N}$ã€‚\n $s_{ji}$ è¡¡é‡çš„æ˜¯è¡¡é‡ç¬¬ i ä¸ªä½ç½®å¯¹ç¬¬ j ä¸ªä½ç½®çš„å½±å“ã€‚ä¸¤ç§ä½ç½®çš„ç‰¹å¾è¡¨å¾è¶Šç›¸ä¼¼ï¼Œæœ‰åŠ©äºæé«˜å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚  åŒæ—¶ï¼Œéœ€è¦å°† A ç»è¿‡ä¸€ä¸ªå·ç§¯å±‚å¾—åˆ°ç‰¹å¾å›¾ Dï¼Œ$D \\in \\mathbb{R}^{C \\times H \\times W}$ï¼Œç„¶åå°† D reshape æˆ $D \\in \\mathbb{R}^{C \\times N}$ã€‚ç„¶åå† D å’Œ S è¿›è¡ŒçŸ©é˜µä¹˜æ³•åï¼Œå°†å¾—åˆ°çš„ç»“æœ reshape ä¸º $\\mathbb{R}^{C \\times H \\times W}$ã€‚ä¹˜ä»¥æ¯”ä¾‹å‚æ•°ï¼ˆscale parameterï¼‰$\\alpha$ åä¸ç‰¹å¾å›¾ A è¿›è¡Œ element-wise sum operation å¾—åˆ°æœ€ç»ˆçš„ç»“æœ $E \\in \\mathbb{R}^{C \\times H \\times W}$ã€‚\nç”±å…¬å¼ (2) å¯ä»¥æ¨æ–­ï¼Œæ¯ä¸ªä½ç½®çš„ç»“æœç‰¹å¾ E æ˜¯æ‰€æœ‰ä½ç½®çš„ç‰¹å¾å’ŒåŸå§‹ç‰¹å¾çš„åŠ æƒå’Œã€‚å› æ­¤ï¼Œå®ƒå…·æœ‰å…¨å±€çš„ä¸Šä¸‹æ–‡è§†å›¾ï¼Œå¹¶æ ¹æ®ç©ºé—´æ³¨æ„åŠ›å›¾æœ‰é€‰æ‹©åœ°èšåˆä¸Šä¸‹æ–‡ã€‚ç›¸ä¼¼çš„è¯­ä¹‰ç‰¹å¾å®ç°äº†ç›¸äº’å¢ç›Šï¼Œä»è€Œå¯¼å…¥äº†ç±»å†…ç´§å‡‘å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚\n3.3. Channel Attention Module é€šè¿‡åˆ©ç”¨é€šé“å›¾ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¯ä»¥å¼ºè°ƒç›¸äº’ä¾èµ–çš„ç‰¹å¾å›¾ï¼Œæ”¹å–„ç‰¹å®šè¯­ä¹‰çš„ç‰¹å¾è¡¨ç¤ºã€‚\nå› æ­¤ï¼Œæ„å»ºä¸€ä¸ªé€šé“æ³¨æ„åŠ›æ¨¡å—æ¥æ˜ç¡®å»ºæ¨¡é€šé“ä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ã€‚\nä¸ä½ç½®æ³¨æ„åŠ›æ¨¡å—ä¸åŒï¼Œè¿™é‡Œç›´æ¥é€šè¿‡åŸå§‹ç‰¹å¾å›¾ $A \\in \\mathbb{R}^{C \\times H \\times W}$ è®¡ç®—é€šé“æ³¨æ„åŠ›å›¾ï¼ˆchannel attention mapï¼‰$X \\in \\mathbb{R}^{C \\times C}$ã€‚\nç»†èŠ‚ä¸Šï¼Œå…ˆå°† $A \\in \\mathbb{R}^{C \\times H \\times W}$ reshape æˆ $A \\in \\mathbb{R}^{C \\times N}$ï¼Œç„¶å A ä¸ A çš„è½¬ç½®è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œ$A \\cdot A^T$ã€‚æœ€åï¼Œåº”ç”¨ softmax å±‚å¾—åˆ°é€šé“æ³¨æ„åŠ›å›¾ $X \\in \\mathbb{R}^{C \\times C}$ï¼š\n å…¶ä¸­ï¼Œå…¶ä¸­ $x_{ji}$è¡¡é‡ç¬¬ i ä¸ªé€šé“å¯¹ç¬¬ j ä¸ªé€šé“çš„å½±å“ã€‚  $X^T \\cdot A$ å¹¶å°†ç»“æœ reshape ä¸º $\\mathbb{R}^{C \\times H \\times W}$ï¼Œæœ€åçš„æ“ä½œå’Œ Position Attention Module ç±»ä¼¼ï¼Œscale parameter ä¸º $\\beta$ã€‚\nä»å…¬å¼ (4) å¯ä»¥çœ‹å‡ºï¼Œæ¯ä¸ªé€šé“çš„æœ€ç»ˆç‰¹å¾æ˜¯æ‰€æœ‰é€šé“çš„ç‰¹å¾å’ŒåŸå§‹ç‰¹å¾çš„åŠ æƒå’Œï¼Œå®ƒæ¨¡æ‹Ÿäº†ç‰¹å¾å›¾ä¹‹é—´çš„é•¿ç¨‹è¯­ä¹‰ä¾èµ–å…³ç³»ã€‚å®ƒæœ‰åŠ©äºæå‡ç‰¹å¾çš„å¯åˆ†è¾¨æ€§ã€‚\nåˆ©ç”¨æ‰€æœ‰ç›¸å…³ä½ç½®çš„ç©ºé—´ä¿¡æ¯æ¥æ¨¡æ‹Ÿé€šé“ç›¸å…³æ€§ã€‚\n we exploit spatial information at all corresponding positions to model channel correlations.\n 3.4. Attention Module Embedding with Networks æœ¬ç¯‡è®ºæ–‡æå‡ºçš„æ³¨æ„åŠ›æ¨¡å—å¾ˆç®€å•ï¼Œå¯ä»¥ç›´æ¥æ’å…¥åˆ°ç°æœ‰çš„ FCN pipeline ä¸­ã€‚å®ƒä»¬ä¸ä¼šå¢åŠ å¤ªå¤šå‚æ•°ï¼Œå´èƒ½æœ‰æ•ˆåœ°åŠ å¼ºç‰¹å¾è¡¨ç¤ºã€‚\n4. Experiments 4.2.1 Ablation Study for Attention Modules: Table 1, Figure 4, Figure 5.\n4.2.2 Study for Improvement Strategies: Table 2.\n4.2.3 Visualization of Attention Module: Figure 6.\n4.2.4 Comparing with State-of-the-art: Table 3.\n4.3. Results on PASCAL VOC 2012 Dataset: Table4, Table5.\n4.4. Results on PASCAL Context Dataset: Table 6\n4.5. Results on COCO Stuff Dataset: Table 7.\n5. Conclusion  presented a Dual Attention Network (DANet) for scene segmentation, which adaptively integrates local semantic features using the self-attention mechanism. position attention module and a channel attention module to capture global dependencies in the spatial and channel dimensions respectively. dual attention modules capture long-range contextual information effectively and give more precise segmentation results. Dual Attention Network (DANet) achieves outstanding performance consistently on four scene segmentation datasets, Cityscapes, Pascal VOC 2012, Pascal Context, and COCO Stuff.  ",
  "wordCount" : "2236",
  "inLanguage": "en",
  "datePublished": "2021-03-11T10:17:29+08:00",
  "dateModified": "2021-03-11T10:17:29+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://landodo.github.io/posts/20210311-dual-attention-network/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Landodo's NoteBook",
    "logo": {
      "@type": "ImageObject",
      "url": "http://landodo.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://landodo.github.io/" accesskey="h" title="Landodo&#39;s NoteBook (Alt + H)">Landodo&#39;s NoteBook</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://landodo.github.io/search" title="ğŸ”Search (Alt &#43; /)" accesskey=/>
                    <span>ğŸ”Search</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/tags" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://landodo.github.io/cs-zoo" title="CS ZOO">
                    <span>CS ZOO</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://landodo.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="http://landodo.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Dual Attention Network for Scene Segmentation
    </h1>
    <div class="post-meta"><span title='2021-03-11 10:17:29 +0800 CST'>March 11, 2021</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;2236 words

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#dual-attention-network-for-scene-segmentation" aria-label="Dual Attention Network for Scene Segmentation">Dual Attention Network for Scene Segmentation</a><ul>
                        
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                <li>
                    <a href="#1-introduction" aria-label="1. Introduction">1. Introduction</a></li>
                <li>
                    <a href="#2-related-work" aria-label="2. Related Work">2. Related Work</a></li>
                <li>
                    <a href="#3-dual-attention-network" aria-label="3. Dual Attention Network">3. Dual Attention Network</a><ul>
                        
                <li>
                    <a href="#31-overview" aria-label="3.1. Overview">3.1. Overview</a></li>
                <li>
                    <a href="#32-position-attention-module" aria-label="3.2. Position Attention Module">3.2. Position Attention Module</a></li>
                <li>
                    <a href="#33-channel-attention-module" aria-label="3.3. Channel Attention Module">3.3. Channel Attention Module</a></li>
                <li>
                    <a href="#34-attention-module-embedding-with-networks" aria-label="3.4. Attention Module Embedding with Networks">3.4. Attention Module Embedding with Networks</a></li></ul>
                </li>
                <li>
                    <a href="#4-experiments" aria-label="4. Experiments">4. Experiments</a></li>
                <li>
                    <a href="#5-conclusion" aria-label="5. Conclusion">5. Conclusion</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="dual-attention-network-for-scene-segmentation">Dual Attention Network for Scene Segmentation<a hidden class="anchor" aria-hidden="true" href="#dual-attention-network-for-scene-segmentation">#</a></h1>
<p>arXiv: 1809.02983</p>
<p>â€œç”¨äºåœºæ™¯åˆ†å‰²çš„åŒæ³¨æ„åŠ›ç½‘ç»œâ€œ</p>
<h2 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<p>é’ˆå¯¹åœºæ™¯åˆ†å‰²ä»»åŠ¡ï¼ŒåŸºäº self-Attention æœºåˆ¶æ•æ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚</p>
<p>ä¸ä»¥å¾€é€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆæ¥æ•æ‰ä¸Šä¸‹æ–‡çš„å·¥ä½œä¸åŒï¼Œæœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŒæ³¨æ„åŠ›ç½‘ç»œï¼ˆDual Attention Network, DANetï¼‰æ¥è‡ªé€‚åº”åœ°æ•´åˆå±€éƒ¨ç‰¹å¾ä¸å…¶å…¨å±€ä¾èµ–æ€§ã€‚</p>
<p>two types of attention modulesï¼šåˆ†åˆ«åœ¨ç©ºé—´ç»´åº¦å’Œé€šé“ç»´åº¦ä¸Šå»ºç«‹è¯­ä¹‰ç›¸äº’ä¾èµ–çš„æ¨¡å‹ã€‚</p>
<p>ï¼ˆ1ï¼‰position attention module</p>
<ul>
<li>å¯¹æ‰€æœ‰ä½ç½®çš„ç‰¹å¾è¿›è¡ŒåŠ æƒå’Œï¼Œé€‰æ‹©æ€§åœ°èšåˆæ¯ä¸ªä½ç½®çš„ç‰¹å¾ã€‚</li>
</ul>
<p>ï¼ˆ2ï¼‰channel attention module</p>
<ul>
<li>é€šé“å…³æ³¨æ¨¡å—æœ‰é€‰æ‹©åœ°å¼ºè°ƒç›¸äº’ä¾èµ–çš„é€šé“å›¾ï¼Œé€šè¿‡æ•´åˆæ‰€æœ‰é€šé“å›¾ä¹‹é—´çš„ç›¸å…³ç‰¹å¾ã€‚</li>
</ul>
<p>å°†ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºç›¸åŠ ï¼Œä»¥è¿›ä¸€æ­¥å®ç°æ”¹å–„ç‰¹å¾è¡¨ç¤ºï¼Œè¿™æœ‰åŠ©äºæ›´ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚</p>
<p>åœ¨ Cityscapesã€PASCAL Context å’Œ COCO Stuff æ•°æ®é›†ä¸Šå–å¾—äº† SOTAã€‚</p>
<h2 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h2>
<p>æœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶å·¥ä½œï¼Œç§°ä¸ºä½œä¸ºåŒæ³¨æ„åŠ›ç½‘ç»œï¼ˆDual Attention Network, DANetï¼‰ï¼Œç”¨äºè‡ªç„¶åœºæ™¯å›¾åƒåˆ†å‰²ï¼Œå¦‚ Figure 2ã€‚</p>
<p><img loading="lazy" src="./20210311/1.png" alt=""  />
</p>
<p>å®ƒå¼•å…¥äº†ä¸€ç§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«æ•æ‰ç©ºé—´å’Œé€šé“ç»´åº¦çš„ç‰¹å¾ä¾èµ–æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ‰©å¼ çš„ FCN ä¹‹ä¸Šé™„åŠ äº†ä¸¤ä¸ªå¹³è¡Œçš„æ³¨æ„åŠ›æ¨¡å—ã€‚ä¸€ä¸ªæ˜¯ä½ç½®æ³¨æ„åŠ›æ¨¡å—ï¼ˆposition attention moduleï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯é€šé“æ³¨æ„åŠ›æ¨¡å—ï¼ˆchannel attention moduleï¼‰ã€‚</p>
<p>å¯¹äºä½ç½®æ³¨æ„åŠ›æ¨¡å—ï¼Œå¼•å…¥è‡ªå…³æ³¨æœºåˆ¶æ¥æ•æ‰ç‰¹å¾å›¾ä¸­<strong>ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„ç©ºé—´ä¾èµ–æ€§</strong>ã€‚</p>
<p>å¯¹äºé€šé“æ³¨æ„åŠ›æ¨¡å—ï¼Œä½¿ç”¨ç±»ä¼¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰<strong>ä»»æ„ä¸¤ä¸ªé€šé“å›¾ä¹‹é—´çš„é€šé“ä¾èµ–æ€§</strong>ã€‚</p>
<p>æœ€åï¼Œå°†è¿™ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºè¿›è¡Œèåˆï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚</p>
<p><img loading="lazy" src="./20210311/2.png" alt=""  />
</p>
<blockquote>
<p>å›¾ 1ï¼šåœºæ™¯åˆ†å‰²çš„ç›®æ ‡æ˜¯è¯†åˆ«æ¯ä¸ªåƒç´ ï¼ŒåŒ…æ‹¬ä¸œè¥¿ã€ä¸åŒçš„ç‰©ä½“ã€‚ç‰©ä½“/ä¸œè¥¿çš„å„ç§å°ºåº¦ã€é®æŒ¡å’Œå…‰ç…§å˜åŒ–ï¼Œä½¿å¾—è§£ææ¯ä¸ªåƒç´ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
</blockquote>
<h2 id="2-related-work">2. Related Work<a hidden class="anchor" aria-hidden="true" href="#2-related-work">#</a></h2>
<p>â€œAttention is all your needâ€ ç‡å…ˆæå‡ºäº†ç»˜åˆ¶è¾“å…¥çš„å…¨å±€ä¾èµ–æ€§çš„è‡ªæ³¨æ„æœºåˆ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœºå™¨ç¿»è¯‘ä¸­ã€‚</p>
<h2 id="3-dual-attention-network">3. Dual Attention Network<a hidden class="anchor" aria-hidden="true" href="#3-dual-attention-network">#</a></h2>
<p>åœ¨æœ¬èŠ‚ä¸­ï¼Œé¦–å…ˆä»‹ç»ç½‘ç»œçš„æ€»ä½“æ¡†æ¶ï¼Œç„¶åä»‹ç»äº†ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒä»¬åˆ†åˆ«åœ¨ç©ºé—´å’Œé€šé“ç»´åº¦ä¸Šæ•æ‰é•¿ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ€åä»‹ç»å¦‚ä½•å°†å®ƒä»¬èšåˆåœ¨ä¸€èµ·è¿›è¡Œè¿›ä¸€æ­¥çš„å®Œå–„ã€‚</p>
<h3 id="31-overview">3.1. Overview<a hidden class="anchor" aria-hidden="true" href="#31-overview">#</a></h3>
<p><img loading="lazy" src="./20210311/1.png" alt=""  />
</p>
<p>å»æ‰äº†å‘ä¸‹é‡‡æ ·æ“ä½œï¼Œå¹¶åœ¨æœ€åä¸¤ä¸ª ResNet å—ä¸­é‡‡ç”¨äº†ç©ºæ´å·ç§¯ï¼Œä»è€Œå°†æœ€åçš„ç‰¹å¾å›¾çš„å¤§å°æ”¾å¤§åˆ°è¾“å…¥å›¾åƒçš„ 1/8ã€‚å®ƒä¿ç•™äº†æ›´å¤šçš„ç»†èŠ‚ï¼Œè€Œæ²¡æœ‰å¢åŠ é¢å¤–çš„å‚æ•°ã€‚ç„¶åï¼Œæ®‹å·®ç½‘ç»œçš„ç‰¹å¾å°†è¢«è¾“å…¥åˆ°ä¸¤ä¸ªå¹³è¡Œçš„æ³¨æ„åŠ›æ¨¡å—ä¸­ã€‚</p>
<p>Figure 2 åœ¨ä¸Šéƒ¨åˆ†æ˜¯ spatial attention modulesã€‚é¦–å…ˆåº”ç”¨å·ç§¯å±‚æ¥è·å¾—é™ç»´çš„ç‰¹å¾ã€‚ç„¶åå°†è¿™äº›ç‰¹å¾åé¦ˆåˆ°ä½ç½®æ³¨æ„åŠ›æ¨¡å—ä¸­ï¼Œé€šè¿‡ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ç”Ÿæˆæ–°çš„ç©ºé—´é•¿ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ç‰¹å¾ï¼ˆspatial long-range contextual informationï¼‰ã€‚</p>
<p>ï¼ˆ1ï¼‰ç¬¬ä¸€æ­¥æ˜¯ç”Ÿæˆä¸€ä¸ªç©ºé—´æ³¨æ„åŠ›çŸ©é˜µï¼Œè¯¥çŸ©é˜µå¯¹ç‰¹å¾çš„ä»»æ„ä¸¤ä¸ªåƒç´ ä¹‹é—´çš„ç©ºé—´å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚</p>
<p>ï¼ˆ2ï¼‰åœ¨æ³¨æ„åŠ›çŸ©é˜µå’ŒåŸå§‹ç‰¹å¾ä¹‹é—´è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚</p>
<p>ï¼ˆ3ï¼‰å¯¹ä¸Šè¿°ä¹˜æ³•ç»“æœçŸ©é˜µå’ŒåŸå§‹ç‰¹å¾è¿›è¡Œå…ƒç´ æ±‚å’Œæ“ä½œï¼Œä»¥è·å¾—åæ˜ è¿œè·ç¦»ä¸Šä¸‹æ–‡çš„æœ€ç»ˆè¡¨ç¤ºã€‚</p>
<p>åŒæ—¶ï¼Œé€šé“ç»´åº¦çš„è¿œè·ç¦»ä¸Šä¸‹æ–‡ä¿¡æ¯ç”±é€šé“æ³¨æ„åŠ›æ¨¡å—æ•è·ã€‚æ•æ‰é€šé“å…³ç³»çš„è¿‡ç¨‹ä¸ä½ç½®å…³æ³¨æ¨¡å—ç±»ä¼¼ï¼Œåªæ˜¯ç¬¬ä¸€æ­¥ï¼Œåœ¨é€šé“ç»´åº¦ä¸Šè®¡ç®—é€šé“æ³¨æ„åŠ›çŸ©é˜µã€‚</p>
<p>æœ€åï¼Œå°†ä¸¤ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºæ±‡æ€»ï¼Œä»¥è·å¾—æ›´å¥½çš„ç‰¹å¾è¡¨ç¤ºï¼Œç”¨äºåƒç´ çº§é¢„æµ‹ã€‚</p>
<h3 id="32-position-attention-module">3.2. Position Attention Module<a hidden class="anchor" aria-hidden="true" href="#32-position-attention-module">#</a></h3>
<p><img loading="lazy" src="./20210311/3.png" alt=""  />
</p>
<p>ç»™å®šä¸€ä¸ªå±€éƒ¨ç‰¹å¾å›¾ $A \in \mathbb{R}^{CÃ—HÃ— W}$ï¼Œé¦–å…ˆå°†å…¶é€å…¥å·ç§¯å±‚ï¼Œåˆ†åˆ«ç”Ÿæˆä¸¤ä¸ªæ–°çš„ç‰¹å¾å›¾ B å’Œ Cï¼Œ$(B, C) \in \mathbb{R}^{C \times H \times W}$ã€‚</p>
<p>ç„¶åå°† Bã€C reshape ä¸º $(B, C) \in \mathbb{R}^{C \times N}$ï¼Œå…¶ä¸­ $N = H \times W$ï¼Œè¡¨ç¤ºä¸€ä¸ªé€šé“çš„ç‰¹å¾å›¾çš„åƒç´ æ€»æ•°ã€‚</p>
<p>ä¹‹åï¼ŒçŸ©é˜µä¹˜æ³• $C^T \cdot B$ï¼Œå¹¶åº”ç”¨ softmax å±‚è®¡ç®—ç©ºé—´æ³¨æ„åŠ›å›¾ $S \in \mathbb{R}^{NÃ—N}$ã€‚</p>
<p><img loading="lazy" src="./20210311/4.png" alt=""  />
</p>
<ul>
<li>$s_{ji}$ è¡¡é‡çš„æ˜¯è¡¡é‡ç¬¬ i ä¸ªä½ç½®å¯¹ç¬¬ j ä¸ªä½ç½®çš„å½±å“ã€‚ä¸¤ç§ä½ç½®çš„ç‰¹å¾è¡¨å¾è¶Šç›¸ä¼¼ï¼Œæœ‰åŠ©äºæé«˜å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li>
</ul>
<p>åŒæ—¶ï¼Œéœ€è¦å°† A ç»è¿‡ä¸€ä¸ªå·ç§¯å±‚å¾—åˆ°ç‰¹å¾å›¾ Dï¼Œ$D \in \mathbb{R}^{C \times H \times W}$ï¼Œç„¶åå°† D reshape æˆ $D \in \mathbb{R}^{C \times N}$ã€‚ç„¶åå† D å’Œ S è¿›è¡ŒçŸ©é˜µä¹˜æ³•åï¼Œå°†å¾—åˆ°çš„ç»“æœ reshape ä¸º $\mathbb{R}^{C \times H \times W}$ã€‚ä¹˜ä»¥æ¯”ä¾‹å‚æ•°ï¼ˆscale parameterï¼‰$\alpha$ åä¸ç‰¹å¾å›¾ A è¿›è¡Œ element-wise sum operation å¾—åˆ°æœ€ç»ˆçš„ç»“æœ $E \in \mathbb{R}^{C \times H \times W}$ã€‚</p>
<p><img loading="lazy" src="./20210311/5.png" alt=""  />
</p>
<p>ç”±å…¬å¼ (2) å¯ä»¥æ¨æ–­ï¼Œæ¯ä¸ªä½ç½®çš„ç»“æœç‰¹å¾ E æ˜¯æ‰€æœ‰ä½ç½®çš„ç‰¹å¾å’ŒåŸå§‹ç‰¹å¾çš„åŠ æƒå’Œã€‚å› æ­¤ï¼Œå®ƒå…·æœ‰å…¨å±€çš„ä¸Šä¸‹æ–‡è§†å›¾ï¼Œå¹¶æ ¹æ®ç©ºé—´æ³¨æ„åŠ›å›¾æœ‰é€‰æ‹©åœ°èšåˆä¸Šä¸‹æ–‡ã€‚ç›¸ä¼¼çš„è¯­ä¹‰ç‰¹å¾å®ç°äº†ç›¸äº’å¢ç›Šï¼Œä»è€Œå¯¼å…¥äº†ç±»å†…ç´§å‡‘å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<h3 id="33-channel-attention-module">3.3. Channel Attention Module<a hidden class="anchor" aria-hidden="true" href="#33-channel-attention-module">#</a></h3>
<p>é€šè¿‡åˆ©ç”¨é€šé“å›¾ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¯ä»¥å¼ºè°ƒç›¸äº’ä¾èµ–çš„ç‰¹å¾å›¾ï¼Œæ”¹å–„ç‰¹å®šè¯­ä¹‰çš„ç‰¹å¾è¡¨ç¤ºã€‚</p>
<p>å› æ­¤ï¼Œæ„å»ºä¸€ä¸ªé€šé“æ³¨æ„åŠ›æ¨¡å—æ¥æ˜ç¡®å»ºæ¨¡é€šé“ä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ã€‚</p>
<p><img loading="lazy" src="./20210311/6.png" alt=""  />
</p>
<p>ä¸ä½ç½®æ³¨æ„åŠ›æ¨¡å—ä¸åŒï¼Œè¿™é‡Œç›´æ¥é€šè¿‡åŸå§‹ç‰¹å¾å›¾ $A \in \mathbb{R}^{C \times H \times W}$ è®¡ç®—é€šé“æ³¨æ„åŠ›å›¾ï¼ˆchannel attention mapï¼‰$X \in \mathbb{R}^{C \times C}$ã€‚</p>
<p>ç»†èŠ‚ä¸Šï¼Œå…ˆå°† $A \in \mathbb{R}^{C \times H \times W}$ reshape æˆ $A \in \mathbb{R}^{C \times N}$ï¼Œç„¶å A ä¸ A çš„è½¬ç½®è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œ$A \cdot A^T$ã€‚æœ€åï¼Œåº”ç”¨ softmax å±‚å¾—åˆ°é€šé“æ³¨æ„åŠ›å›¾ $X \in \mathbb{R}^{C \times C}$ï¼š</p>
<p><img loading="lazy" src="./20210311/7.png" alt=""  />
</p>
<ul>
<li>å…¶ä¸­ï¼Œå…¶ä¸­ $x_{ji}$è¡¡é‡ç¬¬ i ä¸ªé€šé“å¯¹ç¬¬ j ä¸ªé€šé“çš„å½±å“ã€‚</li>
</ul>
<p>$X^T \cdot A$ å¹¶å°†ç»“æœ reshape ä¸º $\mathbb{R}^{C \times H \times W}$ï¼Œæœ€åçš„æ“ä½œå’Œ Position Attention Module ç±»ä¼¼ï¼Œscale parameter ä¸º $\beta$ã€‚</p>
<p><img loading="lazy" src="./20210311/8.png" alt=""  />
</p>
<p>ä»å…¬å¼ (4) å¯ä»¥çœ‹å‡ºï¼Œæ¯ä¸ªé€šé“çš„æœ€ç»ˆç‰¹å¾æ˜¯æ‰€æœ‰é€šé“çš„ç‰¹å¾å’ŒåŸå§‹ç‰¹å¾çš„åŠ æƒå’Œï¼Œå®ƒæ¨¡æ‹Ÿäº†ç‰¹å¾å›¾ä¹‹é—´çš„é•¿ç¨‹è¯­ä¹‰ä¾èµ–å…³ç³»ã€‚å®ƒæœ‰åŠ©äºæå‡ç‰¹å¾çš„å¯åˆ†è¾¨æ€§ã€‚</p>
<p>åˆ©ç”¨æ‰€æœ‰ç›¸å…³ä½ç½®çš„ç©ºé—´ä¿¡æ¯æ¥æ¨¡æ‹Ÿé€šé“ç›¸å…³æ€§ã€‚</p>
<blockquote>
<p>we exploit <strong>spatial information</strong> at all corresponding positions to model <strong>channel correlations</strong>.</p>
</blockquote>
<h3 id="34-attention-module-embedding-with-networks">3.4. Attention Module Embedding with Networks<a hidden class="anchor" aria-hidden="true" href="#34-attention-module-embedding-with-networks">#</a></h3>
<p>æœ¬ç¯‡è®ºæ–‡æå‡ºçš„æ³¨æ„åŠ›æ¨¡å—å¾ˆç®€å•ï¼Œå¯ä»¥ç›´æ¥æ’å…¥åˆ°ç°æœ‰çš„ FCN pipeline ä¸­ã€‚å®ƒä»¬ä¸ä¼šå¢åŠ å¤ªå¤šå‚æ•°ï¼Œå´èƒ½æœ‰æ•ˆåœ°åŠ å¼ºç‰¹å¾è¡¨ç¤ºã€‚</p>
<h2 id="4-experiments">4. Experiments<a hidden class="anchor" aria-hidden="true" href="#4-experiments">#</a></h2>
<p>4.2.1 Ablation Study for Attention Modules: Table 1, Figure 4, Figure 5.</p>
<p>4.2.2 Study for Improvement Strategies: Table 2.</p>
<p><img loading="lazy" src="./20210311/9.png" alt=""  />
</p>
<p><img loading="lazy" src="./20210311/10.png" alt=""  />
</p>
<p>4.2.3 Visualization of Attention Module: Figure 6.</p>
<p><img loading="lazy" src="./20210311/11.png" alt=""  />
</p>
<p>4.2.4 Comparing with State-of-the-art: Table 3.</p>
<p><img loading="lazy" src="./20210311/12.png" alt=""  />
</p>
<p>4.3. Results on PASCAL VOC 2012 Dataset: Table4, Table5.</p>
<p><img loading="lazy" src="./20210311/13.png" alt=""  />
</p>
<p>4.4. Results on PASCAL Context Dataset: Table 6</p>
<p><img loading="lazy" src="./20210311/14.png" alt=""  />
</p>
<p>4.5. Results on COCO Stuff Dataset: Table 7.</p>
<p><img loading="lazy" src="./20210311/15.png" alt=""  />
</p>
<h2 id="5-conclusion">5. Conclusion<a hidden class="anchor" aria-hidden="true" href="#5-conclusion">#</a></h2>
<ul>
<li>presented a Dual Attention Network (DANet) for scene segmentation, which adaptively integrates local semantic features using the <strong>self-attention mechanism</strong>.</li>
<li><strong>position attention module</strong> and a <strong>channel attention module</strong> to capture global dependencies in the spatial and channel dimensions respectively.</li>
<li>dual attention modules <strong>capture long-range contextual information</strong> effectively and give more precise segmentation results.</li>
<li>Dual Attention Network (DANet) achieves <strong>outstanding performance</strong> consistently on four scene segmentation datasets, Cityscapes, Pascal VOC 2012, Pascal Context, and COCO Stuff.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://landodo.github.io/tags/cnn/">CNN</a></li>
      <li><a href="http://landodo.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">è®ºæ–‡é˜…è¯»</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://landodo.github.io/posts/20210313-wrn/">
    <span class="title">Â« Prev Page</span>
    <br>
    <span>Wide Residual Networks</span>
  </a>
  <a class="next" href="http://landodo.github.io/posts/20210310-attentive-inception-module/">
    <span class="title">Next Page Â»</span>
    <br>
    <span>Attentive Inception Module based Convolutional Neural Network for Image Enhancement</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Landon</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
